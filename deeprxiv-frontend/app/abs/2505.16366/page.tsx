'use client';

import { useState, useEffect, useRef } from 'react';
import Link from 'next/link';
import { ArrowLeft, Image as ImageIcon, ExternalLink, X, Play, FileText, BookOpen, Menu } from 'lucide-react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import remarkMath from 'remark-math';
import rehypeKatex from 'rehype-katex';
import 'katex/dist/katex.min.css';

// Custom CSS for hiding scrollbars and responsive margins
const customStyles = `
  .scrollbar-hide {
    -ms-overflow-style: none;  /* Internet Explorer 10+ */
    scrollbar-width: none;  /* Firefox */
  }
  .scrollbar-hide::-webkit-scrollbar {
    display: none;  /* Safari and Chrome */
  }
  .main-content {
    margin-left: 0;
    margin-right: 0;
  }
  @media (min-width: 768px) {
    .main-content {
      margin-left: 352px;
      margin-right: 0;
    }
  }
  @media (min-width: 1024px) {
    .main-content {
      margin-left: 416px;
      margin-right: 512px;
    }
  }
`;

// Types for better TypeScript support
interface ImageData {
  id: string;
  page: number;
  original_position?: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  expanded_position?: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  path?: string;
  url?: string;
}

interface SubSection {
  id: string;
  title: string;
  content: string;
  citations?: string[];
  page_number?: number;
}

interface Section {
  id: string;
  title: string;
  content: string;
  citations?: string[];
  page_number?: number;
  subsections?: SubSection[];
}

// Paper data
const paperData = {
  id: 4,
  arxiv_id: '2505.16366',
  title: 'RECOPILOT : REVERSE ENGINEERING COPILOT IN BINARY ANALYSIS',
  authors: 'Guoqiang Chen, Huiqi Sun, Daguang Liu, Zhiqi Wang, Qiang Wang, Bin Yin, Lu Liu, Lingyun Ying',
  abstract: 'Binary analysis plays a pivotal role in security domains such as malware detection and vulnerability discovery, yet it remains labor-intensive and heavily reliant on expert knowledge. General-purpose large language models (LLMs) perform well in programming analysis on source code, while binary-specific LLMs are underexplored. In this work, we present ReCopilot, an expert LLM designed for binary analysis tasks. ReCopilot integrates binary code knowledge through a meticulously constructed dataset, encompassing continue pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO) stages. It leverages variable data flow and call graph to enhance context awareness and employs test-time scaling to improve reasoning capabilities. Evaluations on a comprehensive binary analysis benchmark demonstrate that ReCopilot achieves state-of-the-art performance in tasks such as function name recovery and variable type inference on the decompiled pseudo code, outperforming both existing tools and LLMs by 13%. Our findings highlight the effectiveness of domain-specific training and context enhancement, while also revealing challenges in building super long chain-of-thought. ReCopilot represents a significant step toward automating binary analysis with interpretable and scalable AI assistance in this domain.',
  processed: true
};

// Sections data
const sectionsData: Section[] = [{"id": "foundation-and-context-of-binary-analysis", "title": "Foundation and Context of Binary Analysis", "content": "Here is a comprehensive educational breakdown for the \"Foundation and Context of Binary Analysis\" section, structured for advanced learners and researchers:\n\n---\n\n## Learning Objectives and Context\n\nThis section introduces the core problems and motivations that drive research in binary analysis for cybersecurity. You will learn:\n\n- **Why binary analysis is essential**: It enables the detection of vulnerabilities, malware, and other security threats when source code is unavailable\u2014a common scenario in real-world security audits[1][2].\n- **The challenge of stripped binaries**: Many binaries lack symbolic information (e.g., function/variable names), making it hard to understand their behavior.\n- **Limitations of traditional decompilers**: Tools like IDA Pro and Ghidra generate pseudo code, but struggle to recover meaningful names and types, leading to obscure outputs.\n- **The emerging role of large language models (LLMs)**: While LLMs excel at source code analysis, adapting them to binary analysis is underexplored and presents unique challenges.\n- **How ReCopilot addresses these gaps**: By leveraging domain-specific training and context enhancement, ReCopilot advances the state of the art in automated binary analysis.\n\nThis section lays the groundwork for understanding the research rationale and connects to broader efforts in AI-assisted reverse engineering[1][2][3].\n\n---\n\n## Core Concepts and Motivations\n\n### What Is Binary Analysis?\n\n**Binary analysis** is the process of examining executable files to understand their structure, behavior, and potential vulnerabilities. Unlike source code analysis, binary analysis often works on files that have been compiled and stripped of useful debugging information, leaving only machine instructions and raw data[1][2]. This is critical in security domains\u2014ranging from malware detection to vulnerability discovery\u2014because attackers often target software in its deployed form, not its original source[2][3].\n\n### The Challenge of Stripped Binaries\n\nWhen binaries are stripped, crucial debugging symbols (like function and variable names) are removed. For example, an AES encryption function in source code might look like this:\n\n\`\`\`c\nvoid AES_CBC_encrypt_buffer(struct AES_ctx *ctx, uint8_t* buf, size_t length) {\n  // Implementation\n}\n\`\`\`\nAfter stripping and decompilation, it might appear as:\n\`\`\`c\nvoid __fastcall sub_1909(__int64 a1, __int64 a2, unsigned __int64 a3) {\n  // Obfuscated implementation\n}\n\`\`\`\nThe decompiled version lacks meaningful names and types, making it difficult for analysts to understand the code\'s true purpose, as illustrated in Figure 1 (page 2)[3].\n\n### Limitations of Traditional Decompilers\n\nTraditional decompilers (e.g., IDA Pro and Ghidra) convert machine code into pseudo-code resembling C. However, this pseudo-code is limited by:\n\n- **Loss of semantic information**: Variable and function names are replaced by generic placeholders.\n- **Incomplete type recovery**: The structure of complex data types (like \`struct\`) is often lost.\n- **Difficulty in semantic reasoning**: Analysts must infer meaning from context, which is error-prone and time-consuming.\n\nThese limitations are highlighted in the paper\u2019s example comparing decompiled pseudo-code with its original source (page 2), showing how much semantic information is lost[3].\n\n### The Promise and Limits of LLMs for Binary Analysis\n\nLarge language models (LLMs) have revolutionized programming tasks by generating code, fixing bugs, and summarizing logic when trained on source code. However, when applied to binary analysis, general-purpose LLMs face several challenges:\n\n- **Data scarcity**: Public datasets for binary code are small and lack alignment between binaries and source code (page 3).\n- **Semantic gap**: Binary code lacks the rich structure and context of source code, making it harder for LLMs to learn meaningful patterns.\n- **Performance limitations**: General LLMs underperform in tasks requiring deep semantic understanding of stripped binaries, as shown by recent evaluations (page 4).\n\nSome prior work has attempted to predict function names or variable types using neural models, but these efforts are limited to single tasks and do not address the full scope of real-world binary analysis (page 5)[3].\n\n---\n\n## Mathematical and Technical Foundation\n\n### The Semantic Gap: A Mathematical Perspective\n\nLet $S$ be a set of source code functions and $B$ the corresponding set of decompiled binary functions after stripping. The decompilation process can be modeled as a function:\n\n\\[\nD: B \\rightarrow P\n\\]\n\nwhere $P$ represents pseudo-code. The recovery of source-level symbols (names, types) is a mapping:\n\n\\[\nR: P \\rightarrow S\'\n\\]\n\nwhere $S\'$ is a partial reconstruction of the original source $S$.\n\nThe challenge is that $D$ is often lossy, and $R$ must infer missing information using contextual clues, call graphs, and data flow analysis, as shown in Figure 4 (page 7)[3].\n\n### Context Enhancement through Static Analysis\n\nReCopilot leverages static program analysis to construct call chains and data flow graphs, enriching the context available to the LLM. This can be formalized as:\n\n\\[\nC(f) = \\text{Context}(f) = (\\text{CallChains}(f), \\text{DataFlow}(f), \\text{NeighboringFunctions}(f))\n\\]\n\nwhere $f$ is the target binary function. The model input is a prompt constructed as:\n\n\`\`\`\n<context-pseudocode>\n{context}\n</context-pseudocode>\n<pseudocode>\n{target_func}\n</pseudocode>\n<Call-Chains>\n{call_chains}\n</Call-Chains>\n<Data-Flow>\n{data_flow}\n</Data-Flow>\nAnalysis Task Tag:\n{task_tag}\n\`\`\`\nThis modular input format allows the model to leverage both local and global context, as detailed on page 7[3].\n\n---\n\n## Implementation Details and Design Choices\n\n### Training Strategy\n\nReCopilot is trained in three stages, as depicted in Figure 2 (page 4):\n\n1. **Continued Pretraining (CPT)**: The base LLM is adapted to the binary domain using a large dataset of decompiled pseudo-code and corresponding source code.\n2. **Supervised Fine-Tuning (SFT)**: The model is fine-tuned on a mixture of general and domain-specific data to support reasoning-intensive tasks like function name recovery and type inference.\n3. **Direct Preference Optimization (DPO)**: The model\u2019s outputs are refined using preference learning, ensuring logical consistency and correct formatting.\n\n### Dataset Construction\n\nThe dataset is meticulously constructed to include:\n\n- **Raw binary functions** (101M from 11K projects)\n- **Corresponding source code** (aligned via debug information)\n- **General code and natural language data** (to prevent overfitting)\n\nThe data mixture is carefully balanced (60% binary, 25% code, 15% text), as shown in Table 2 (page 6)[3].\n\n### Generator-Discriminator Framework for SFT Data\n\nTo generate high-quality training data with reasoning processes, ReCopilot uses a generator-discriminator framework (see Figure 5, page 8). This framework:\n\n- **Generates candidate reasoning steps** using existing LLMs and domain heuristics.\n- **Discriminates high-quality examples** by filtering out incorrect or nonsensical reasoning.\n- **Ensures the dataset contains realistic chain-of-thought examples**, which are crucial for training the model to reason about binary code.\n\n---\n\n## Significance, Innovations, and Broader Impact\n\n### Why This Approach Matters\n\nReCopilot addresses critical gaps in binary analysis:\n\n- **Comprehensive task support**: Unlike prior work focused on single tasks, ReCopilot supports function name recovery, variable/type inference, decompilation, and code summarization in a unified framework (page 5)[3].\n- **Domain-specific training**: By pretraining and fine-tuning on a massive dataset of real-world binaries and source code, the model achieves state-of-the-art performance with a relatively compact architecture (7B parameters)[3].\n- **Context-aware reasoning**: The model leverages call chains and data flow to provide richer context, enabling more accurate and interpretable results (page 7).\n\n### Connections to Related Work\n\nReCopilot builds on and extends recent advances in neural-based and LLM-based binary analysis, including:\n\n- **Function name prediction**[21\u201327]\n- **Variable name and type inference**[28\u201335]\n- **Binary code summarization and decompilation**[36, 31, 10]\n\nHowever, prior methods are limited to narrow tasks and do not provide the comprehensive, context-aware analysis that ReCopilot achieves (page 5)[3].\n\n### Implications for the Field\n\nReCopilot represents a significant advance toward automating binary analysis with AI, reducing reliance on expert knowledge and enabling scalable, reproducible security audits. Its success highlights the importance of domain-specific training and context enhancement, setting a new standard for AI-assisted reverse engineering[3].\n\n---\n\n## Summary Table: Key Features of ReCopilot\n\n| Feature                  | Description                                                                 | Page Reference |\n|--------------------------|-----------------------------------------------------------------------------|----------------|\n| Domain-specific training | Pretraining and fine-tuning on large binary/source datasets                 | 4\u20136            |\n| Context enhancement      | Leverages call chains and data flow for richer context                      | 7              |\n| Multi-task support       | Handles function/variable recovery, decompilation, summarization            | 5              |\n| Efficient architecture   | Achieves SOTA with 7B parameters                                            | 3              |\n| High-quality data        | 101M binary functions, carefully curated and balanced                       | 6              |\n\n---\n\nThis breakdown provides a robust foundation for understanding the challenges, innovations, and technical underpinnings of binary analysis as addressed by ReCopilot, setting the stage for deeper exploration of its methodology and results.", "citations": ["https://bugprove.com/knowledge-hub/binary-analysis-fundamentals/", "https://www.reversinglabs.com/glossary/complex-binary-analysis", "https://www.blackduck.com/glossary/what-is-binary-code-binary-analysis.html", "https://ctf101.org/binary-exploitation/overview/", "https://systemweakness.com/binary-file-analysis-techniques-tools-and-challenges-c610f52382ff"], "page_number": 2, "subsections": [{"id": "research-problem-motivation-in-binary-analysis", "title": "Research Problem and Motivation in Binary Analysis", "content": "## Research Problem and Motivation in Binary Analysis\n\nThis section details the fundamental challenges and motivations driving research in binary analysis, a critical area in cybersecurity focused on understanding and interpreting executable binaries without access to source code. The discussion lays the groundwork to appreciate why advanced automated tools, especially those leveraging machine learning, are essential for improving the effectiveness and efficiency of binary analysis.\n\n### Introduction\n\nBinary analysis refers to examining compiled executable files (binaries) to extract meaningful information about their behavior and structure, crucial for activities such as vulnerability detection, malware analysis, and software supply chain security. Unlike source code, binaries lack symbolic metadata\u2014meaning function names, variable names, and type information are typically stripped during compilation to reduce size or obfuscate functionality. This absence complicates comprehension and makes manual reverse engineering labor-intensive and error-prone. Understanding these challenges provides essential context for this paper\'s research, which aims to develop an expert large language model (LLM) tailored specifically for binary analysis tasks, addressing the gaps in existing tools and methodologies.\n\nIn the broader research landscape, binary analysis intersects domains such as static program analysis, software security, and machine learning. The motivation arises not only from the practical necessity in cybersecurity operations\u2014where rapid and accurate binary understanding can thwart sophisticated attacks\u2014but also from the theoretical challenge of reconstructing high-level semantics from low-level, symbol-deprived code. This sets the stage for the work presented in this paper, which introduces ReCopilot, an AI assistant combining deep domain knowledge and reasoning capabilities to automate and enhance binary analysis workflows (see Introduction, page 1; Figure 1 on page 2).\n\n### Core Content\n\nAt the heart of the binary analysis problem is the *symbolic information recovery* challenge. When binaries are stripped, meaningful identifiers are replaced by generic placeholders (e.g., \`a1\`, \`v1\`), severely limiting the interpretability of the generated pseudo code. Traditional decompilers like IDA Pro or Ghidra can lift machine code into C-like pseudo code but fail to recover semantic symbols such as variable names, types, or function signatures, which are critical for understanding program logic and security implications.\n\n**Figure 1 (page 2)** vividly contrasts stripped pseudo code and original source code of a real-world AES encryption function. The source code uses descriptive symbols such as \`AES_ctx\` and \`Iv\` that immediately convey functionality, while the pseudo code shows ambiguous names like \`a1\` and memory offsets without structural context. This symbolic gap hinders comprehension and subsequent security tasks like vulnerability analysis and reverse engineering.\n\nFrom a formal perspective, symbol recovery can be viewed as a mapping problem: Given a binary function represented in decompiled pseudo code \\( B \\), the goal is to infer a mapping function\n\n$$\nf: B \\rightarrow S\n$$\n\nwhere \\( S \\) represents the reconstructed symbolic information, including function names \\( F \\), variable names \\( V \\), and types \\( T \\). This mapping is highly non-trivial due to information loss and the complexity of the binary semantics.\n\nThe paper argues for leveraging domain-specific large language models trained on paired datasets of binaries and corresponding source code to learn this mapping implicitly. Unlike general-purpose models, a domain-focused LLM can encode binary-specific patterns and reasoning about control flow, data flow, and call graphs to predict symbols with higher accuracy and context awareness.\n\nThe problem is compounded by several practical challenges:\n\n- **Data scarcity and quality:** Public datasets with aligned binary and source code are limited and noisy, necessitating large-scale, high-quality dataset construction to train effective models (page 5-7, Dataset Building).\n- **Model scalability and efficiency:** Binary analysis often requires offline, local inference on resource-constrained devices, motivating compact yet powerful models.\n- **Context limitations:** Analyzing isolated functions ignores valuable inter-procedural context such as call chains and data flow, which must be incorporated for deeper understanding.\n\nThe paper introduces a modular input-output template (Figure 4, page 6) that integrates decompiled pseudo code, static program analysis context (call chains, data flow), and task-specific tags to enable multi-task learning for function name recovery, variable type inference, summarization, and more.\n\n### Technical Details\n\nReCopilot\'s development involves a multi-stage training pipeline:\n\n1. **Continued Pretraining (CPT):** Starting from a base general LLM, CPT injects domain knowledge by training on a carefully curated dataset with 36 billion tokens, dominated by binary pseudo code aligned with source code and natural language comments (see Table 2, page 7). This stage helps the model learn bidirectional mappings between binary and source modalities using a next-token prediction task.\n\n2. **Supervised Fine-Tuning (SFT):** The model is then fine-tuned to perform reasoning-intensive binary analysis tasks using a large-scale, automatically generated dataset covering 14 defined tasks such as function name recovery, variable inference, and code summarization (pages 6\u20138). A generator-discriminator framework is employed to produce rich chain-of-thought (CoT) reasoning examples, enabling the model to \'think deeply\' before producing predictions.\n\n3. **Direct Preference Optimization (DPO):** Finally, the model undergoes reinforcement learning from preferences to improve output format adherence and reasoning consistency without requiring costly human feedback (page 8). DPO fine-tunes the model to prefer logically sound and well-formatted responses, critical for usability in automated binary analysis pipelines.\n\nAlgorithmically, the multi-task training input consists of:\n\n\`\`\`\n<context-pseudocode>\n{context functions}\n</context-pseudocode>\n<pseudocode>\n{target function}\n</pseudocode>\n<Call-Chains>\n{call chains}\n</Call-Chains>\n<Data-Flow>\n{data flow}\n</Data-Flow>\n<Task Tag>\n{selected task}\n</Task Tag>\n<Thought>\nModel reasoning process...\n</Thought>\n<Output>\nFinal prediction in JSON\n</Output>\n\`\`\`\n\nThe context enhancement leverages static program analysis to build call graphs and data flows, enabling the LLM to use enriched semantic context rather than isolated snippets, addressing the context limitation challenge (page 8, \u00a73.4).\n\nParameter choices in dataset composition (60% binary domain, 25% general code, 15% natural language) and careful sanitization/deduplication using locality-sensitive hashing (MinHash) ensure high-quality, diverse, and scalable training data (Table 1, page 5).\n\n### Significance & Connections\n\nThis approach embodies a significant advancement in binary analysis by integrating expert domain knowledge with powerful LLM architectures specialized for reverse engineering tasks. Unlike prior works, which either tackled single tasks or used general-purpose models, ReCopilot supports multiple critical tasks simultaneously, leveraging context enrichment and multi-stage training to attain state-of-the-art accuracy (page 1, Abstract).\n\nBy enabling automated recovery of symbolic information and enhanced code comprehension directly from stripped binaries, this research opens new avenues for accelerating vulnerability detection, malware analysis, and software supply chain integrity verification. It also demonstrates the potential of domain-specialized AI models to overcome limitations of generic LLMs in highly technical fields, offering scalable, interpretable aid to security analysts.\n\nReCopilot\'s innovations align seamlessly with broader efforts in cybersecurity to enhance tool automation while maintaining human-guided interpretability and precision, contributing a valuable building block toward fully AI-augmented binary reverse engineering workflows.\n\n---\n\nThis comprehensive overview synthesizes the research problem\'s context, technical foundations, and innovative contributions, providing a solid foundation for readers to grasp the motivations and challenges addressed in this paper.", "citations": ["https://finitestate.io/blog/why-automation-isnt-enough-for-embedded-security", "https://www.contrastsecurity.com/glossary/binary-code-analysis", "https://www.cs.montana.edu/izurieta/thesis/JohnsonAndrewThesis.pdf", "https://wpcdn.web.wsu.edu/wp-vcea/uploads/sites/3267/2022/06/BinaryAnalysis101.pdf", "https://scantist.com/resources/blogs/decoding-defense-why-binary-analysis-is-the-linchpin-of-modern-cybersecurity"], "page_number": 2}, {"id": "background-on-llms-and-binary-analysis", "title": "Background on Large Language Models and Binary Analysis", "content": "## Background on Large Language Models and Binary Analysis\n\nThis section provides foundational knowledge on large language models (LLMs) and their emerging application to binary code analysis. Understanding this background is essential for appreciating the motivations, design choices, and contributions of the research paper presenting ReCopilot, an expert LLM tailored for complex binary analysis tasks. As LLMs demonstrate remarkable capabilities in source code understanding and generation, exploring their potential and challenges in analyzing compiled binaries deepens insight into this novel research direction.\n\nBinary analysis is a crucial but traditionally labor-intensive process in cybersecurity, focused on deciphering executable programs stripped of symbolic information such as function and variable names. Addressing this gap could revolutionize vulnerability detection, malware analysis, and reverse engineering. This section situates ReCopilot\u2019s innovations within the broader landscape of programming LLMs and binary analysis methodologies, highlighting their interplay and identifying the key challenges that necessitate domain-specialized expert models.\n\n---\n\n### Core Concepts and Current Landscape\n\n**Large Language Models (LLMs)** are deep learning models designed primarily for natural language processing tasks but have shown extraordinary versatility. They are typically built using transformer architectures that use self-attention mechanisms to capture long-range dependencies in input sequences, enabling them to model complex patterns in text. Mathematically, LLMs are trained to maximize the likelihood of predicting the next token given a sequence of previous tokens, often by minimizing the negative log-likelihood loss:\n\n$$\n\\mathcal{L} = -\\sum_{t=1}^T \\log p_\\theta(x_t | x_{<t})\n$$\n\nwhere $x_t$ is the token at position $t$, and $\\theta$ are the model parameters.\n\nLLMs like OpenAI\u2019s GPT-series model billions of parameters and are trained on massive corpora of text and code, which allow them to perform tasks such as code generation, bug fixing, summarization, and reasoning across multiple programming languages [4]. For example, GPT-3 has 175 billion parameters, enabling it to predict patterns in text and code effectively, while newer models push these capabilities further. Importantly, LLMs process context windows of fixed length (e.g., 8K to 100K tokens), influencing how much information they can consider simultaneously.\n\nApplying LLMs for **source code tasks** has proven highly successful, with benchmarks like HumanEval and SWE-bench showing state-of-the-art performance in tasks such as function synthesis and bug resolution [3]. This success motivates attempts to extend LLM applications into **binary analysis**, which involves working with machine code or decompiled pseudo code that lacks source-level information.\n\nThe **binary analysis problem** is challenging primarily due to the absence of symbolic information in stripped binaries. Decompiled pseudo code, as shown in Figure 1a of the paper, often contains placeholders for function names and variables (e.g., \`sub_1909\`, \`a1\`, \`v7\`), making it difficult to infer high-level semantics compared to the original source code shown in Figure 1b [page 2]. Recovering this information\u2014such as function and variable names, types, and data structures\u2014is crucial for understanding the program\u2019s functionality.\n\nPreliminary studies have leveraged general-purpose LLMs to assist tasks like symbol recovery and binary code summarization . For example, BinSum demonstrates that LLMs can generate high-quality summaries of binary functions but their performance degrades significantly on stripped binaries lacking debug symbols. Similarly, other works reveal that general LLMs struggle to surpass domain-tuned models in key binary tasks.\n\nExisting methods usually target **individual binary analysis tasks**\u2014such as function name recovery or vulnerability detection\u2014and do not provide comprehensive solutions adaptable to real-world analysis scenarios [21\u201336]. This fragmentation points to the need for **domain-specialized expert LLMs** that can handle multiple tasks and leverage richer context, such as data flow and call graphs, to improve accuracy and scalability.\n\n---\n\n### Technical Details: Training and Context Enhancement\n\nThe ReCopilot approach hinges on a multi-stage training pipeline designed to inject domain knowledge and reasoning capabilities into a compact LLM (~7B parameters), suitable for local inference [pages 3-7].\n\n1. **Continue Pretraining (CPT):** Starting from a pretrained base LLM, continued pretraining on large-scale binary-related data injects domain-specific knowledge. The dataset blends pseudo code of stripped binaries, corresponding source code, and natural language comments (Figure 3). The training objective remains next-token prediction on a shuffled mixture of these modalities, fostering bidirectional understanding between binary and source code representations [page 5].\n\n2. **Supervised Fine-Tuning (SFT):** The model is further fine-tuned on a mixture of 14 different binary analysis tasks (function name recovery, variable type prediction, summarization, etc.). Crucially, prompts to the model include not only the target function\u2019s pseudo code but also **context enhancement** components derived from static program analysis:  \n   - Function context (neighboring functions)  \n   - Call chains  \n   - Data flow information  \n\n   This enriched input helps the model to reason about the binary code within a broader program scope (Figure 4 illustrates the prompt-output template) [page 6].\n\n3. **Direct Preference Optimization (DPO):** Finally, a reinforcement learning technique improves format adherence and reasoning consistency without requiring human feedback, optimizing the model\u2019s outputs toward user-preferred responses [page 7].\n\nThe **generator-discriminator framework** automatically produces fine-tuning examples with chain-of-thought reasoning, addressing the lack of human-labeled reasoning traces. This is essential for training the model to perform deep semantic reasoning rather than superficial pattern matching.\n\nThe context enhancement is critical due to LLMs\u2019 **context window size constraint**, where analyzing a large binary program atomically is infeasible. By incorporating static analysis summaries of related functions and data flows, the model effectively operates with a \"wider lens\" on the binary, improving task performance beyond isolated function-based predictions [pages 6-7].\n\n---\n\n### Significance and Broader Connections\n\nReCopilot\u2019s key novelty lies in developing an expert LLM explicitly tailored for **multi-task binary analysis**, leveraging domain-specialized datasets, context enhancement, and sophisticated training strategies (CPT, SFT, DPO). This contrasts with prior work that mostly applied general LLMs or neural nets to isolated tasks, which limited their practical applicability.\n\nBy integrating symbolic, syntactic, and semantic information from binaries and their source code counterparts, ReCopilot advances the state-of-the-art in function name recovery, variable type inference, struct recovery, and binary summarization, outperforming both general LLMs and previous domain models by a significant margin (13% on average) [page 8, Table 3].\n\nThis work bridges the gap between **natural language-based LLM capabilities** and the **domain-specific challenges of binary analysis**, demonstrating how domain expertise embedded in training data and context-aware prompting can unlock more accurate and scalable analysis tools.\n\nImplications include enhanced automation in cybersecurity workflows, reduced expert workload, and expanded possibilities for AI-assisted reverse engineering. The methodologies also set a precedent for applying expert LLMs in other specialized code-understanding domains, illustrating a path from general-purpose models to task-optimized, domain-focused language models.\n\n---\n\n## Summary\n\n- LLMs excel at source code tasks but face challenges with stripped binaries due to missing symbolic information.  \n- Binary analysis requires recovering function/variable names, types, and summarizing functionality from decompiled pseudo code.  \n- Existing LLM approaches mostly focus on single subtasks and exhibit performance gaps on real-world, stripped binaries.  \n- ReCopilot addresses these challenges via multi-stage training on large domain-specific datasets, enhanced context inputs, and reasoning-focused fine-tuning.  \n- This expert LLM advances practical binary analysis, boosting accuracy and enabling multi-task support, crucial for security applications.\n\nThe following sections will detail ReCopilot\u2019s methodology and evaluation, building on this foundational understanding of LLMs and binary analysis domain challenges.\n\n---\n\n*This explanation integrates knowledge from the research paper (pages 2\u20138, Figures 1, 3, 4, Tables 1, 2, 3) and general LLM principles [1][2][3][4], structured to facilitate comprehension and connect technical innovations with broader research context.*", "citations": ["https://en.wikipedia.org/wiki/Large_language_model", "https://aws.amazon.com/what-is/large-language-model/", "https://hatchworks.com/blog/gen-ai/large-language-models-guide/", "https://dev.to/hackmamba/these-are-the-best-large-language-models-for-coding-1co2", "https://snorkel.ai/large-language-models/"], "page_number": 2}, {"id": "related-work-in-binary-symbol-recovery-and-analysis", "title": "Related Work in Binary Symbol Recovery and Analysis", "content": "## Related Work in Binary Symbol Recovery and Analysis\n\nThis section provides a comprehensive overview of prior research in the domain of binary analysis, focusing specifically on methods for recovering symbolic information such as function names, variable names and types, and related code structure from stripped binaries. Understanding this background is essential to appreciate the motivations and innovations introduced by ReCopilot, the expert large language model (LLM) designed for multi-task binary analysis. This context situates ReCopilot among existing neural network and LLM approaches, highlighting gaps addressed by this work and its advancements within the broader research landscape.\n\nBinary analysis is a critical technique in cybersecurity fields such as malware detection, vulnerability identification, and reverse engineering. However, stripped binaries lack symbolic information (e.g., function and variable names), substantially complicating the understanding of code logic and semantics. Prior works have sought to fill this gap using machine learning and neural network methods to predict missing symbols or reconstruct source-level information. Nonetheless, most existing efforts have targeted specific tasks in isolation, such as function name recognition or variable type inference, without providing an integrated multi-task framework. Furthermore, the emergence of large language models in source code understanding has sparked new interest in applying LLMs to binary analysis, yet domain-specific LLM development remains nascent.\n\n---\n\n### Core Concepts and Historical Approaches\n\n**Function Name Recovery and Symbol Prediction**\n\nOne foundational challenge in binary analysis is identifying function boundaries and recovering their original names. Early approaches utilized recurrent neural networks (RNNs) to recognize function start points and predict function names directly from binary instruction sequences. For example, Shin et al. (USENIX 2015) demonstrated that RNNs can identify functions within binaries more accurately and efficiently than traditional machine learning methods, significantly reducing error rates across benchmarks[1]. These neural approaches treat disassembled instructions as sequential input, learning contextual patterns indicative of function boundaries and semantics.\n\n**Variable Name and Type Inference**\n\nFollowing function recovery, predicting variable names and their data types in decompiled pseudo code is another key task. Many models use neural networks to analyze the structure and usage patterns within the binary and attempt to assign semantically meaningful names and types to variables. Work by researchers such as those behind TYGR and ReSym employs graph neural networks (GNNs) and attention mechanisms to infer complex variable structures like structs, enhancing the recovery of rich symbolic information from low-level code. These models analyze control/data flow graphs and pseudo code syntax to deduce plausible type information.\n\n**Graph and Embedding-Based Binary Similarity**\n\nTo support variable and function recognition, several works embed binary instructions or control flow graphs (CFGs) into vector spaces, enabling similarity comparison across binaries or architectures. Techniques inspired by natural language processing (NLP), such as instruction2vec and neural machine translation (NMT) approaches, model sequences of assembly instructions or basic blocks analogously to words and sentences, respectively, facilitating robust cross-architecture binary similarity detection and function matching[3][4]. These embeddings help downstream models reason about binary semantics in a continuous latent space.\n\n**Limitations of Prior Single-Task Models**\n\nDespite progress, many existing methods address only one symbol recovery task at a time, e.g., predicting function names independently of variable types or struct reconstruction. This siloed approach restricts practical applicability, as real-world binary analysis demands integrated reasoning across multiple symbolic levels \u2014 from functions and variables to type structures and code summarization.\n\n**LLM Integration in Binary Analysis**\n\nRecent advances in LLMs have revolutionized source code analysis, providing capabilities in code generation, summarization, and bug fixing. However, their application to binary analysis is less mature. Preliminary works fine-tune LLMs for specific tasks such as decompilation or function name recovery, but these tend to be task-specific and limited in context awareness. General-purpose LLMs struggle with binary domain nuances due to lack of domain-adaptation and context limitations. Large-scale binary-specific datasets and multi-task training remain underexplored.\n\n---\n\n### Mathematical and Methodological Foundations\n\nAt the heart of neural-symbolic binary analysis lies the modeling of binary functions and code snippets as sequences or graphs that encapsulate semantic meaning. Formally, a binary function \\( f \\) can be represented as a sequence of instructions\n\n$$\nf = (i_1, i_2, \\ldots, i_n)\n$$\n\nwhere each instruction \\( i_j \\) is a token encoding an opcode and operands. The goal is to learn a model \\( M \\) mapping from \\( f \\) to symbolic attributes such as function name \\( \\phi_f \\), variable names \\( \\Phi_v = \\{\\phi_{v_i}\\} \\), and variable types \\( \\Theta_v = \\{\\theta_{v_i}\\} \\):\n\n$$\nM: f \\rightarrow \\{\\phi_f, \\Phi_v, \\Theta_v, \\ldots\\}\n$$\n\nNeural networks, particularly recurrent (RNN, LSTM) and transformer-based architectures, are employed to learn this mapping by optimizing parameters \\(\\theta\\) to minimize a loss function:\n\n$$\n\\mathcal{L}(\\theta) = \\sum_{k=1}^N \\ell \\big(M_\\theta(f^{(k)}), y^{(k)} \\big)\n$$\n\nwhere \\( y^{(k)} \\) denotes the ground truth symbolic annotations for the \\(k\\)-th function in the dataset, and \\(\\ell\\) is an appropriate loss such as cross-entropy for classification or token prediction.\n\nIn graph-based methods, the function is modeled as a control flow graph \\( G = (V, E) \\), where nodes \\( V \\) represent basic blocks or instructions and edges \\( E \\) encode control transitions. Graph neural networks update node embeddings \\( h_v \\) using message passing:\n\n$$\nh_v^{(l+1)} = \\sigma \\left( W^{(l)} \\cdot \\sum_{u \\in \\mathcal{N}(v)} h_u^{(l)} + b^{(l)} \\right)\n$$\n\nwhere \\(\\mathcal{N}(v)\\) is the set of neighbors of \\(v\\), \\(\\sigma\\) a nonlinear activation, and \\(l\\) the layer index. Final embeddings serve to classify or regress symbolic attributes.\n\nReCopilot advances this by jointly modeling multiple tasks within a unified architecture and training regime, employing chain-of-thought (CoT) reasoning to improve interpretability (discussed in later sections).\n\n---\n\n### Technical Details and Implementation\n\n**Datasets and Multi-Task Setup**\n\nEarlier works typically rely on datasets specifically annotated for a single task, such as function name labels only[1]. ReCopilot innovates by assembling a large-scale, multi-modal dataset that aligns stripped pseudo code, source code, and natural language comments (page 5). This dataset enables multi-task supervised fine-tuning (SFT) to simultaneously recover function names, variable names/types, structures, and summaries (page 6, Table 1, Figure 3).\n\nThe input to the model includes not only the target decompiled function but also enriched context through static analysis artifacts: call chains and variable data flow graphs (page 7, Figure 4). These context enhancement techniques provide richer semantic signals, mitigating the limited local context problem common in previous approaches.\n\n**Training Pipeline**\n\nReCopilot\'s training involves three key stages (page 6, Figure 2):\n\n1. **Continued Pretraining (CPT):** The base LLM is further pretrained on the domain-specific dataset comprising 60B tokens of binary pseudo code, source code, and natural language, to internalize binary-specific language patterns.\n\n2. **Supervised Fine-Tuning (SFT):** The model is fine-tuned on multi-task labeled data to predict symbolic information and generate human-readable explanations with chain-of-thought reasoning, aiding interpretability and accuracy.\n\n3. **Direct Preference Optimization (DPO):** A reinforcement learning technique where the model learns to prefer high-quality predictions over less accurate ones without human feedback, improving output format adherence and logical consistency (page 8).\n\n**Algorithmic Outline of the Generator-Discriminator Framework for SFT Data (page 7):**\n\n\`\`\`pseudo\nFor each binary function sample:\n    1. Generate candidate outputs with chain-of-thought reasoning using generator LLM.\n    2. Use discriminator model to score candidates based on accuracy and reasoning correctness.\n    3. Select top candidates as training data.\n    4. Repeat to augment SFT dataset with rich reasoning chains.\n\`\`\`\n\nThis framework addresses the scarcity of reasoning-annotated data by combining model synthesis and automatic quality filtering.\n\n---\n\n### Significance and Connections\n\nReCopilot represents a notable advancement by integrating multi-task symbolic recovery into a single expert LLM tuned specifically for the binary domain, addressing critical limitations of prior neural-symbolic and LLM methods that were task-specific or general-purpose without domain adaptation. Its joint modeling of function names, variable types/names, structures, and code summarization within a reasoning-capable architecture underscores the importance of holistic analysis in real-world binary reverse engineering.\n\nBy leveraging domain-specific datasets with static analysis-derived context and employing innovative training strategies such as chain-of-thought fine-tuning and direct preference optimization, ReCopilot sets a new state-of-the-art baseline in binary analysis tasks (page 10, Table 3). This work bridges gaps between traditional binary analysis methods, neural-symbolic approaches, and LLM-based programming tools, contributing to more automated, interpretable, and scalable reverse engineering capabilities.\n\nFurthermore, the approach anticipates broader adoption of expert domain LLMs in cybersecurity, pushing the frontier from single-task models and general-purpose LLMs toward integrated, context-aware AI assistants for complex code understanding challenges.\n\n---\n\nThis educational overview synthesizes prior research on binary symbol recovery and situates ReCopilot\u2019s contributions, preparing readers to appreciate the technical and methodological details presented in the subsequent sections of the paper.", "citations": ["https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/shin", "https://www.mdpi.com/2079-9292/12/22/4671", "https://www.ndss-symposium.org/ndss-paper/neural-machine-translation-inspired-binary-code-similarity-comparison-beyond-function-pairs/", "https://arxiv.org/pdf/2306.14168", "https://github.com/SystemSecurityStorm/Awesome-Binary-Similarity"], "page_number": 3}]}, {"id": "methodology-and-technical-approach-of-recopilot", "title": "Methodology and Technical Approach of ReCopilot", "content": "## Methodology and Technical Approach of ReCopilot\n\nThis section provides a detailed exploration of the methodology and technical strategies behind **ReCopilot**, a specialized large language model (LLM) designed for multi-task binary analysis. Understanding this section is crucial because it explains how ReCopilot is constructed and trained to tackle the complex challenges of reverse engineering stripped binaries\u2014a task traditionally requiring extensive domain expertise. ReCopilot integrates advances in LLM training, domain-specific data construction, and static program analysis to significantly improve accuracy and interpretability in binary analysis tasks.\n\nWithin the broader research context, this section connects foundational concepts of language model training to the specific challenges in binary analysis, such as the absence of symbolic information in decompiled code. It outlines the sequential training pipeline\u2014continued pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO)\u2014alongside innovative dataset creation and context enhancement methods, situating ReCopilot at the intersection of AI and cybersecurity research.\n\n---\n\n### Core Methodology\n\n#### Three-Stage Model Building Process\n\nReCopilot is built through a three-stage training pipeline, as illustrated in **Figure 2 on page 3**:\n\n1. **Continued Pretraining (CPT)**: Starting from a general-purpose pretrained base LLM, continued pretraining adapts the model to binary code\u2019s unique characteristics. CPT uses a large-scale curated dataset of decompiled pseudo code, corresponding source code, and natural language comments (about 60 billion tokens) to teach the model the syntax and semantics of binary-specific programming languages. Training optimizes for next-token prediction with an objective of modeling the joint distribution over the multi-modal data (binary, source code, natural language), conceptually:\n\n   $$\n   \\max_\\theta \\sum_{t=1}^T \\log p_\\theta(x_t \\mid x_{<t}),\n   $$\n\n   where $x_t$ are the tokens from the mixed domain data, and $\\theta$ are the model parameters.\n\n   Inner-shuffling of data segments encourages learning bi-directional mappings between code and binary[pages 3\u20135].\n\n2. **Supervised Fine-Tuning (SFT)**: This stage adapts the model to perform specific binary analysis tasks by training on labeled input-output pairs, reflecting 14 diverse binary analysis subtasks (e.g., function name recovery, variable type inference). A key innovation is the use of a **generator-discriminator framework** to produce chain-of-thought (CoT) reasoning data. The CoT enables the model to generate explicit intermediate reasoning steps before producing final outputs, enhancing interpretability and accuracy. The supervised objective involves minimizing cross-entropy loss over token sequences that include both reasoning and output, facilitating reasoning-intensive programming tasks.\n\n3. **Direct Preference Optimization (DPO)**: Finally, ReCopilot undergoes DPO, which refines the SFT model by optimizing alignment with user preferences without requiring human feedback or complex reward functions. DPO uses pairs of chosen and rejected model outputs to update the model to prefer higher-quality responses. Formally, DPO seeks to adjust parameters $\\theta$ to maximize preference likelihood:\n\n   $$\n   \\max_\\theta \\sum_i \\log \\sigma(s_\\theta(x_i, y_i^+) - s_\\theta(x_i, y_i^-)),\n   $$\n\n   where $s_\\theta$ is the scoring function of the model on prompt $x_i$ with preferred response $y_i^+$ versus rejected $y_i^-$, and $\\sigma$ is the sigmoid function[pages 3\u20134].\n\n#### Dataset Construction and Context Enhancement\n\nDataset building is critical due to domain specificity and large-scale data needs. The authors assembled a **raw dataset of 101 million binary functions** from 11,472 projects via three pipelines: compiling from scratch, collecting off-the-shelf software artifacts, and using CompileAgent (a LLM-driven compilation agent). They then sanitized and deduplicated this dataset using MinHash to reduce redundancy and noise[pages 4\u20136, Table 1].\n\nThe pretraining dataset (Figure 3) includes triplets of binary pseudo code, source code, and natural language comments, shuffled internally to foster cross-modal learning. The final CPT dataset is a mix of 60% binary domain data, 25% general code, and 15% natural language to balance domain expertise and general language understanding[page 5, Table 2].\n\nFor SFT, a **modular input-output template** (Figure 4) integrates enhanced context\u2014static program analysis outputs including call chains and variable data flow\u2014to provide a comprehensive view of each function\u2019s surroundings. This enhanced context addresses LLMs\u2019 context window constraints and enriches reasoning with inter-procedural information, improving the model\u2019s ability to infer missing symbols and types[page 6].\n\n---\n\n### Technical Details\n\n#### Generator-Discriminator Framework for Chain-of-Thought Data Generation\n\nA novel contribution is the **generator-discriminator framework** that automates the creation of high-quality SFT data with explicit reasoning steps. Since human-labeled reasoning processes are scarce, the generator is a reasoning-capable LLM producing candidate CoT outputs for binary analysis tasks. The discriminator evaluates these outputs for correctness and coherence against ground truth source code and removes flawed samples.\n\nThis framework cycles through generating hypotheses and filtering them, resulting in scalable generation of training data with rich reasoning annotations crucial for training ReCopilot\u2019s deep thinking ability[pages 6\u20137, Figure 5].\n\n#### Model Input and Output Specification\n\nThe input to ReCopilot includes:\n\n- Target function decompiled pseudo code\n- Context functions, call chains, and data flow graphs from static analysis\n- Task tag specifying the specific binary analysis problem\n\nOutput comprises:\n\n- Chain-of-thought reasoning steps encapsulated in a \`<Thought>\` tag\n- Final prediction formatted as structured JSON for easy parsing and integration into tools\n\n\`\`\`json\n{\n  \"return_type\": \"...\",\n  \"function_name\": \"...\",\n  \"arguments\": [...],\n  \"variables\": [...],\n  \"comments\": \"...\",\n  \"category\": \"...\",\n  \"algorithm\": \"...\"\n}\n\`\`\`\n\nThis design prioritizes **format consistency** and **logic rigor**, both reinforced during DPO training to reduce format errors and improve reasoning consistency[page 6, Figure 4].\n\n#### Parameter and Design Decisions\n\n- A relatively small model size (~7B parameters) is chosen to enable **local inference on laptops**, addressing practical deployment constraints.\n- The mixture ratio in CPT datasets (60:25:15) balances domain specificity and general language proficiency, preventing overfitting on binary data.\n- Static analysis results are integrated directly into prompts to overcome LLM context window limitations and improve reasoning about inter-function relationships.\n- The CoT mechanism is explicitly embedded during fine-tuning to promote transparency and make model predictions interpretable, addressing typical black-box issues in neural binary analysis.\n\n---\n\n### Significance and Connections\n\nReCopilot\u2019s methodology represents a **novel synthesis of domain-specific training, enhanced context integration, and reasoning-focused fine-tuning** in binary analysis. Unlike prior models that focus on single tasks like decompilation or variable name prediction, ReCopilot supports a broad multi-task framework critical for realistic reverse engineering scenarios. The generator-discriminator framework for chain-of-thought data is an innovative solution to the scarcity of reasoning annotations in this domain.\n\nBy demonstrating state-of-the-art performance (13% gains over current domain models) with a modestly sized model suitable for offline use, ReCopilot bridges the gap between large, computationally expensive LLMs and practical binary analysis tools. Its approach aligns with emerging trends in specialized AI model training\u2014fine-tuning pre-trained LLMs on domain datasets enriched with symbolic program analysis and structured outputs.\n\nThis work connects the AI programming language model community with cybersecurity and binary analysis fields, advancing towards automated, interpretable, and scalable reverse engineering assistance. Future extensions might explore even longer chain-of-thought reasoning and tighter integration with dynamic analysis for enriched program understanding[pages 3\u20138].\n\n---\n\nBy systematically building from data collection, through multi-stage training and innovative dataset synthesis, ReCopilot\u2019s methodology exemplifies a comprehensive approach to crafting expert LLMs tailored for the challenging binary analysis domain.", "citations": ["https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-overview", "https://learn.microsoft.com/en-us/copilot/microsoft-365/copilot-tuning-overview", "https://labs.zenity.io/p/inside-microsoft-365-copilot-technical-breakdown", "https://jannikreinhard.com/2023/12/11/deep-dive-into-co-pilots-understanding-architecture-llms-and-advanced-concepts/", "https://www.vamsitalkstech.com/ai/the-copilot-pattern-an-architectural-approach-to-ai-assisted-software/"], "page_number": 3, "subsections": [{"id": "overview-of-model-building-process", "title": "Overview of Model Building Process", "content": "## Overview of Model Building Process\n\nThis section provides a detailed explanation of the model building stages employed in training ReCopilot, an expert large language model (LLM) tailored for binary code analysis. The process encompasses three sequential phases\u2014continued pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO)\u2014each with specific objectives and datasets designed to enhance the model\u2019s domain knowledge, reasoning ability, and output consistency. Understanding these stages is crucial because they collectively enable the model to tackle complex binary analysis tasks effectively, overcoming the challenges posed by stripped binaries and limited symbolic information, which are common in real-world reverse engineering scenarios.\n\nThis discussion situates the ReCopilot approach within the broader research landscape of specialized LLM training, highlighting its methodological innovations and practical implications. Section 3.1 and Figure 2 on page 3 of the paper provide a visual roadmap of these stages and the related datasets, which we will elaborate throughout this overview.\n\n---\n\n### Core Methodology\n\nThe ReCopilot model building involves three core training stages, each addressing a distinct aspect of model capability:\n\n1. **Continued Pretraining (CPT):**\n\n   CPT aims to inject domain-specific knowledge into a generic pretrained base model by further training it on a carefully curated dataset rich in binary and decompiled pseudo code data. This stage adapts the general language model to the syntax and semantics of binary code, critical for understanding stripped binaries where symbolic information is missing.\n\n   At its core, CPT follows a **next-token prediction task**, where the model learns to predict the next token in sequences constructed from three modalities: pseudo code from stripped binaries, corresponding source code, and natural language comments (Figure 3, page 5). The dataset incorporates inner-shuffling of these segments to enforce bidirectional learning of the relationships among binary code, source code, and comments, thereby fostering a robust internal representation of binary code context.\n\n   Mathematically, given a sequence of tokens $(t_1, t_2, ..., t_n)$, the training objective maximizes the likelihood:\n   $$\n   \\max_{\\theta} \\sum_{i=1}^n \\log P_\\theta(t_i | t_{1:i-1})\n   $$\n   where $P_\\theta$ is the model\'s predicted probability parameterized by $\\theta$.\n\n2. **Supervised Fine-Tuning (SFT):**\n\n   The SFT phase adapts the pretrained base model to specific binary analysis tasks by training on labeled examples with explicit input-output mappings. The key goal is to enhance the model\u2019s instruction following, reasoning, and multi-task performance.\n\n   The authors identify 14 fundamental binary analysis tasks, such as function name recovery, variable type inference, algorithm identification, and detailed binary function analysis. Each task uses a consistent input-output format, as shown in Figure 4 (page 6), where the model receives the decompiled pseudo code along with enhanced context information\u2014including call chains and data flow analysis\u2014and produces predictions in structured JSON format.\n\n   To encourage deep reasoning, the training incorporates *chain-of-thought* (CoT) reasoning processes, where the model generates intermediate thought steps before final prediction. This is enabled via a *generator-discriminator* data synthesis framework (section 3.3.3), which autonomously creates reasoning-enriched training data rather than relying on imperfect external LLM outputs.\n\n3. **Direct Preference Optimization (DPO):**\n\n   The final training stage focuses on improving the model\u2019s consistency in reasoning and strict adherence to output formats\u2014critical for practical usability in reverse engineering tools. DPO is a reinforcement learning-like method that optimizes the model based on preference data comprising pairs of preferred (chosen) and less desired (rejected) outputs for the same input prompt.\n\n   Unlike traditional RL from human feedback (RLHF) requiring extensive human annotations and reward functions, DPO directly learns from these pairwise preferences, making it computationally efficient and scalable. This improves output quality by penalizing format errors and inconsistent reasoning paths, resulting in better prediction usability.\n\n---\n\n### Technical Details\n\n**Dataset Construction:** The training process depends heavily on carefully engineered datasets (section 3.3). The raw dataset includes over 101 million binary functions collected from various pipelines: compiling from source, off-the-shelf software artifacts, and automated compilation via the CompileAgent framework (Table 1, page 5). The data is sanitized and deduplicated using MinHash, ensuring high-quality training material.\n\n**Pretraining Dataset (page 5):** The CPT dataset consists of approximately 36 billion tokens, divided into 60% binary-focused pseudo code, 25% general programming code, and 15% natural language. This balanced mixture prevents overfitting while emphasizing binary domain knowledge (Table 2, page 5).\n\n**SFT Dataset and Generator-Discriminator Framework (page 6):** To generate reasoning-rich supervised data, the authors design a framework where a *generator* model proposes candidate reasoning sequences and predictions, while a *discriminator* evaluates their correctness and quality, filtering out suboptimal outputs. This procedure iteratively refines the dataset without manual human annotation.\n\n**Input-Output Template:** As illustrated in Figure 4, the input includes:\n\n\`\`\`\n<context-pseudocode>\n{context functions}\n</context-pseudocode>\n<pseudocode>\n{target function}\n</pseudocode>\n<Call-Chains>\n{call chains}\n</Call-Chains>\n<Data-Flow>\n{data flow}\n</Data-Flow>\nAnalysis Task Tag:\n{task tag}\n<Thought>\nThinking...\n</Thought>\n<Output>JSON format</Output>\n\`\`\`\n\nThe output is a formal JSON prediction with detailed fields relevant to the task (page 6).\n\n**Training Algorithms:**\n\nCPT uses a standard autoregressive language modeling loss:\n\n\`\`\`latex\n\\mathcal{L}_{CPT} = -\\sum_{i=1}^n \\log P_\\theta(t_i | t_{1:i-1})\n\`\`\`\n\nSFT optimizes supervised cross-entropy loss over task-specific examples with both reasoning and final output targets.\n\nDPO applies a preference-based loss defined as:\n\n\`\`\`latex\n\\mathcal{L}_{DPO} = -\\log \\sigma\\left( s_\\theta(\\text{chosen}) - s_\\theta(\\text{rejected}) \\right)\n\`\`\`\n\nwhere $s_\\theta$ denotes the model\'s score for an output, and $\\sigma$ is the logistic sigmoid function. This encourages the model to prefer chosen samples over rejected ones for identical inputs.\n\n---\n\n### Significance and Connections\n\nThe three-stage training approach of ReCopilot represents a novel and holistic methodology tailored for the unique challenges of binary code analysis. Injecting domain knowledge via CPT addresses the underfitting issue of general LLMs on stripped binaries\u2014a problem that prior general-purpose models struggle with. The multi-task SFT with chain-of-thought reasoning enables the model to handle complex, reasoning-intensive tasks beyond simple symbol recovery, a significant advance over single-task prior works.\n\nMoreover, the utilization of DPO for output consistency and format adherence ensures that ReCopilot\u2019s predictions are reliable and usable in downstream reverse engineering workflows. Unlike costly human feedback loops, DPO offers an efficient way to optimize user-preference alignment.\n\nThis process connects with broader trends in specialized LLM training, emphasizing the importance of domain adaptation, task-specific fine-tuning, and preference learning to push LLM capabilities beyond general language understanding into expert application domains (as supported by research on LLM training phases [5], page 3).\n\nUltimately, ReCopilot\u2019s model building innovations contribute a blueprint for creating small yet highly competent domain expert LLMs, advancing the field of AI-assisted binary analysis and reverse engineering.\n\n---\n\n*References to paper pages and figures:*\n\n- Model building overview and Figure 2: page 3\n- Pretraining dataset and Figure 3: page 5\n- SFT tasks and input-output template Figure 4: page 6\n- Dataset statistics and Table 1, Table 2: page 5\n- Training strategy and DPO explanation: pages 3\u20134\n\nThis structured model building process equips ReCopilot with strong domain knowledge, reasoning capacity, and output quality, enabling state-of-the-art performance in diverse binary analysis tasks.", "citations": ["https://lumenalta.com/insights/7-stages-of-ml-model-development", "https://www.simplilearn.com/tutorials/machine-learning-tutorial/machine-learning-steps", "https://domino.ai/blog/what-is-machine-learning-model-training", "https://www.alkymi.io/resources/blog/the-5-steps-in-our-ml-model-training-process", "https://snorkel.ai/blog/large-language-model-training-three-phases-shape-llm-training/"], "page_number": 3}, {"id": "training-strategies-cpt-sft-dpo", "title": "Training Strategies: Continued Pretraining, Supervised Fine-Tuning, and Direct Preference Optimization", "content": "Here is a comprehensive educational breakdown of the training strategy section, as requested, following advanced academic writing and KaTeX formatting standards.\n\n---\n\n## Understanding the Training Strategies: Continued Pretraining, Supervised Fine-Tuning, and Direct Preference Optimization\n\nThis section explains the core training strategies used in the ReCopilot research: continued pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO). By following the three-stage training pipeline, ReCopilot adapts a general-purpose language model (LLM) into a specialized tool for binary analysis, achieving state-of-the-art performance and practical deployability.\n\n**Why This Matters**\n\nUnderstanding these training strategies is essential for appreciating how modern LLMs are specialized for expert tasks like binary analysis. These stages\u2014CPT for domain adaptation, SFT for task-specific reasoning, and DPO for output quality\u2014are key to overcoming limitations of general models and making AI more useful in high-stakes security applications. The combination of these methods, as detailed in the paper, results in a model that outperforms existing tools by 13% and supports multiple complex analyses in a single deployment[arXiv:2505.16366v1, p.3].\n\n---\n\n## Core Content: Three Pillars of Training\n\n### 1. Continued Pretraining (CPT)\n\n**Definition and Purpose**\n\nContinued pretraining is the process of further training a pre-existing LLM on domain-specific data, aligning it more closely with the target application. In the case of ReCopilot, CPT introduces the model to decompiled pseudo code, source code, and natural language descriptions, helping it learn mappings between these forms. This stage is foundational: it teaches the LLM the vocabulary and structure of binary analysis before fine-tuning on specific tasks[arXiv:2505.16366v1, p.3-4].\n\n**Technical Details**\n\n- **Data Mixture:** The pretraining dataset is carefully balanced, with 60% binary domain data, 25% general code, and 15% natural language text (see Table 2 on p.6).\n- **Inner Shuffling:** To promote bidirectional understanding, data samples are shuffled to include pseudo code, source code, and comments in varied orderings (see Figure 3).\n- **Objective:** The model is trained to predict the next token in a sequence, learning to translate between binary, source, and natural language.\n\n**Mathematical Foundation**\n\nPretraining minimizes the cross-entropy loss for next-token prediction:\n\n$$\n\\mathcal{L}_{\\text{pretrain}} = -\\sum_{i=1}^N \\log p(x_i | x_1, ..., x_{i-1})\n$$\n\nwhere $x_i$ is the $i$-th token in the sequence, and $N$ is the sequence length[arXiv:2505.16366v1, p.5].\n\n---\n\n### 2. Supervised Fine-Tuning (SFT)\n\n**Definition and Purpose**\n\nAfter CPT, the model is fine-tuned on task-specific datasets using supervised learning. In ReCopilot, SFT is enhanced with chain-of-thought (CoT) reasoning: the model learns to generate step-by-step explanations before providing the final answer. This is crucial for complex reasoning tasks like binary function analysis, where understanding program logic is as important as the final output[arXiv:2505.16366v1, p.4][2].\n\n**Technical Details**\n\n- **Task Variety:** SFT covers 14 binary analysis tasks, such as function name recovery, variable type inference, and code summarization (see p.6-7).\n- **Context Enhancement:** Inputs include not just the target function, but also call chains and data flow information from static program analysis, enriching model context (see Figure 4).\n- **Generator-Discriminator Framework:** To automate the creation of CoT examples, a generator produces reasoning steps while a discriminator filters out incorrect or low-quality explanations. This approach ensures high-quality SFT data without manual annotation (see Figure 5)[arXiv:2505.16366v1, p.7].\n\n**Example: Chain-of-Thought in Action**\n\nConsider the task of recovering a function name from decompiled pseudo code. The model is trained to think:\n\n1. **Analyze the pseudo code structure.**\n2. **Identify key operations and data types.**\n3. **Compare with known patterns from source code.**\n4. **Propose possible function names.**\n5. **Select the most plausible name based on reasoning.**\n\nThis process is explicitly included in the training data, enabling the model to generalize to new, unseen functions[1][3].\n\n**Mathematical Formulation**\n\nSFT optimizes the supervised loss:\n\n$$\n\\mathcal{L}_{\\text{SFT}} = -\\sum_{(x, y)} \\log p(y | x)\n$$\n\nwhere $x$ is the input (pseudo code + context), and $y$ is the desired output (reasoning steps + final answer)[arXiv:2505.16366v1, p.5].\n\n---\n\n### 3. Direct Preference Optimization (DPO)\n\n**Definition and Purpose**\n\nDPO is a reinforcement learning technique that refines the model\u2019s outputs by learning from pairs of preferred and rejected responses, rather than requiring explicit human feedback. In ReCopilot, DPO ensures that the model\u2019s outputs are not only correct but also well-formatted and logically consistent[arXiv:2505.16366v1, p.4][5].\n\n**Technical Details**\n\n- **Data:** Pairs of \u201cchosen\u201d (preferred) and \u201crejected\u201d responses are generated for the same prompt (see p.4).\n- **Objective:** The model is trained to increase the likelihood of the preferred response and decrease the likelihood of the rejected one.\n- **Advantage:** DPO is more efficient and scalable than traditional reinforcement learning from human feedback (RLHF), requiring less compute and no manual reward engineering[5].\n\n**Mathematical Formulation**\n\nLet $x$ be the input, and $(y^+, y^-)$ be a pair of chosen and rejected outputs. The DPO loss is:\n\n$$\n\\mathcal{L}_{\\text{DPO}} = -\\log\\sigma\\left(\\beta \\log\\frac{p(y^+ | x)}{p(y^- | x)}\\right)\n$$\n\nwhere $\\sigma$ is the sigmoid function, and $\\beta$ is a hyperparameter controlling the strength of the preference[arXiv:250516366v1, p.4].\n\n---\n\n## Technical Implementation Details\n\n### Continued Pretraining: Building the Foundation\n\n- **Dataset Construction:** Raw binary functions are collected from open-source packages, off-the-shelf software artifacts, and automated compilation tools (see Table 1).\n- **Sanitization:** Functions are filtered by length, duplicates are removed using MinHash, and auxiliary functions are excluded to reduce noise (see p.5).\n- **Pretraining Setup:** The model is trained on shuffled samples of pseudo code, source code, and comments, promoting robust cross-modal understanding.\n\n### Supervised Fine-Tuning: Enabling Reasoning\n\n- **Task Templates:** Each SFT example is structured as a modular input-output pair, including context functions, call chains, data flow, task tag, reasoning process, and final answer (see Figure 4).\n- **Chain-of-Thought Generation:** The generator-discriminator framework ensures that reasoning steps are high quality and relevant to the task, without manual annotation (see Figure 5).\n\n### Direct Preference Optimization: Refining Outputs\n\n- **DPO Dataset:** Contains 2.4K examples of preferred vs. rejected outputs (see p.4).\n- **Training Efficiency:** DPO can be run with limited compute resources, making it practical for deployment.\n\n---\n\n## Significance and Connections\n\n### Why This Approach is Novel\n\n- **Domain-Specific Adaptation:** ReCopilot is one of the first models to apply CPT, SFT, and DPO together for binary analysis, enabling a single model to handle multiple complex tasks.\n- **Automated Reasoning:** The use of chain-of-thought reasoning and the generator-discriminator framework automates the creation of high-quality training data, reducing the need for manual annotation[arXiv:2505.16366v1, p.7].\n- **Practical Deployability:** By carefully balancing model size and training efficiency, ReCopilot can be deployed on standard hardware, making it accessible to security researchers and practitioners.\n\n### Connections to Broader Research\n\n- **Related Work:** Previous efforts focused on single tasks or lacked robust reasoning capabilities. ReCopilot\u2019s multi-task, reasoning-enabled approach is a significant advance over existing methods[arXiv:2505.16366v1, p.3-4].\n- **Implications:** The success of ReCopilot suggests that specialized LLMs, trained with advanced strategies like CPT, SFT, and DPO, can outperform both general models and traditional tools in complex, real-world domains.\n\n---\n\n## Summary Table: Training Stages in ReCopilot\n\n| Stage        | Purpose                                   | Key Features                                 | Data Source/Structure         |\n|--------------|-------------------------------------------|----------------------------------------------|------------------------------|\n| CPT          | Domain adaptation                         | Pseudo code, source code, natural language   | 36B tokens, 60:25:15 mix     |\n| SFT          | Task-specific reasoning                   | Chain-of-thought, context enhancement        | 1.7B tokens, 14 tasks        |\n| DPO          | Output quality and consistency            | Preferred vs. rejected responses             | 2.4K examples                |\n\n---\n\n## Example Code: DPO Training Loop Pseudocode\n\n\`\`\`python\nfor batch in dpo_dataset:\n    x, y_plus, y_minus = batch\n    logits_plus = model(x, y_plus)\n    logits_minus = model(x, y_minus)\n    loss = -log_sigmoid(beta * (logits_plus - logits_minus))\n    optimizer.step(loss)\n\`\`\`\nThis pseudocode illustrates the core DPO training loop, as described in the paper[arXiv:2505.16366v1, p.4].\n\n---\n\n## Key Takeaways\n\n- **Continued Pretraining** adapts a general LLM to the binary domain by learning cross-modal mappings.\n- **Supervised Fine-Tuning** equips the model with chain-of-thought reasoning and context awareness for complex tasks.\n- **Direct Preference Optimization** refines output quality using a scalable, feedback-efficient reinforcement learning method.\n- **ReCopilot\u2019s training pipeline** is a novel combination of these strategies, resulting in a highly effective and deployable model for binary analysis[arXiv:2505.16366v1, p.3-4].\n\nThis approach not only advances the state of the art in binary analysis but also provides a blueprint for how to specialize LLMs for other expert domains.", "citations": ["https://www.youtube.com/watch?v=Fkj1OuWZrrI", "https://www.splunk.com/en_us/blog/learn/chain-of-thought-cot-prompting.html", "https://www.invisible.co/blog/how-to-teach-chain-of-thought-reasoning-to-your-llm", "https://arxiv.org/html/2411.15382v1", "https://bdtechtalks.substack.com/p/bytedances-new-fine-tuning-technique"], "page_number": 4}, {"id": "dataset-building-and-generator-discriminator-framework", "title": "Dataset Building and Generator-Discriminator Framework for SFT", "content": "## Introduction\n\nThis section explains how the **dataset for supervised fine-tuning (SFT)** is constructed and introduces the **generator-discriminator framework** used to automatically generate high-quality training examples for binary analysis tasks. These concepts are foundational for understanding how ReCopilot, an expert large language model (LLM) for binary analysis, achieves strong performance on tasks like function name recovery, variable type inference, and code summarization. By detailing the pipeline for data collection, filtering, and synthetic reasoning generation, this section provides essential context for how training data quality and reasoning capabilities are ensured\u2014critical for advancing the state of binary analysis automation[arXiv:2505.16366v1, page 4\u20136].\n\n**Why is this important?**  \nIn machine learning, especially for domains like binary analysis, the quality and diversity of training data directly influence model performance. However, collecting human-annotated reasoning data is costly and often insufficient for training complex models. The novel approach of using a generator-discriminator framework\u2014instead of manual annotation\u2014enables scalable, automatic creation of high-quality training examples with chain-of-thought reasoning, which is crucial for fine-tuning LLMs to reason about complex binary code[arXiv:2505.16366v1, page 7].\n\n**How does this fit into the broader research?**  \nThis section bridges the raw data collection (covered in \u00a73.3.1) and the model training pipeline (covered in \u00a73.2). It explains the technical and methodological choices that address key challenges in binary analysis, such as limited availability of annotated reasoning data and the need for scalable, robust training datasets[arXiv:2505.16366v1, page 6]. The innovations here directly support the model\u2019s high accuracy and interpretability, distinguishing ReCopilot from previous work.\n\n---\n\n## Core Content\n\n**Dataset Construction Overview**\n\nThe dataset construction process begins with large-scale raw data collection from multiple pipelines, including compilation from scratch, off-the-shelf software artifacts, and automated compilation by CompileAgent. This results in a massive dataset of over 101 million binary functions from 11,000 projects, as detailed in Table 1 on page 6. Data undergoes rigorous filtering and deduplication to remove noise (e.g., short/long functions, thunk functions, and duplicates via MinHash), ensuring high-quality, representative samples[arXiv:2505.16366v1, page 6].\n\n**Supervised Fine-Tuning (SFT) Tasks**\n\nFor SFT, 14 common binary analysis tasks are defined, such as function name recovery, variable type inference, algorithm identification, and code summarization. Each task is designed to address a core challenge in binary analysis, and multiple task formulations allow for transfer learning between tasks[arXiv:2505.16366v1, page 7]. The input-output template for these tasks is modular and flexible, shown in Figure 4 on page 7, and includes:\n\n- **Target function (pseudocode)**\n- **Context functions**\n- **Call chains**\n- **Data flow**\n- **Task tag**\n- **Reasoning process (Thought)**\n- **Final prediction (JSON output)**\n\nThis structure enables the model to leverage contextual information for more accurate and interpretable predictions.\n\n**Chain-of-Thought (CoT) Reasoning**\n\nChain-of-thought prompting is a technique that guides AI models through step-by-step reasoning, making the model\u2019s thought process transparent and verifiable[2][4]. For binary analysis tasks, this means the model must not only produce an answer but also generate a reasoning chain explaining how it arrived at the answer. CoT is particularly effective for complex, multistep problems and aligns well with the reasoning-intensive nature of binary analysis[arXiv:2505.16366v1, page 7]. The CoT process can be visualized as follows:\n\n\`\`\`\n<Thought>\nStep 1: Identify function boundaries in pseudocode.\nStep 2: Match decompiled variables with source counterparts using debug info.\nStep 3: Infer variable types and names.\n...\n</Thought>\n<Output>\n{ \"function_name\": \"foo\", \"variable_names\": {\"a1\": \"input\", ...}, ... }\n</Output>\n\`\`\`\n\n**Generator-Discriminator Framework**\n\nA major challenge in building the SFT dataset is the lack of human-annotated reasoning chains. To address this, the authors propose a **generator-discriminator framework** (illustrated in Figure 5, page 8), which automatically synthesizes high-quality training examples by leveraging two components:\n\n- **Generator:** Produces candidate reasoning chains for each training example.\n- **Discriminator:** Evaluates and selects the highest-quality reasoning chains, ensuring that only plausible and accurate reasoning is included in the dataset.\n\nThis framework is inspired by adversarial data synthesis methods[arXiv:2505.16366v1, page 8] and operationalized as follows:\n\n1. **Collect raw SFT data** (input, output without CoT, source code, metadata).\n2. **Use the generator** (e.g., a pre-trained LLM) to propose reasoning chains for each example.\n3. **Apply the discriminator** (e.g., a verification model or rule-based filter) to select the best reasoning chains.\n4. **Assemble the final dataset** by pairing input-output examples with validated reasoning chains.\n\nThis approach overcomes the limitations of manual annotation and allows for scalable generation of high-quality reasoning data.\n\n**Super-CoT: Extending Reasoning Chains**\n\nTo further enhance the model\u2019s reasoning capability, a **Super-CoT** technique is introduced, which generates longer, stepwise reasoning chains. This helps the model to tackle more complex tasks and provides deeper insights into the reasoning process[arXiv:2505.16366v1, page 7]. Super-CoT is especially important for tasks requiring multiple inference steps or contextual understanding across function boundaries.\n\n---\n\n## Technical Details\n\n**Implementation of the Generator-Discriminator Framework**\n\nThe generator-discriminator framework is implemented as follows:\n\n1. **Raw Data Collection:**  \n   Input-output pairs are extracted from the raw dataset, linked via debug information. Each pair includes pseudocode, source code, and metadata.\n2. **Reasoning Chain Generation:**  \n   The generator (e.g., a fine-tuned LLM) is prompted to produce reasoning chains for each task. For example:\n   \`\`\`\n   Given the pseudocode for function sub_1909, infer the original function name and variable types.\n   \`\`\`\n   The generator outputs a reasoning chain in natural language.\n3. **Discrimination and Selection:**  \n   The discriminator evaluates the reasoning chains for logical consistency, factual accuracy, and relevance to the task. This can involve a separate verification model or handcrafted rules.\n4. **Dataset Assembly:**  \n   Validated reasoning chains are paired with the original input-output pairs to form the final SFT dataset.\n\n**Pseudocode for the Framework**\n\n\`\`\`python\ndef generate_sft_example(raw_data, generator, discriminator):\n    # Input: raw_data (pseudocode, source code, metadata)\n    # Step 1: Generate reasoning chain\n    reasoning_chain = generator.generate_reasoning(raw_data)\n    # Step 2: Filter reasoning chain\n    if discriminator.is_valid(reasoning_chain):\n        return {\n            \"input\": raw_data[\"pseudocode\"],\n            \"context\": raw_data[\"context\"],\n            \"task\": raw_data[\"task\"],\n            \"thought\": reasoning_chain,\n            \"output\": raw_data[\"output\"]\n        }\n    else:\n        return None\n\`\`\`\n\n**Design Choices and Parameter Selection**\n\n- **Generator Model:** A pre-trained LLM fine-tuned on binary and source code data is used to ensure domain-specific reasoning.\n- **Discriminator Model:** Can be a smaller LLM or a rule-based system, depending on the task complexity and available resources.\n- **Quality Control:** Only reasoning chains that pass the discriminator\u2019s checks are included, minimizing noise and bias in the dataset.\n- **Scalability:** The framework is designed to process millions of examples efficiently, leveraging parallel computation and automated pipelines[arXiv:2505.16366v1, page 8].\n\n**Integration with Model Training**\n\nThe resulting SFT dataset is used to fine-tune the base LLM, enabling it to generate both accurate predictions and interpretable reasoning chains. The modular input-output template (Figure 4, page 7) ensures that the model can handle a variety of binary analysis tasks and leverage contextual information for improved performance.\n\n---\n\n## Significance & Connections\n\n**Novelty and Importance**\n\nThe generator-discriminator framework represents a significant advance in dataset construction for binary analysis. By automating the synthesis of reasoning chains, it addresses the key bottleneck of limited human-annotated reasoning data, enabling scalable, high-quality training for LLMs[arXiv:2505.16366v1, page 8]. This approach is particularly important for domains where manual annotation is impractical or prohibitively expensive.\n\n**Broader Research Context**\n\nThis work connects to broader trends in AI, such as the use of chain-of-thought prompting to enhance model interpretability and reasoning[1][2]. It also builds on recent advances in data synthesis and adversarial learning, demonstrating how these techniques can be applied to complex, real-world problems in cybersecurity and software engineering.\n\n**Key Innovations and Contributions**\n\n- **Scalable, automatic generation of reasoning data** for binary analysis tasks\n- **Integration of static program analysis** (call chains, data flow) for context-aware reasoning\n- **Super-CoT technique** for deeper, stepwise reasoning chains\n- **Modular, flexible input-output template** for diverse task support[arXiv:2505.16366v1, page 7]\n\n**Implications for the Field**\n\nThis approach lowers the barrier to building expert models for binary analysis, making advanced AI tools more accessible to security professionals. By providing interpretable reasoning and supporting a wide range of tasks, ReCopilot demonstrates the potential of domain-specific LLMs to transform reverse engineering and vulnerability analysis[arXiv:2505.16366v1, page 8].\n\n---\n\n## Summary Table\n\n| Feature                        | Description                                                                 |\n|---------------------------------|-----------------------------------------------------------------------------|\n| Data Source                    | 101M binary functions from 11K projects, multiple pipelines                 |\n| SFT Tasks                      | 14 tasks: function name recovery, variable type inference, summarization, etc|\n| Input-Output Template           | Modular, includes context, call chains, data flow, task tag, reasoning, JSON|\n| Generator-Discriminator         | Automatically generates and validates reasoning chains                       |\n| Super-CoT                      | Generates longer, stepwise reasoning chains for complex tasks                |\n| Key Innovation                 | Scalable, automatic reasoning data synthesis                                 |\n\n---\n\n## Connections to Other Sections\n\nThis section is closely linked to the **pretraining dataset construction** (covered earlier in \u00a73.3.1) and the **model training pipeline** (covered in \u00a73.2). The quality and diversity of the SFT dataset directly impact the model\u2019s ability to perform well on downstream tasks, and the generator-discriminator framework ensures that the model is equipped with robust reasoning capabilities, as demonstrated in the evaluation section[arXiv:2505.16366v1, page 6\u20139].", "citations": ["https://www.ibm.com/think/topics/chain-of-thoughts", "https://www.superannotate.com/blog/chain-of-thought-cot-prompting", "https://arxiv.org/abs/2503.15341", "https://www.f22labs.com/blogs/a-guide-on-chain-of-thought-cot-prompting/", "https://learnprompting.org/docs/advanced/thought_generation/automatic_chain_of_thought"], "page_number": 6}, {"id": "context-enhancement-using-static-program-analysis", "title": "Context Enhancement Using Static Program Analysis", "content": "## Context Enhancement Using Static Program Analysis\n\nThis section explores how ReCopilot overcomes the inherent limitations of large language models (LLMs)\u2014specifically the \u201ccontext window\u201d problem\u2014by leveraging static program analysis to enrich the information fed into the model. It is a central pillar of the paper\u2019s methodology, underpinning both the reasoning power and practical applicability of ReCopilot in binary analysis tasks. The approach is critical for enabling LLMs to handle complex, real-world binaries where source-level symbols are missing, and to provide accurate, context-aware results for challenges like function name recovery and variable type inference[3][4].\n\n### Introduction\n\n**Context Enhancement Using Static Program Analysis** is at the heart of ReCopilot\u2019s design. Traditional LLMs, even when trained on code, struggle because binary code analysis requires more than just semantic understanding: it demands an awareness of the program\u2019s control and data flow, and the ability to synthesize information from multiple functions and variables[2][3]. This section explains how ReCopilot augments LLMs with precise, actionable context\u2014a key innovation that sets it apart from previous approaches.\n\nThe importance of this topic, as demonstrated in the paper, lies in its direct impact on model accuracy and usability in real-world binary analysis. Without context enhancement, LLMs would be restricted to analyzing isolated functions, missing critical clues about variable usage, control flow, and program logic that span multiple functions and files (see \u00a73.4, page 7). The broader research context, as discussed in \u00a72 and \u00a73, shows that previous methods either failed to leverage multi-function context or were overwhelmed by the complexity and noise of analyzing entire repositories[2][5].\n\n### Core Content\n\nStatic program analysis is the process of examining code without executing it, aiming to infer properties about its behavior. In ReCopilot, this analysis is applied to decompiled pseudo code to extract two main types of context: **calling function chains** and **detailed variable data flow**.\n\n**Call Chain Analysis:**\nTo ensure the LLM has access to the most relevant context without overwhelming its limited context window, ReCopilot performs a bidirectional breadth-first search (BFS) on the program\u2019s call graph. This traversal identifies all functions that directly or indirectly call the target function (callees) or are called by it (callers). Not all functions are equally informative, so each function is assigned an *informative score* computed as follows (see Equation 2, page 9):\n\n\\[\n\\text{Score}(f) = w_1 \\cdot \\text{NameMeaningfulness}(f) + w_2 \\cdot \\text{StringDensity}(f) + w_3 \\cdot \\text{CalleeSemantics}(f)\n\\]\n\nHere, $w_1, w_2, w_3$ are weights (empirically determined), and the three features are:\n- **NameMeaningfulness**: How descriptive or human-readable the function name is (often low for stripped binaries, but can be inferred from usage).\n- **StringDensity**: The prevalence of string literals in the function, a proxy for semantic significance.\n- **CalleeSemantics**: The semantic richness of functions called within $f$.\n\nThis score prioritizes the inclusion of functions that are most likely to provide useful context for understanding the target function\u2019s purpose and behavior.\n\n**Variable Data Flow Analysis:**\nStatic analysis is also used to trace how variables propagate between functions. This is crucial for understanding complex data structures and inferring types, especially since stripped binaries lack source-level variable names and types (see Figure 1, page 4). ReCopilot uses custom abstract syntax tree (AST) traversals with inference rules to track variable definitions, uses, and aliases across function boundaries (as detailed in Figure 7, page 10). This enables the model to \u201csee\u201d where a variable originates and how it is transformed as it flows through the program.\n\n**Example:**\nConsider a function $f$ that takes a pointer to a struct. In the decompiled code, this may appear as a generic pointer (e.g., \`void*\`). By analyzing the data flow, ReCopilot can detect that the pointer is passed to a helper function that initializes the struct, and to another function that uses its fields. This context allows the model to infer the underlying structure and type information, even though it is not explicitly present in the binary.\n\n### Technical Details\n\nReCopilot\u2019s context enhancement is implemented as a pre-processing step before feeding data to the LLM. The following algorithm outlines the key steps (see \u00a73.4, pages 7\u201310):\n\n\`\`\`\n1. Decompile the binary function to pseudo code using tools like IDA Pro.\n2. Build the call graph for the binary.\n3. Perform a bidirectional BFS on the call graph from the target function.\n4. Score each reachable function using Equation 2 (p. 9).\n5. Select the top-K highest-scoring functions as context.\n6. Perform AST traversal and data flow analysis to track variable definitions and uses.\n7. Collect variable aliases and usages across function boundaries using inference rules.\n8. Assemble all context into a structured prompt for the LLM, as shown in Figure 4 (p. 8).\n\`\`\`\n\n**Parameter Choices:**  \nThe value of $K$ (number of top-scoring context functions) is chosen to fit within the LLM\u2019s context window, balancing information richness and computational feasibility. The weights $w_1, w_2, w_3$ are calibrated on a validation set to maximize the model\u2019s task performance.\n\n**Design Decisions:**  \nThe use of AST-based inference rules for data flow analysis is motivated by the need for precision in tracking complex variable relationships. The bidirectional BFS ensures that both upstream (callers) and downstream (callees) context are considered, providing a holistic view of the target function\u2019s role and behavior.\n\n### Significance & Connections\n\n**Novelty and Importance:**\nReCopilot\u2019s context enhancement approach is novel in its use of static program analysis to systematically extract and prioritize context, rather than relying on ad-hoc or overly broad context selection. This addresses the critical limitation of LLM context windows and enables more accurate reasoning about binary code, as demonstrated by the model\u2019s state-of-the-art performance on multiple tasks (see \u00a75, pages 12\u201314).\n\n**Broader Research Context:**\nThis work connects to emerging trends in software engineering and security, where LLMs are increasingly used to assist with code understanding, vulnerability detection, and reverse engineering[2][3][5]. By showing how static analysis can be used to \u201cfocus\u201d LLMs on the most relevant information, ReCopilot provides a template for future research at the intersection of program analysis and AI.\n\n**Key Innovations:**\n- **Context Prioritization:** The informative score (Equation 2, p. 9) ensures that only the most relevant context is included, addressing LLM context length limits.\n- **Data Flow Integration:** Custom AST traversals and inference rules enable the model to reason about variable types and structures even in stripped binaries (Figure 7, p. 10).\n- **Scalable Prompt Assembly:** The structured prompt format (Figure 4, p. 8) is modular and extensible, supporting a wide range of binary analysis tasks.\n\n**Implications:**  \nThis approach enables LLMs to serve as effective \u201ccopilots\u201d for reverse engineers, providing accurate, context-aware assistance in understanding and analyzing stripped binaries\u2014a long-standing challenge in the field of cybersecurity[3][5].\n\n---\n\n**Summary Table: Key Features of Context Enhancement in ReCopilot**\n\n| Feature                | Description                                                                 | Reference      |\n|------------------------|-----------------------------------------------------------------------------|----------------|\n| Call Chain Analysis    | Bidirectional BFS on call graph, prioritized by informative score           | Eq. 2, p. 9    |\n| Variable Data Flow     | AST traversal with inference rules for cross-function variable tracking     | Fig. 7, p. 10  |\n| Context Assembly       | Structured prompt with target function, context, call chains, and data flow | Fig. 4, p. 8   |\n| Task Performance       | SOTA results in function name and variable type inference                   | \u00a75, p. 12\u201314   |\n\n---\n\nBy systematically extracting and prioritizing context, ReCopilot sets a new standard for LLM-assisted binary analysis, bridging the gap between expert-driven and automated techniques in cybersecurity and reverse engineering[3][5].", "citations": ["https://arxiv.org/html/2503.05394v1", "https://arxiv.org/html/2504.16877v1", "https://insights.sei.cmu.edu/blog/evaluating-static-analysis-alerts-with-llms/", "https://research.ibm.com/publications/codellm-devkit-a-framework-for-contextualizing-code-llms-with-program-analysis-insights-tool-demo", "https://www.cs.ucr.edu/~zhiyunq/pub/oopsla24_llift.pdf"], "page_number": 8}]}, {"id": "results-and-performance-analysis", "title": "Results and Performance Analysis of ReCopilot", "content": "Below is a comprehensive, educational breakdown of the **Results and Performance Analysis of ReCopilot** section, designed for advanced researchers and graduate students with technical accuracy, clear illustrations, and contextual depth.\n\n---\n\n## Introduction\n\nThis section provides a thorough evaluation of ReCopilot, an expert large language model (LLM) designed specifically for multi-task binary analysis, including tasks such as function name recovery, variable and type inference, structure recovery, binary code summarization, and decompilation quality assessment. The section is critical because it demonstrates how well ReCopilot addresses challenges in binary code understanding\u2014typically hampered by a lack of symbolic information in stripped binaries\u2014by leveraging domain-specific training, context enhancement, and advanced evaluation protocols[1][3].\n\nUnderstanding this analysis is essential for appreciating how ReCopilot advances the state-of-the-art in binary reverse engineering and why its multi-task, context-aware approach offers significant improvements over existing tools and general-purpose LLMs. The section also situates ReCopilot within broader research on automating security analysis and code understanding, highlighting its contributions to both technology and methodology[1].\n\n---\n\n## Core Content\n\n### Key Evaluation Concepts\n\nReCopilot\u2019s performance is measured on a **multi-task binary analysis benchmark** that was constructed to reflect the most important and challenging aspects of real-world binary analysis. The benchmark is designed to be **automatic** and **extensible**, allowing comparisons across a diverse set of tasks and domains[1][2].\n\nThe selected tasks\u2014function name recovery, variable name and type prediction, structure recovery, binary code summarization, and decompilation quality\u2014are among the most critical for security analysts and reverse engineers. For example, **function name recovery** enables analysts to quickly understand a function\u2019s purpose even when debugging symbols have been stripped, while **variable type inference** is essential for reconstructing the original data structures used in the code[1][3].\n\n#### Evaluation Metrics\n\nReCopilot employs several advanced evaluation metrics, each capturing a different aspect of model performance:\n\n- **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation): Measures the overlap between generated and reference text, useful for code summarization accuracy.\n- **F1 Score**: Balances precision and recall, especially relevant for classification tasks (e.g., variable type prediction).\n- **CodeBLEU**: Extends BLEU to code, evaluating syntactic and semantic similarity between generated and reference code.\n- **LLM-Judged Summarization Scores**: Uses another LLM to assess the quality of generated summaries, mimicking human judgment.\n\nMathematically, these metrics can be expressed as:\n\n$$\n\\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\n\n$$\n\\text{CodeBLEU} = \\lambda_1 BLEU + \\lambda_2 \\text{SyntacticMatch} + \\lambda_3 \\text{SemanticMatch}\n$$\n\nwhere $\\lambda_1, \\lambda_2, \\lambda_3$ are weights, and \"SyntacticMatch\" and \"SemanticMatch\" are additional similarity scores for code structure and meaning[1].\n\n### Benchmark Design and Data Diversity\n\nThe benchmark pipeline (illustrated in Figure 8 of the paper) takes a binary as input and triggers an analysis plugin (e.g., IDA Pro) to evaluate performance on both function-level and file-level contexts, providing richer analytics than traditional function-only benchmarks[1]. The test set is drawn from multiple software domains, including **crypto libraries, network protocols, and multimedia codecs**, ensuring broad applicability and robust generalization[1].\n\nTo illustrate, consider the following example from the paper (see Figure 1, page 7):\n\n- **Decompiled Pseudo Code**: Lacks meaningful symbols, making semantic understanding challenging for analysts.\n- **Source Code**: Clearly annotated with function names, variable types, and comments, which are erased in binaries.\n\nThe benchmark\u2019s automatic extractor parses prediction results from IDA Pro\u2019s .idb files, enabling reproducible and scalable evaluation[1].\n\n---\n\n## Technical Details\n\n### Model Architecture and Training\n\nReCopilot is built through a three-stage process:\n\n1. **Continued Pretraining (CPT)**: Injects domain knowledge using a large-scale dataset of binary, source, and natural language code.\n2. **Supervised Fine-Tuning (SFT)**: Adapts the model to specific binary analysis tasks using a generator-discriminator framework that synthesizes reasoning examples.\n3. **Direct Preference Optimization (DPO)**: Further refines model outputs for consistency and format-following[1].\n\nThe SFT dataset is constructed using a **generator-discriminator framework** (detailed in \u00a73.3.3, page 11), which automates the creation of high-quality training examples by simulating the reasoning process a human expert might follow. This framework:\n\n- **Generates candidate reasoning paths** for each task.\n- **Discriminates between plausible and implausible reasoning** using automated filtering, ensuring only high-quality data is used for training[1].\n\n### Context Enhancement\n\nReCopilot\u2019s input template (Figure 4, page 11) includes not only the target binary function but also context functions, call chains, and data flow information. This **context enhancement** mimics how human analysts consider related functions and variable usage when reverse engineering code, providing the model with a richer understanding of the code\u2019s intent and structure[1].\n\n> **Example Input Template:**\n> \n> \`\`\`xml\n> <context-pseudocode>\n> {context}\n> </context-pseudocode>\n> <pseudocode>\n> {target_func}\n> </pseudocode>\n> <Call-Chains>\n> {call_chains}\n> </Call-Chains>\n> <Data-Flow>\n> {data_flow}\n> </Data-Flow>\n> Analysis Task Tag:\n> {task_tag}\n> <Thought>\n> Thinking...\n> </Thought>\n> <Output>JSON Format</Output>\n> \`\`\`\n>\n> This modular approach supports scalability and flexibility for additional tasks[1].\n\n---\n\n## Significance & Connections\n\n### Novelty and Contributions\n\nReCopilot\u2019s **multi-task, context-aware approach** represents a significant step forward in automating binary analysis. By integrating domain-specific knowledge, advanced data synthesis techniques, and context enhancement, it outperforms existing LLMs and domain models by an average of 13%, even with a smaller model size (7B parameters)[1][2]. This demonstrates that **specialized training and context awareness** are key to success in complex, reasoning", "citations": ["https://arxiv.org/html/2505.16366v1", "https://www.themoonlight.io/review/recopilot-reverse-engineering-copilot-in-binary-analysis", "https://arxiv.org/abs/2505.16366", "https://neurips.cc/virtual/2024/poster/97859", "https://pmc.ncbi.nlm.nih.gov/articles/PMC6572845/"], "page_number": 11, "subsections": [{"id": "benchmark-design-and-evaluation-metrics", "title": "Benchmark Design and Evaluation Metrics", "content": "Here is a comprehensive educational content section for **\"Benchmark Design and Evaluation Metrics\"** in the context of ReCopilot, following all principles and formatting requirements.\n\n---\n\n## Introduction to Benchmark Design and Evaluation Metrics\n\nThis section covers the design, implementation, and significance of the benchmark used to evaluate ReCopilot\u2019s performance across multiple binary analysis tasks. Understanding these metrics is crucial because they form the foundation for assessing whether the model\u2019s outputs are accurate, useful, and reliable for real-world reverse engineering scenarios[1][5]. The benchmark is not just a static set of tasks but a dynamic, extensible pipeline that integrates precise evaluators to measure performance across function, variable, and structural recovery, as well as code understanding and summarization.\n\nThe importance of this benchmark within the broader research is highlighted by its role in comparing ReCopilot\u2019s outputs to both traditional decompilers and advanced language models. By leveraging both classical and language model (LLM)-based metrics, the research establishes a standardized way to measure progress in automating and improving binary analysis\u2014a field highly dependent on expert knowledge and time-consuming manual processes (see Figure 8 on page 11).\n\n---\n\n## Core Concepts and Methodological Choices\n\n### Key Concepts Defined\n\n- **Benchmark**: A standardized test suite designed to evaluate and compare the performance of tools or models on specific tasks. In this context, it integrates multiple binary analysis tasks and their respective evaluators.\n- **Evaluation Metrics**: Quantitative measures used to assess the performance of a model or system. These include traditional NLP metrics (like ROUGE and F1) and coding-specific metrics (like CodeBLEU).\n- **Extensibility**: The ability of a benchmark to accommodate new tasks, metrics, or analysis domains, ensuring its continued relevance as the field evolves[5].\n\n### Mathematical Formulations\n\n#### ROUGE for Name Recovery\n\nROUGE metrics are commonly used for evaluating the quality of summaries or generated texts by comparing them to reference texts. In this benchmark, ROUGE is adapted for function and variable name recovery tasks. The ROUGE score for a set of predicted names $\\mathcal{P}$ and reference names $\\mathcal{R}$ is calculated as the recall, precision, and F1 over n-gram overlaps:\n\n$$\n\\text{ROUGE-1} = \\frac{|\\mathcal{P} \\cap \\mathcal{R}|}{|\\mathcal{R}|}, \\quad\n\\text{ROUGE-L} = \\frac{|\\text{LCS}(\\mathcal{P}, \\mathcal{R})|}{|\\mathcal{R}|}\n$$\n\nwhere $\\text{LCS}$ is the longest common subsequence.\n\n#### F1 Score for Struct Recovery\n\nThe F1 score is a balanced measure of precision and recall, essential for tasks where both false positives and false negatives are undesirable, such as struct recovery:\n\n$$\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\n\n#### CodeBLEU for Decompilation Accuracy\n\nCodeBLEU extends BLEU by incorporating code syntax and semantics, making it suitable for assessing decompilation quality:\n\n$$\n\\text{CodeBLEU} = \\alpha \\cdot \\text{BLEU} + \\beta \\cdot \\text{SyntaxMatch} + \\gamma \\cdot \\text{SemanticMatch} + \\delta \\cdot \\text{GraphMatch}\n$$\n\nwhere $\\alpha, \\beta, \\gamma, \\delta$ are weighting factors.\n\n#### LLM-Based Summarization Evaluator\n\nA four-dimensional evaluator assesses semantic coverage, accuracy, misleading information, and readability:\n\n$$\n\\text{Score} = \\frac{1}{4} \\left( \\text{Coverage} + \\text{Accuracy} - \\text{Misleading} + \\text{Readability} \\right)\n$$\n\nEach dimension is rated on a 0\u20131 scale, and the final score provides a holistic view of summarization quality.\n\n### Example and Reasoning\n\nSuppose a decompiler recovers a function from binary code. The benchmark would:\n\n1. **Extract predictions** for function names, variable names, types, and decompiled code.\n2. **Compare predictions** to ground truth using ROUGE (for names), F1 (for structs), and CodeBLEU (for code).\n3. **Summarize** the function and use the LLM-based evaluator to assess the quality of the summary.\n\nFor instance, if the decompiler recovers a function \`aes_encrypt\` and a variable \`ctx\` with type \`struct AES_ctx\`, the evaluators would check if these match the original source code (see Figure 1 on page 2 for an example).\n\n### Reasoning Behind Methodological Choices\n\n- **Multiple Metrics**: Different aspects of binary analysis (names, types, code, summaries) require different evaluation strategies. Using a combination ensures comprehensive assessment.\n- **Extensibility**: The benchmark is designed to easily add new tasks or metrics, reflecting the evolving needs of the binary analysis community[5].\n- **Automation**: Integrating with IDA Pro and using automatic prediction extraction reduces manual effort and increases reproducibility.\n\n---\n\n## Technical Implementation Details\n\n### Evaluation Pipeline\n\nThe benchmark\u2019s evaluation pipeline is illustrated in Figure 8 on page 11. Key components include:\n\n1. **Task Integration**: Multiple binary analysis tasks are defined, each with its own evaluator (e.g., name recovery, struct recovery, summarization).\n2. **Runtime Environment**: Predictions are extracted automatically using IDA Pro integration, ensuring a consistent and reproducible setup.\n3. **Metric Calculation**: For each task, the relevant metric (ROUGE, F1, CodeBLEU, LLM-based) is computed and aggregated.\n\n### Algorithms and Procedures\n\nBelow is a pseudocode outline of the evaluation loop for a single binary function:\n\n\`\`\`python\n# Pseudocode: Benchmark Evaluation Loop\ndef evaluate_binary_function(binary_func, ground_truth):\n    # 1. Extract predictions from decompiler/LLM\n    predictions = extract_predictions(binary_func, tool=\"Recopilot\")\n    \n    # 2. Calculate metrics for each task\n    name_scores = evaluate_names(predictions.names, ground_truth.names, metric=\"ROUGE-L\")\n    struct_scores = evaluate_structs(predictions.structs, ground_truth.structs, metric=\"F1\")\n    code_scores = evaluate_code(predictions.code, ground_truth.code, metric=\"CodeBLEU\")\n    summary_scores = evaluate_summary(predictions.summary, ground_truth.summary, metric=\"LLM-4D\")\n    \n    # 3. Aggregate results\n    results = {\n        \"name_recovery\": name_scores,\n        \"struct_recovery\": struct_scores,\n        \"code_recovery\": code_scores,\n        \"summary\": summary_scores\n    }\n    return results\n\`\`\`\n\n### Parameter Choices and Design Decisions\n\n- **Weighted Metrics**: Some metrics are weighted to prioritize certain aspects of evaluation, reflecting their importance in real-world analysis.\n- **Contextual Evaluation**: The benchmark encourages the use of contextual information (e.g., call chains, data flow) to improve prediction quality, as described in the prompt template in Figure 4 on page 7.\n- **Automated Data Extraction**: Leveraging IDA Pro and automated scripts ensures scalability and reduces human error.\n\n---\n\n## Significance and Connections to Broader Research\n\n### Why This Approach Is Novel\n\nThis benchmark represents a significant step forward in binary analysis research. Previous efforts focused on single tasks (e.g., name recovery or decompilation), but this benchmark integrates multiple tasks and metrics, enabling holistic assessment of model capabilities[5]. The use of both classical and LLM-based metrics is innovative, reflecting the increasing role of AI in reverse engineering.\n\n### Connections to Related Work\n\nThe benchmark builds on established evaluation practices in both NLP (ROUGE, BLEU) and software engineering (F1, CodeBLEU). It extends these to the unique challenges of binary analysis, where the lack of symbolic information and the complexity of decompiled code make evaluation especially challenging.\n\n### Key Contributions and Implications\n\n- **Comprehensive Evaluation**: By covering name, type, structure, code, and summarization tasks, the benchmark provides a thorough assessment tool for the community.\n- **Extensibility**: The modular design allows for easy integration of new tasks and metrics, supporting future research directions.\n- **Practical Impact**: The benchmark\u2019s automation and integration with industry-standard tools (e.g., IDA Pro) make it immediately useful for both researchers and practitioners.\n\n### Integration with Model Training\n\nThe benchmark is tightly coupled with the model training process, as described in \u00a73.3 (page 5\u20137). The SFT dataset is designed to reflect the tasks and metrics used in the benchmark, ensuring that model improvements directly translate to real-world performance gains. The context enhancement method, illustrated in Figure 4 (page 7), further improves the model\u2019s ability to leverage program context for more accurate predictions.\n\n---\n\n## Summary Table: Benchmark Tasks and Metrics\n\n| Task                 | Metric(s)                | Purpose                                 |\n|----------------------|--------------------------|-----------------------------------------|\n| Name recovery        | ROUGE                    | Assess accuracy of recovered names      |\n| Struct recovery      | F1                       | Measure correctness of recovered structs|\n| Decompilation        | CodeBLEU                 | Evaluate code reconstruction quality    |\n| Summarization        | LLM-based 4D evaluator   | Assess semantic, accuracy, readability  |\n\n---\n\n## Final Thoughts\n\nThis section provides the foundation for understanding how ReCopilot\u2019s performance is measured and compared. The benchmark\u2019s design ensures that the model\u2019s outputs are not only technically correct but also practically useful for reverse engineers. By combining classical and AI-driven metrics, and by integrating with industry-standard tools, the benchmark sets a new standard for evaluating advances in binary analysis research[1][5]. Its modular and extensible design also paves the way for future innovations in the field.", "citations": ["https://www.arigs.com/services/product-engineering-services/benchmarking-reverse-engineering/", "https://docanco.com/measurement-techniques-for-reverse-engineering/", "https://www.creaform3d.com/en/resources/blog/what-is-reverse-engineering", "https://www.verisurf.com/blog/article/reverse-engineering-benchmark/", "https://pure.qub.ac.uk/files/131816281/soft_v8_n12_2015_pp_115_124.pdf"], "page_number": 11}, {"id": "performance-results-on-binary-and-general-benchmarks", "title": "Performance Results on Binary and General Benchmarks", "content": "Below is a comprehensive, educational expansion of the section: **\"Performance Results on Binary and General Benchmarks\"** for the ReCopilot research paper. This content is designed for advanced researchers and graduate students, following the principles of clarity, progressive explanation, and technical accuracy\u2014while referencing figures, tables, and mathematical formulations from the paper.\n\n---\n\n## Performance Results on Binary and General Benchmarks\n\nThis section provides a detailed analysis of ReCopilot\u2019s performance across both specialized binary analysis benchmarks and broader, general-purpose evaluation suites. Understanding these results is crucial because they demonstrate the effectiveness of domain-specific training and context enhancement in language models, and show how innovations in model architecture and data handling translate to tangible improvements in real-world reverse engineering tasks.\n\nIn the context of the broader research, these results are a litmus test for whether smaller, focused models can outperform\u2014or at least match\u2014large general-purpose models on both their home domain and established standard tasks. This is especially important in fields like cybersecurity, where model size and deployment constraints are practical concerns[1].\n\n---\n\n## Core Content\n\n### Key Concepts and Definitions\n\n**1. Binary Analysis Benchmarks**  \nBinary analysis is the process of examining executable files (binaries) to extract information about their structure, behavior, or vulnerabilities, often without access to source code. A benchmark in this context is a standardized set of tasks\u2014such as function name recovery, variable type inference, struct recovery, and code summarization\u2014used to evaluate how well models perform[3].\n\n**2. General Benchmarks**  \nGeneral benchmarks assess broader capabilities such as math reasoning (MATH), factual knowledge (MMLU), code generation (HumanEval, MBPP), and instruction following (IFEval). These tasks require a model to demonstrate reasoning, comprehension, and generalization beyond a single domain[3].\n\n**3. Domain-Specific Training**  \nDomain-specific training tailors the model to excel in a particular field\u2014in this case, binary analysis\u2014by exposing it to large amounts of relevant data and specialized tasks. This approach contrasts with general-purpose training, which covers a wide range of topics but often lacks depth in niche areas[1].\n\n### Mathematical Formulation and Data Flow\n\nThe performance evaluation can be formalized as follows. Given a set of analysis tasks $T = \\{t_1, t_2, ..., t_n\\}$ and a binary file $B$ produced by compiling source code $S$ ($B = R(C(S))$), the model generates outputs $O = A(B) = \\{o_1, o_2, ..., o_n\\}$ for each task[3].\n\nFor example, in function name recovery, the input is a decompiled pseudo-code function and the output is the predicted function name, ideally matching the original source code symbol. This process is repeated over a large dataset, and performance is measured as accuracy or another appropriate metric.\n\n### Performance Results and Comparison\n\nReCopilot, with only 7 billion parameters (7B), achieves a **13% average performance gain over existing models** on the constructed binary analysis benchmark, according to Table 3 and Figure 6 on page 8 of the paper. This improvement is attributed to:\n\n- **Domain-Specific Training:** Continued pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO) inject specialized knowledge into the model, as detailed on page 5[1].\n- **Context Enhancement:** By incorporating function call chains and variable data flow into the input prompt, the model gains a richer context for analysis (see Figures 4 and 5, pages 6\u20137)[1].\n- **Reasoning Capabilities:** Test-time scaling and chain-of-thought (CoT) reasoning enable deeper analysis and more reliable predictions.\n\nDespite its smaller size, ReCopilot **competes with much larger general-purpose models** such as DeepSeek-V3 (671B parameters) on both binary analysis and general tasks. This indicates a strong transfer of reasoning skills from the specialized domain to broader coding and reasoning challenges.\n\n### Example: Task Performance\n\nFor illustration, consider the task of function name recovery from decompiled pseudo-code (as shown in Figure 1, page 3). Traditional decompilers often output generic function names like \`sub_1909\` for functions whose original names have been stripped. ReCopilot, given the pseudo-code and context (function calls, data flow), predicts the original name (e.g., \`AES_CBC_encrypt_buffer\`), significantly aiding reverse engineers[1].\n\n### Reasoning Behind Methodological Choices\n\nThe choice to combine domain-specific training with context enhancement and CoT reasoning is driven by the need to address the complexity of stripped binaries, where symbolic information is missing. This approach overcomes the limitations of previous models that failed to leverage both task-specific data and contextual understanding (page 5)[1].\n\nThe use of a generator-discriminator framework for SFT data generation (page 7) ensures that the model is exposed to a diverse set of reasoning paths, improving its ability to handle complex, multi-step tasks.\n\n---\n\n## Technical Details\n\n### Implementation and Parameter Choices\n\n**1. Model Architecture and Training Stages**  \nReCopilot is built upon a pre-trained base LLM (Qwen2.5-Coder-7B) and undergoes three stages of training:\n- **Continued Pretraining (CPT):** Injects domain knowledge by training on a mix of binary, source, and natural language data (see Table 2, page 6)[1].\n- **Supervised Fine-Tuning (SFT):** Adapts the model to specific binary analysis tasks using a dataset enriched with reasoning chains (see Figure 4, page 7)[1].\n- **Direct Preference Optimization (DPO):** Improves the model\u2019s reasoning consistency and instruction-following ability by learning from user preference data[1].\n\n**2. Context Enhancement and Prompt Engineering**  \nEach prompt for binary analysis tasks is carefully constructed to include:\n- The target function\u2019s pseudo-code\n- Relevant context functions\n- Call chains and data flow information\n\nThis modular input-output template (Figure 4, page 7) allows the model to leverage static program analysis results, providing a comprehensive view of the function\u2019s environment[1].\n\n**3. Algorithmic Consideration: MinHash Deduplication**\nTo ensure data quality, the authors use the MinHash algorithm to deduplicate similar pseudo-code functions. MinHash is a locality-sensitive hashing technique that efficiently identifies and removes redundant data, reducing noise and overfitting (page 5)[1].\n\nThe algorithm can be summarized in pseudocode as follows:\n\n\`\`\`python\ndef deduplicate_functions(functions_list):\n    signatures = []\n    for f in functions_list:\n        sig = minhash(f)\n        if sig not in signatures:\n            signatures.append(sig)\n            yield f\n\`\`\`\nHere, \`minhash(f)\` computes a locality-sensitive hash of function \`f\`, and only unique signatures are retained.\n\n---\n\n## Significance and Connections\n\n### Novelty and Broader Impact\n\nReCopilot\u2019s **13% performance gain** on binary analysis benchmarks is a significant advance, demonstrating that domain-specific training and context enhancement can overcome the limitations of general-purpose models[1]. The approach is novel in several ways:\n\n- **Multi-Task Training:** ReCopilot is trained to handle multiple binary analysis tasks simultaneously, unlike previous models that focused on single tasks (page 2)[1].\n- **Contextual Reasoning:** By integrating static analysis results into the prompt, ReCopilot achieves a deeper understanding of binary code, enabling more accurate and interpretable predictions (page 7)[1].\n- **Efficient Deployment:** The model\u2019s relatively small size (7B) makes it suitable for local inference, addressing practical constraints in security analysis (page 4)[1].\n\n### Connections to Related Work\n\nReCopilot builds upon and surpasses previous efforts in binary analysis, such as BinSum and DeBinVul, which focused on single tasks or lacked context awareness[1]. The integration of chain-of-thought reasoning and context enhancement is inspired by recent advances in LLMs for code and math, but ReCopilot adapts these techniques to the unique challenges of binary analysis[3].\n\n### Implications for the Field\n\nThe success of ReCopilot suggests that **smaller, specialized models** can achieve state-of-the-art results in binary analysis, provided they are trained on high-quality, domain-specific data and equipped with context-aware reasoning capabilities. This has important implications for the automation of reverse engineering and vulnerability discovery, enabling faster and more accurate analysis of stripped binaries[1][3].\n\n---\n\n## Summary Table: ReCopilot Performance Highlights\n\n| Benchmark Type      | Metric/Result                        | Comparison                    | Significance                        |\n|---------------------|--------------------------------------|-------------------------------|--------------------------------------|\n| Binary Analysis     | 13% avg. gain over existing models   | Outperforms domain models     | Demonstrates domain expertise[1]     |\n| General Benchmarks  | Competitive with DeepSeek-V3 (671B)  | Matches large general models  | Shows reasoning transfer[1]          |\n| Model Size          | 7B parameters                        | Much smaller than competitors | Enables local deployment[1]          |\n\n---\n\n*This section provides a detailed, educational breakdown of ReCopilot\u2019s performance results, connecting technical details and methodological choices to broader implications for reverse engineering and AI-assisted cybersecurity.*[1][3]", "citations": ["https://www.themoonlight.io/review/recopilot-reverse-engineering-copilot-in-binary-analysis", "https://github.com/armijnhemel/binaryanalysis-ng", "https://www.themoonlight.io/en/review/binmetric-a-comprehensive-binary-analysis-benchmark-for-large-language-models", "https://escholarship.org/uc/item/3028d9vk", "https://github.com/SoftSec-KAIST/BinKit"], "page_number": 11}]}, {"id": "impact-limitations-and-future-directions", "title": "Impact, Limitations, and Future Directions of ReCopilot", "content": "## Introduction: Why Impact, Limitations, and Future Directions Matter\n\nThis section synthesizes the significance of ReCopilot\u2014an expert large language model (LLM) for binary analysis\u2014by evaluating its real-world impact, identifying current limitations, and outlining promising future directions. Understanding these aspects is crucial for appreciating how ReCopilot advances the field beyond previous efforts, where general-purpose LLMs and domain-specific tools struggled with the complexity and specialized requirements of binary reverse engineering[5][1]. This section also sets the stage for the broader adoption of AI-driven tools in cybersecurity and software analysis, highlighting how community engagement and open access can accelerate both research and practical deployment.\n\nIn the context of the broader research paper, this discussion is positioned as a reflective conclusion, integrating empirical results (such as ReCopilot\u2019s 13% average improvement over existing models) with critical analysis of what still needs to be addressed[5]. It connects the technical achievements\u2014such as domain-specific datasets, context-aware reasoning, and robust training pipelines\u2014to the challenges and opportunities that remain.\n\n---\n\n## Core Content: Significance, Challenges, and Directions\n\n### Automating and Enhancing Binary Analysis\n\nReCopilot represents a major step toward automating labor-intensive tasks in binary analysis, such as decompilation, function and variable name recovery, and code summarization. These tasks are traditionally performed by human experts, often requiring deep domain knowledge and time-consuming manual effort[5]. By leveraging an LLM specifically trained on binary code and source code pairs, ReCopilot can provide interpretable and scalable AI assistance, making the analysis of stripped binaries more accurate and accessible.\n\n**Example of Impact:**  \nConsider a stripped binary with dozens of functions, each represented by cryptic names like \`sub_1234\`. ReCopilot can infer meaningful function names and variable types, and even reconstruct data structures, by analyzing the decompiled pseudocode and its context (see Figure 1, page 2)[5]. This ability is transformative for malware analysis, vulnerability research, and legacy code maintenance.\n\n### Key Limitations and Methodological Rationale\n\nDespite its achievements, ReCopilot faces several limitations:\n\n- **Context Length Constraints:**  \n  Modern LLMs, including ReCopilot, are limited by the maximum context length they can process in a single pass. This means that analyzing large binaries or complex chains of reasoning\u2014where the function call graph is deep or the data flow is intricate\u2014requires splitting the input into smaller chunks, potentially losing important cross-function context. This limitation is highlighted in the paper\u2019s discussion of \u201cvery long chain-of-thought reasoning\u201d (page 6, \u00a73.3.3)[5].\n- **Dataset Quality and Coverage:**  \n  The quality of the training data, especially the alignment between stripped binaries and source code, directly affects the model\u2019s performance. The authors note that publicly available datasets often lack high-quality annotations or sufficient coverage of real-world binaries, which can bias the model or limit its generalization (page 8, \u00a73.3.1)[5].\n- **Reasoning Depth and Format Consistency:**  \n  While ReCopilot is fine-tuned to reason about code semantics and generate accurate outputs, it still struggles with complex logic or rare edge cases. The use of direct preference optimization (DPO) helps improve format consistency and logical rigor, but there is room for further improvement in reasoning depth and robustness (page 6, \u00a73.2)[5].\n\n### Future Directions: Extending the Model\u2019s Capabilities\n\nThe authors propose several avenues for future work:\n\n- **Extending Context Integration:**  \n  Future versions of ReCopilot could incorporate more sophisticated context-building mechanisms, such as hierarchical attention or incremental reasoning, to better handle large binaries and complex call graphs.\n- **Improving Reasoning Depth:**  \n  Advances in chain-of-thought prompting and iterative refinement could enable the model to tackle more challenging analysis tasks, such as identifying advanced obfuscation techniques or inferring security vulnerabilities from subtle patterns.\n- **Larger-Scale Datasets and Models:**  \n  Expanding the training corpus to include more diverse binaries and source code pairs, as well as exploring larger model architectures, could further improve accuracy and generalization (page 11, \u00a7Future Work)[5].\n- **Community Engagement:**  \n  The public release of the ReCopilot demo and benchmark encourages community involvement, fostering collaborative research and practical deployment in real-world security workflows[5].\n\n### Why These Choices Matter\n\nThe decision to focus on domain-specific training, context enhancement, and rigorous data curation is driven by the unique challenges of binary analysis. Unlike source code, binary programs lack symbolic information and are often optimized or obfuscated, making them harder to analyze. By tailoring the LLM to this domain and continuously refining its training pipeline, the authors ensure that ReCopilot remains both practical and effective for security researchers and reverse engineers[5].\n\n---\n\n## Technical Details: Implementation Insights\n\n### Training Pipeline and Data Preparation\n\nReCopilot\u2019s training pipeline is meticulously designed to address the limitations of general-purpose LLMs in the binary analysis domain:\n\n1. **Continued Pretraining (CPT):**  \n   The base model is further trained on a large corpus of decompiled pseudocode, source code, and natural language annotations. This stage injects domain-specific knowledge into the model, enabling it to understand the unique characteristics of binary code (page 4, \u00a73.2)[5].\n2. **Supervised Fine-Tuning (SFT):**  \n   The model is fine-tuned on a mixture of binary and general code datasets, with a focus on reasoning-intensive tasks. A generator-discriminator framework is used to automatically generate chain-of-thought reasoning examples, ensuring that the model learns to think step-by-step before making predictions (page 6, \u00a73.3.3)[5].\n3. **Direct Preference Optimization (DPO):**  \n   DPO is applied to improve the model\u2019s output formatting and logical consistency. Unlike traditional reinforcement learning from human feedback (RLHF), DPO directly optimizes the model using pairs of chosen and rejected responses, making the training process more efficient and scalable (page 6, \u00a73.2)[5].\n\n### Context Enhancement and Prompt Design\n\nTo maximize the model\u2019s context awareness, static program analysis is used to extract call chains and data flow information, which are included in the model\u2019s input (Figure 4, page 7)[5]. This allows ReCopilot to reason about the relationships between functions and variables, providing more accurate and interpretable results.\n\n**Example Prompt Template:**\n\`\`\`plaintext\n<context-pseudocode>\n  {context}\n</context-pseudocode>\n<pseudocode>\n  {target_func}\n</pseudocode>\n<Call-Chains>\n  {call_chains}\n</Call-Chains>\n<Data-Flow>\n  {data_flow}\n</Data-Flow>\nAnalysis Task Tag:\n  {task_tag}\n<Thought>\n  Thinking...\n</Thought>\n<Output>JSON Format</Output>\n\`\`\`\nThis modular design ensures that the model can handle a wide range of tasks and adapt to new challenges as they arise.\n\n### Model Architecture and Parameter Choices\n\nReCopilot is built on a 7B-parameter LLM, which is significantly smaller than general-purpose models like DeepSeek-V3 (671B parameters), yet it achieves comparable performance on binary analysis tasks (page 1, Abstract)[5]. This compact architecture enables efficient local inference, making the tool practical for use on standard laptops\u2014a critical consideration for security analysts working in offline or resource-constrained environments.\n\n---\n\n## Significance, Connections, and Broader Impact\n\n### Novelty and Contribution\n\nReCopilot\u2019s most significant innovation lies in its domain-specific approach to LLM training and its focus on real-world binary analysis challenges. By integrating static program analysis, context enhancement, and rigorous data curation, the model achieves state-of-the-art performance on a comprehensive benchmark, outperforming existing tools and general-purpose LLMs by 13% on average (page 1, Abstract)[5].\n\n### Connections to Related Work\n\nPrevious efforts in binary analysis have typically focused on single tasks, such as function name recovery or variable type inference, and have relied on limited datasets or narrow scopes[5]. ReCopilot\u2019s multi-task framework and large-scale, high-quality training data set it apart from these earlier approaches, enabling more comprehensive and accurate analysis of stripped binaries.\n\n### Implications for Cybersecurity and Beyond\n\nThe success of ReCopilot has broad implications for cybersecurity, vulnerability research, and software maintenance. By automating tedious and error-prone tasks, the model empowers security analysts to focus on higher-level reasoning and threat detection. Moreover, the public release of the demo and benchmark encourages community collaboration and accelerates progress in the field[5].\n\n---\n\n## Summary Table: Impact, Limitations, and Future Directions\n\n| Aspect                | Current State (ReCopilot)                | Key Limitations                        | Future Directions                         |\n|-----------------------|------------------------------------------|-----------------------------------------|-------------------------------------------|\n| Domain Expertise      | Specialized for binary analysis          | Limited by context window               | Extend context integration                |\n| Training Pipeline     | CPT, SFT, DPO, data curation             | Dataset quality and coverage            | Larger-scale datasets, diverse binaries   |\n| Reasoning Capability  | Chain-of-thought, context-aware          | Challenges with very long reasoning     | Improve reasoning depth, iterative ref.   |\n| Practical Deployment  | Efficient local inference (7B model)     | Rare edge cases, format errors          | Robustness, community engagement          |\n\n---\n\n## Mathematical and Algorithmic Highlights\n\n### Direct Preference Optimization (DPO)\n\nDPO is a reinforcement learning method that optimizes the model\u2019s output by directly learning from user preferences. Given a prompt $x$ and two model outputs $y_1$ and $y_2$ (where $y_1$ is preferred over $y_2$), the loss function for DPO is:\n\n$$\n\\mathcal{L}_{\\text{DPO}}(\\theta) = -\\mathbb{E}_{(x,y_1,y_2)} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)} - \\beta \\log \\frac{\\pi_\\theta(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)} \\right) \\right]\n$$\n\nwhere $\\pi_\\theta$ is the learned policy, $\\pi_{\\text{ref}}$ is the reference policy, and $\\beta$ is a temperature parameter[5].\n\n### Data Deduplication with MinHash\n\nTo ensure dataset quality, the MinHash algorithm is used for function-level deduplication:\n\n$$\nh_{\\text{min}}(S) = \\min_{x \\in S} h(x)\n$$\n\nwhere $S$ is a set (of tokens in a function), $h$ is a hash function, and $h_{\\text{min}}(S)$ is the minimal hash value in the set[5].\n\n---\n\n## Final Thoughts\n\nReCopilot represents a major advance in the application of LLMs to binary analysis, addressing longstanding challenges in automation, interpretability, and scalability. While current limitations\u2014such as context length constraints and dataset quality\u2014remain, the model\u2019s innovative design, rigorous training pipeline, and focus on real-world use cases set a strong foundation for future research and practical deployment in cybersecurity and beyond[5].", "citations": ["https://arxiv.org/abs/2505.16366", "https://www.gartner.com/peer-community/post/anybody-considered-using-llm-s-to-reverse-engineer-legacy-mainframe-code-to-generate-documentation-github-copilot-quite-good", "https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html", "https://labs.zenity.io/p/rce", "https://arxiv.org/html/2505.16366v1"], "page_number": 12, "subsections": [{"id": "significance-and-implications", "title": "Significance and Implications", "content": "## Significance and Implications\n\n### Introduction\n\nThis section explores why the approach, innovations, and results presented in the ReCopilot research paper are important for both the field of binary program analysis and broader AI-assisted software security. Understanding the significance of ReCopilot is essential because it marks a leap forward in automating labor-intensive and highly specialized tasks that have traditionally required expert knowledge. By integrating domain-specific training, context-aware inputs, and advanced reasoning capabilities into a single expert large language model (LLM), ReCopilot demonstrates how AI can be tailored to excel in niche but critical areas of cybersecurity and reverse engineering[3][2][1].\n\nThe implications extend beyond technical achievements: ReCopilot enables more efficient, scalable, and accessible analysis of stripped binaries\u2014software components compiled without source-level symbols\u2014making it easier for security professionals to discover vulnerabilities, detect malware, and understand complex codebases. This is particularly relevant as traditional decompilers like IDA Pro and Ghidra, although powerful, often struggle with symbol recovery and semantic understanding[3][2]. ReCopilot\u2019s methodology and results reshape the landscape of automated binary analysis and set new standards for future research and tool development.\n\n### Core Content\n\n#### Key Concepts and Definitions\n\n- **Binary Analysis**: The examination of compiled code (binaries) to understand, verify, or extract information about program behavior, often without access to source code[4][1].\n- **Stripped Binaries**: Executables where debug symbols (function/variable names, types, etc.) have been removed, making analysis harder[3].\n- **Domain-Specific Training**: Continual pretraining and fine-tuning of a language model using data specifically from the binary code domain, enhancing its ability to process and reason about low-level code[3].\n- **Context Awareness**: Enriching model inputs with static analysis data\u2014such as call graphs and data flow\u2014to provide a fuller picture of code structure and semantics[3].\n- **Test-Time Scaling**: Dynamically adjusting the model\u2019s reasoning process during inference, enabling deeper analysis and more accurate output[3].\n- **Multi-Task Support**: The ability of a model to perform a variety of related tasks (e.g., function name recovery, variable type inference, decompilation) simultaneously, reflecting real-world analyst workflows[3].\n\n#### Mathematical Formulations\n\nThe training of ReCopilot involves several stages, each with distinct loss functions and optimization objectives. For example, **continued pretraining (CPT)** minimizes the negative log-likelihood loss over sequences of tokens from the domain-specific dataset:\n\n$$\n\\mathcal{L}_{\\text{CPT}} = -\\sum_{t} \\log p(w_t | w_{<t})\n$$\n\nwhere $w_t$ is the token at position $t$ and $w_{<t}$ are the previous tokens[3]. This foundation allows the model to learn patterns in decompiled pseudo code, source code, and natural language comments.\n\n**Supervised fine-tuning (SFT)** adapts the model to downstream tasks by optimizing the likelihood of correct task outputs given input prompts:\n\n$$\n\\mathcal{L}_{\\text{SFT}} = -\\sum_{i} \\log p(y_i | x_i)\n$$\n\nwhere $x_i$ is the input prompt and $y_i$ is the ground truth output for the $i$-th example[3].\n\n**Direct Preference Optimization (DPO)** further refines model outputs by learning from user-chosen and user-rejected responses, aligning with human preferences and reasoning quality[3].\n\n#### Illustrated Example\n\nConsider Figure 1 in the paper: a decompiled pseudo code function and its corresponding source code. The decompiled version lacks meaningful symbols, making it difficult to interpret. ReCopilot, given the pseudo code and context, can recover function and variable names, infer types, and generate natural language summaries\u2014tasks that require deep semantic understanding, as shown on page 3[3].\n\n#### Why These Methodological Choices?\n\n- **Domain-Specific Training**: Ensures the model is not misled by irrelevant patterns from general code or text datasets, improving accuracy and relevance[3].\n- **Context Enhancement**: Enables the model to reason about code at a broader scale, not just individual functions, by incorporating static analysis data (call chains, data flow)[3].\n- **Test-Time Scaling**: Improves the model\u2019s ability to handle complex, long-chain reasoning tasks, which are common in real-world binary analysis[3].\n- **Multi-Task Fine-Tuning**: Reflects the practical needs of security analysts, who often need to perform multiple analyses in a single workflow[3].\n\n### Technical Details\n\n#### Implementation Specifics\n\nReCopilot\u2019s training pipeline, as outlined in Figures 2 and 3, consists of three main stages: **continued pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO)**. Each stage is supported by a carefully constructed dataset, with rigorous sanitization and deduplication to ensure data quality, as detailed in Table 1 on page 5[3].\n\nThe SFT dataset is built using a generator-discriminator framework, which automatically synthesizes reasoning processes (chain-of-thought) for each example, addressing the lack of such data in existing datasets. This process is described in depth in \u00a73.3.3 and illustrated in Figure 5[3].\n\n#### Algorithm and Procedure\n\nThe following pseudocode summarizes the model input assembly for a binary analysis task:\n\n\`\`\`plaintext\nInput:\n    <context-pseudocode>\n        {context}\n    </context-pseudocode>\n    <pseudocode>\n        {target_func}\n    </pseudocode>\n    <Call-Chains>\n        {call_chains}\n    </Call-Chains>\n    <Data-Flow>\n        {data_flow}\n    </Data-Flow>\n    Analysis Task Tag:\n        {task_tag}\nOutput:\n    <Thought>\n        {reasoning}\n    </Thought>\n    <Output>\n        {results_in_JSON}\n    </Output>\n\`\`\`\nThis modular design supports a wide range of tasks and is scalable for future extensions[3].\n\n#### Parameter Choices\n\n- **Model Size**: ReCopilot achieves comparable performance to much larger LLMs with a 7B parameter model, making it deployable on local hardware[3].\n- **Dataset Composition**: The pretraining dataset mixes binary code, general source code, and natural language in a 60:25:15 ratio, balancing domain specificity and generalizability[3].\n- **Context Length**: The model is designed to handle long sequences, but practical limits are set to ensure efficient analysis of large binaries[3].\n\n### Significance and Connections\n\n#### Novelty and Importance\n\nReCopilot represents a significant advance because it addresses several long-standing challenges in binary analysis:\n\n- **Automation of Expert Knowledge**: By combining domain-specific training, context enhancement, and test-time scaling, ReCopilot automates tasks that previously required manual expert intervention[3][2].\n- **Scalability**: The model\u2019s architecture and training methodology enable efficient, large-scale analysis, far surpassing the capabilities of traditional decompilers and single-task AI approaches[3][2].\n- **Generalizability**: While previous methods focused on single tasks (e.g., decompilation or variable name recovery), ReCopilot supports a comprehensive suite of analyses, mirroring real-world use cases[3][2].\n\n#### Connections to Broader Research\n\nReCopilot builds on recent advances in AI for code, such as LLM4Decompile and ReSym, but goes further by integrating multi-task support and context awareness[3]. It also leverages state-of-the-art techniques in language model training, including continued pretraining and direct preference optimization, to achieve superior performance[3].\n\n#### Implications for the Field\n\n- **Security Analysis**: ReCopilot enables faster, more accurate vulnerability discovery and malware detection, critical for industries like e-commerce and banking where software security is paramount[2][1].\n- **Reverse Engineering**: The model\u2019s reasoning and symbol recovery capabilities make it a powerful tool for reverse engineering, reducing the barrier to entry for understanding complex binaries[3][2].\n- **AI for Specialized Domains**: ReCopilot demonstrates the potential for expert LLMs to outperform general-purpose models in specialized domains, suggesting new directions for AI research and tool development[3][2].\n\n> **In summary:** ReCopilot\u2019s integration of domain-specific training, context enhancement, and advanced reasoning sets a new benchmark for automated binary analysis, with profound implications for security, reverse engineering, and the broader landscape of AI-assisted software engineering[3][2][1].", "citations": ["https://blog.kodezi.com/what-is-a-binary-analysis-tool-understanding-its-importance-and-functionality/", "https://zimperium.com/blog/emerging-trends-of-binary-analysis", "https://www.cs.purdue.edu/news/articles/2025/purdue-cs-researchers-develop-an-ai-powered-tool-for-reverse-engineering-stripped-binary-code-earning-acm-ccs-distinguished-paper-award.html", "https://www.blackduck.com/glossary/what-is-binary-code-binary-analysis.html", "https://arxiv.org/html/2410.07537v1"], "page_number": 12}, {"id": "limitations-and-areas-for-improvement", "title": "Limitations and Areas for Improvement", "content": "## Limitations and Areas for Improvement in ReCopilot\n\nThis section delves into the inherent limitations of the ReCopilot model and identifies avenues for future enhancement. Understanding these boundaries is crucial for appreciating the current capabilities of the model within the broader binary analysis research landscape and setting realistic expectations for its deployment. Furthermore, acknowledging these limitations highlights the ongoing challenges that motivate continued research and innovation in the domain of reverse engineering and binary program analysis.\n\nReCopilot has made notable strides in applying large language models (LLMs) specifically tailored for binary analysis tasks such as function name recovery and variable type inference. However, binary analysis remains a demanding field owing to the complexity of stripped binaries and the need for deep semantic reasoning. This section fits integrally into the paper by providing a balanced, critical perspective on the scope of ReCopilot\u2019s advances and the technical challenges that remain to be addressed for broader and more robust applicability.\n\n### Core Challenges and Conceptual Foundations\n\nA key limitation of ReCopilot lies in its difficulty generating very long and coherent chain-of-thought (CoT) reasoning. CoT reasoning is a technique where the model incrementally builds an explanation or solution, crucial in domains like binary analysis where stepwise logical deduction is necessary. The challenge is partly due to the model\u2019s limited context window and partly due to the inherent complexity of reasoning about low-level binary code semantics. The mathematical essence of CoT can be described as a sequential conditional probability problem, where the model generates tokens based on prior tokens:\n\n$$\nP(\\text{output}) = \\prod_{t=1}^T P(y_t | y_1, y_2, \\ldots, y_{t-1}, x)\n$$\n\nHere, $y_t$ represents the token at step $t$, and $x$ denotes the input context including pseudo code and enhanced features like call graphs and data flow. As $T$ grows with longer reasoning chains, the model must maintain coherence over extended sequences, which strains its capacity and can degrade output quality (see discussion on page 8 and Figure 4 depicting the input-output template).\n\nAnother core limitation is the restricted access to comprehensive and diverse training data. While the authors compiled a large dataset of over 101 million binary functions (Table 1 on page 5), the diversity in real-world binaries, compiler options, and architectures is vast. The existing dataset, although extensive, cannot encapsulate all binary programming idiosyncrasies or rare cases. This shortfall may induce domain gaps during inference on unseen binaries, limiting generalization.\n\nComputational overhead is also a significant bottleneck. The context enhancement strategies employed\u2014such as static program analysis to generate function contexts, call chains, and data flow graphs\u2014increase the input size and computational demands (described around page 9, section 3.4). While these enrich the model\u2019s understanding, they require more memory and processing time, thus limiting the model\u2019s scalability and feasibility for very large or resource-constrained environments.\n\n### Technical Details and Implementation Constraints\n\nThe ReCopilot model balances performance with inference feasibility through a medium-sized model (7B parameters) instead of ultra-large models which, while potentially more powerful, are computationally prohibitive for local laptop deployment as intended (page 7). This size selection reflects a design decision prioritizing practical usability and deployment over maximal raw performance. However, this choice may inherently restrict the model\u2019s capacity to fully analyze ultra-complex binary programs or very large codebases which demand extensive context.\n\nThe dataset construction involved several stages: continued pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO). These stages progressively refined the model\u2019s ability to understand binary code semantics and adhere to output formats (see Figure 2 on page 4). The generator-discriminator framework for SFT dataset building (page 6) addresses the absence of ground-truth reasoning steps by automatically creating chain-of-thought enriched examples. Yet, the quality and diversity of these synthetic reasoning chains are limited by the base model\'s initial capabilities, which may propagate biases or errors.\n\nAlgorithmically, context enhancement incorporates static analysis outputs as parts of the prompt with semantic tags like \`<context-pseudocode>\`, \`<call-chains>\`, and \`<data-flow>\` (Figure 4), which enrich the input representation. However, integrating these partial representations into the transformer model\'s fixed-length input tokens poses challenges in encoding efficiency and relevance weighting, which could degrade model reasoning when faced with extraneous or noisy context.\n\nParameter choices, such as token limits and mix ratios in pretraining datasets (e.g., 60% binary domain data, 25% general code, 15% natural language as per Table 2 on page 5), reflect attempts to prevent overfitting while injecting domain knowledge. Yet, balancing this ratio remains an open question, as too little domain data may underfit binary semantics, while too much may cause over-specialization.\n\n### Significance and Broader Research Connections\n\nDespite these limitations, ReCopilot represents a novel and important advance in adapting LLMs specifically for binary analysis\u2014a domain traditionally reliant on manual expert effort and specialized tools. It bridges a gap by combining large-scale domain-specific training with contextual program analysis to provide interpretable and multi-task capabilities (page 1 abstract and introduction).\n\nThis work aligns with and extends prior efforts that focused narrowly on single tasks like function name or variable type prediction (section 2 background). By supporting a wider array of tasks and multi-stage training strategies, ReCopilot broadens the practical utility of AI in reverse engineering. Its approach to synthetic chain-of-thought generation is particularly innovative, enabling enhanced reasoning capabilities without expensive human annotation.\n\nThe implications for the field include paving the way for more accessible, automated binary analysis tools that can assist cybersecurity professionals in vulnerability detection and malware analysis. Moreover, balancing model size and efficiency addresses real-world deployment concerns, fostering adoption beyond research labs.\n\nIn conclusion, while ReCopilot\'s current design shows state-of-the-art performance, its limitations in long-range reasoning, dataset diversity, and computational overhead represent fertile ground for future research. Addressing these challenges will be crucial for advancing automated binary analysis systems toward fully scalable, robust, and explainable AI assistants.\n\n---\n\nThis exposition connects technical details, mathematical principles, and innovation context to provide a comprehensive understanding of the \"Limitations and Areas for Improvement\" section in the ReCopilot research. It highlights both the achievements and the open challenges critical for graduate-level readers and researchers aiming to build upon this work.", "citations": ["https://www.inspera.com/ai/examples-of-ai-misuse-in-education/", "https://education.illinois.edu/about/news-events/news/article/2024/10/24/ai-in-schools--pros-and-cons", "https://www.waldenu.edu/programs/education/resource/five-pros-and-cons-of-ai-in-the-education-sector", "https://academicintegrity.psu.edu/courses/academic-integrity/artificial-intelligence/limitations-of-ai-models", "https://www.educationnext.org/a-i-in-education-leap-into-new-era-machine-intelligence-carries-risks-challenges-promises/"], "page_number": 12}, {"id": "future-research-and-community-engagement", "title": "Future Research Directions and Community Engagement", "content": "This section, \u201cFuture Research Directions and Community Engagement,\u201d explores the roadmap for scaling automated binary analysis research and invites broader participation from the security and AI communities. The importance of this section lies in its forward-looking vision for both technical advancement and the creation of a collaborative ecosystem\u2014crucial for the sustained evolution of the field and for ensuring that research breakthroughs translate into real-world impact.\n\n## Introduction\n\nThe \u201cFuture Research Directions and Community Engagement\u201d section outlines planned next steps to advance the capabilities and reach of automated binary analysis systems like ReCopilot. It explains not only where research is headed but also why community involvement is essential for accelerating progress and ensuring practical applicability. Binary analysis is a cornerstone of cybersecurity, enabling the detection of vulnerabilities and malware, but remains challenging due to the complexity of binaries and the scarcity of expert knowledge[4][5].\n\nUnderstanding future directions is vital for researchers and practitioners because it helps set priorities, guides resource allocation, and fosters innovation. The section also places these efforts in context: as illustrated throughout the paper, automating binary analysis involves collecting large, high-quality datasets, refining reasoning methods, and integrating advanced language models with specialized program analysis tools. Community engagement, facilitated by open-access tools like ReCopilot, is proposed as a mechanism to accelerate research and democratize access to state-of-the-art capabilities[2][3].\n\n## Core Content: Future Directions\n\n### Scaling Datasets and Models\n\nOne of the primary goals is to expand the scale and quality of datasets used for training and evaluation. As detailed on pages 7\u20139, ReCopilot\u2019s dataset construction process involves collecting millions of binary functions and their source counterparts, applying rigorous sanitization and deduplication (using MinHash), and assembling diverse data from multiple pipelines (see Table 1 on page 8). However, the paper identifies gaps\u2014such as limited coverage of real-world, closed-source binaries\u2014that future research must address.\n\nScaling datasets is not just about quantity but also about diversity and representativeness. For example, future work could involve:\n\n- **Expanding Data Sources:** Incorporating more closed-source and proprietary binaries, which are underrepresented in current datasets but are common in real-world security analysis.\n- **Enhancing Data Annotation:** Developing automated pipelines for labeling and validating binary code with high-level semantic information (e.g., variable types, function purposes).\n- **Mitigating Bias:** Ensuring datasets reflect a broad spectrum of software ecosystems, languages, and compiler configurations.\n\n### Refining Reasoning Supervision and Super-CoT\n\nReasoning supervision techniques, such as Supervised Chain-of-Thought (Super-CoT), are central to enabling models like ReCopilot to perform complex, multi-step analyses. Super-CoT encourages models to generate intermediate reasoning steps before arriving at a final answer, which is especially important for tasks like inferring variable types or recovering function names (see \u00a73.3.3, p. 10\u201311).\n\nFuture research will focus on:\n\n- **Enhancing Super-CoT Methods:** Developing more sophisticated supervision strategies to improve reasoning consistency and accuracy.\n- **Integrating Multiple Reasoning Modes:** Combining symbolic reasoning (e.g., static program analysis) with neural reasoning to handle edge cases and ambiguous code patterns.\n\nAn illustrative example is the generator-discriminator framework for SFT dataset construction (Figure 5, p. 11), which automatically generates reasoning processes for training. Future work could extend this framework to include more nuanced reasoning types and feedback mechanisms.\n\n### Integration with Program Analysis Tools and LLMs\n\nThe integration of program analysis tools (such as call graph and data flow analyzers) with large language models (LLMs) is a key innovation. This integration allows models to leverage both syntactic and semantic information, improving contextual awareness and prediction accuracy (see \u00a73.4, p. 12).\n\nFuture directions include:\n\n- **Extending Context Windows:** Supporting longer context windows so that models can analyze larger binary functions or multi-function call sequences.\n- **Advanced Preference Optimization:** Exploring more sophisticated preference optimization techniques, such as Direct Preference Optimization (DPO), to further align model outputs with user needs and reduce format errors.\n- **Hybrid Analysis Pipelines:** Combining static, dynamic, and symbolic analysis with LLMs to create more robust and explainable binary analysis systems.\n\n### Community Engagement and Open Access\n\nTo foster broad research participation, ReCopilot\u2019s demo is made publicly accessible. This open approach encourages collaboration between security analysts, AI researchers, and software engineers, enabling rapid iteration and knowledge sharing (see \u00a71 and Acknowledgments, p. 2, 14).\n\nKey aspects of community engagement include:\n\n- **Public Benchmarks and Datasets:** Encouraging the publication of reproducible datasets and evaluation metrics to standardize comparisons and accelerate progress[3][5].\n- **Open Source Tools:** Providing extensible frameworks and APIs for integrating new analysis methods and tasks.\n- **Community-Driven Research:** Inviting feedback, contributions, and extensions from a wide range of stakeholders to address real-world challenges.\n\n## Technical Details: Implementation and Design Choices\n\n### Dataset Construction and Sanitization\n\nThe dataset construction process is both scalable and reproducible, as detailed on pages 7\u20139. The pipelines for collecting binary and source code data are automated and involve multiple strategies:\n\n- **Compile from Scratch:** Uses open-source repositories like ArchLinux to compile and strip binaries.\n- **Off-the-shelf Artifacts:** Leverages software repositories (Ubuntu, Debian) for ready-made binaries and debug symbols.\n- **CompileAgent:** Employs LLM-driven automation to handle ad-hoc projects and compilation errors.\n\nSanitization and deduplication are critical for data quality. For example, the MinHash algorithm is used to remove redundant functions, and auxiliary/thunk functions are filtered out to reduce noise.\n\nThe pretraining dataset mixes binary, general code, and natural language data (see Table 2, p. 9). The mixture ratio is set to 60:25:15 (binary:code:text), ensuring domain expertise while maintaining generalization.\n\n### Generator-Discriminator Framework for SFT Data\n\nThe SFT dataset is constructed using a generator-discriminator framework (Figure 5, p. 11). This framework automatically generates training examples with reasoning processes (Chain-of-Thought), which are essential for fine-tuning models to perform multi-step analyses.\n\nThe input-output template for SFT data is modular and flexible (see Figure 4, p. 10), allowing for easy extension to new tasks. The template includes:\n\n- **Target binary function**\n- **Context functions**\n- **Call chains**\n- **Data flow**\n- **Task tag**\n- **Model reasoning process**\n- **Final prediction in JSON format**\n\nThis design ensures that models are trained to reason about both local and global context, improving their ability to perform complex analyses.\n\n### Reasoning Supervision and Model Architecture\n\nReasoning supervision is implemented through a combination of supervised fine-tuning and direct preference optimization. The model is first pretrained on a large corpus of binary, code, and text data, then fine-tuned on multi-task SFT data, and finally optimized for preference alignment using DPO (see \u00a73.2, p. 6\u20137).\n\nThe mathematical formulation for fine-tuning involves minimizing the negative log-likelihood of the correct reasoning path:\n\n$$\n\\mathcal{L}_{\\text{SFT}} = -\\log p(y|x)\n$$\n\nwhere $x$ is the input (binary function and context), $y$ is the target reasoning and prediction sequence, and $p(y|x)$ is the model\u2019s probability of generating $y$ given $x$.\n\nFor preference optimization, DPO is used to maximize the probability of preferred outputs over rejected outputs for the same prompt:\n\n$$\n\\mathcal{L}_{\\text{DPO}} = -\\mathbb{E}_{(x,y_w,y_l)} \\log \\sigma\\left(\\log \\frac{p_{\\theta}(y_w|x)}{p_{\\theta}(y_l|x)}\\right)\n$$\n\nwhere $y_w$ is the preferred output, $y_l$ is the rejected output, and $\\sigma$ is the sigmoid function.\n\n## Significance and Connections\n\n### Novelty and Impact\n\nThe approach outlined in this section represents a significant advance in several ways:\n\n- **Scalable and Reproducible Data Collection:** The automated pipelines for dataset construction and the integration of diverse data sources address critical bottlenecks in binary analysis research.\n- **Advanced Reasoning Supervision:** The use of Super-CoT and DPO enables models to perform complex, multi-step reasoning, a capability previously limited to expert analysts.\n- **Integration of Program Analysis and LLMs:** Combining static analysis tools with LLMs provides a richer context for model predictions, improving accuracy and interpretability.\n\n### Broader Research Context\n\nThese future directions align with ongoing trends in both security and AI research. The need for large, high-quality datasets and robust reasoning models has been highlighted by recent surveys and benchmarks in binary analysis[1][5]. The focus on community engagement and open access reflects a growing recognition that collaborative ecosystems are essential for advancing complex, multidisciplinary fields.\n\n### Key Innovations and Contributions\n\n- **Extensible Dataset Construction:** The flexible, automated pipelines and modular templates enable rapid iteration and extension to new tasks and domains.\n- **Reasoning-centric Training:** The generator-discriminator framework and reasoning supervision techniques set a new standard for training models to perform complex analyses.\n- **Open Research Community:** The public availability of ReCopilot and its datasets fosters collaboration, accelerates research, and ensures broader adoption and impact.\n\n## Summary Table: Key Future Research Directions\n\n| Research Direction            | Technical Focus                                   | Community Impact                  |\n|------------------------------|---------------------------------------------------|-----------------------------------|\n| Dataset Scaling              | Expand coverage, improve annotation, reduce bias   | Standardization, collaboration    |\n| Reasoning Supervision        | Super-CoT, reasoning pipelines, multi-step logic   | Democratize expert reasoning      |\n| Integration with LLMs        | Longer context, advanced preference optimization   | Broaden applicability             |\n| Community Engagement         | Open access, benchmarks, extensible frameworks     | Accelerate innovation, feedback   |\n\nThis section not only lays out a clear agenda for future research but also invites the broader community to participate in shaping the next generation of automated binary analysis tools. By focusing on scalability, reasoning, integration, and openness, the field is poised to make significant strides in tackling the most challenging problems in cybersecurity and software engineering.", "citations": ["https://publications.csiro.au/publications/", "https://arxiv.org/html/2405.03991v2", "https://openreview.net/forum?id=dsK5EmmomU", "http://www.upubscience.com/News11Detail.aspx?id=1146", "https://arxiv.org/html/2505.07360v1"], "page_number": 12}]}];
const citationsData: string[] = ["https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/intro-to-JSON/", "https://libraryjuiceacademy.com/shop/course/161-introduction-json-structured-data/", "https://arxiv.org/html/2408.11061v1", "https://monkt.com/recipies/research-paper-to-json/", "https://developer.apple.com/documentation/applenews/json-concepts-and-article-structure"];

// YouTube URL detection function
const isYouTubeUrl = (url: string): boolean => {
  return /(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)/.test(url);
};

// Extract YouTube video ID
const getYouTubeVideoId = (url: string): string | null => {
  const match = url.match(/(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)/);
  return match ? match[1] : null;
};

// Markdown component with math support
const MarkdownContent = ({ content }: { content: string }) => {
  return (
    <ReactMarkdown
      remarkPlugins={[remarkGfm, remarkMath]}
      rehypePlugins={[rehypeKatex]}
      className="prose prose-lg max-w-none text-gray-900 leading-relaxed"
      components={{
        // Custom styling for different elements
        h1: ({ children }) => <h1 className="text-3xl font-bold text-gray-900 mb-4">{children}</h1>,
        h2: ({ children }) => <h2 className="text-2xl font-semibold text-gray-900 mb-3">{children}</h2>,
        h3: ({ children }) => <h3 className="text-xl font-medium text-gray-900 mb-2">{children}</h3>,
        p: ({ children }) => <p className="text-gray-900 mb-4 leading-relaxed">{children}</p>,
        ul: ({ children }) => <ul className="list-disc list-inside mb-4 text-gray-900">{children}</ul>,
        ol: ({ children }) => <ol className="list-decimal list-inside mb-4 text-gray-900">{children}</ol>,
        li: ({ children }) => <li className="mb-1">{children}</li>,
        blockquote: ({ children }) => <blockquote className="border-l-4 border-blue-500 pl-4 italic text-gray-600 mb-4">{children}</blockquote>,
        code: ({ children, className }) => {
          const isInline = !className;
          if (isInline) {
            return <code className="bg-gray-100 px-1 py-0.5 rounded text-sm font-mono text-gray-900">{children}</code>;
          }
          return <pre className="bg-gray-100 p-4 rounded-lg overflow-x-auto mb-4"><code className="text-sm font-mono">{children}</code></pre>;
        },
        a: ({ children, href }) => <a href={href} className="text-blue-600 hover:text-blue-800 underline" target="_blank" rel="noopener noreferrer">{children}</a>,
      }}
    >
      {content}
    </ReactMarkdown>
  );
};

export default function PaperPage() {
  const [activeContent, setActiveContent] = useState('');
  const [imagesData, setImagesData] = useState<ImageData[]>([]);
  const [imagesLoading, setImagesLoading] = useState(true);
  const [selectedImage, setSelectedImage] = useState<ImageData | null>(null);
  const [selectedPdfPage, setSelectedPdfPage] = useState<number | null>(null);
  const [youtubeModal, setYoutubeModal] = useState<{ isOpen: boolean; videoId: string | null }>({
    isOpen: false,
    videoId: null
  });
  const [mobileMenuOpen, setMobileMenuOpen] = useState(false);
  // Fetch images from API
  useEffect(() => {
    const fetchImages = async () => {
      try {
        setImagesLoading(true);
        const response = await fetch(`http://localhost:8000/api/images/${paperData.arxiv_id}`);
        if (response.ok) {
          const images = await response.json();
          setImagesData(images);
        } else {
          console.error('Failed to fetch images:', response.statusText);
          setImagesData([]);
        }
      } catch (error) {
        console.error('Error fetching images:', error);
        setImagesData([]);
      } finally {
        setImagesLoading(false);
      }
    };

    fetchImages();
  }, []);
  
  // Initialize with the first section
  useEffect(() => {
    if (sectionsData?.length > 0) {
      setActiveContent(sectionsData[0].id);
    }
  }, []);
  
  // Get current content (section or subsection)
  const getCurrentContent = () => {
    // First check if it's a main section
    const section = sectionsData?.find(section => section.id === activeContent);
    if (section) {
      return { type: 'section', content: section };
    }
    
    // Then check if it's a subsection
    for (const section of sectionsData || []) {
      const subsection = section.subsections?.find(sub => sub.id === activeContent);
      if (subsection) {
        return { type: 'subsection', content: subsection, parentSection: section };
      }
    }
    
    return null;
  };
  
  const currentContent = getCurrentContent();
  
  // Get relevant images for current content
  const getRelevantImages = (pageNumber: number | undefined): ImageData[] => {
    if (!pageNumber || !imagesData || !Array.isArray(imagesData)) return [];
    return imagesData.filter(img => img.page === pageNumber);
  };
  
  const relevantImages = getRelevantImages(currentContent?.content?.page_number);
  
  // Get citations for current content
  const getSectionCitations = (citations?: string[]): string[] => {
    if (!citations || !Array.isArray(citations)) return [];
    return citations;
  };
  
  const contentCitations = getSectionCitations(currentContent?.content?.citations);

  // Handle citation click
  const handleCitationClick = (citation: string) => {
    if (isYouTubeUrl(citation)) {
      const videoId = getYouTubeVideoId(citation);
      if (videoId) {
        setYoutubeModal({ isOpen: true, videoId });
        return;
      }
    }
    // For non-YouTube links, open in new tab
    window.open(citation, '_blank', 'noopener,noreferrer');
  };

  // Handle PDF page view - open in new tab
  const handlePdfPageView = (pageNumber: number) => {
    const pdfUrl = `https://arxiv.org/pdf/${paperData.arxiv_id}.pdf#page=${pageNumber}`;
    window.open(pdfUrl, '_blank', 'noopener,noreferrer');
  };



  return (
    <div className="min-h-screen flex flex-col bg-white">
      <style jsx global>{customStyles}</style>
      {/* Header */}
      <header className="bg-white sticky top-0 z-50">
        <div className="max-w-full mx-auto px-4">
          <div className="flex items-center justify-between h-16 lg:pl-32 md:pl-16 pl-4">
            <div className="flex items-center space-x-3">
              <Link href="/" className="flex items-center text-blue-600 hover:text-blue-700">
                <ArrowLeft className="w-6 h-6" />
              </Link>
              <h1 className="text-2xl font-bold text-gray-900 lowercase">deeprxiv</h1>
              <span className="text-lg text-gray-800 font-medium truncate max-w-md lg:max-w-2xl">
                {paperData.title}
              </span>
            </div>
            <button 
              onClick={() => setMobileMenuOpen(!mobileMenuOpen)}
              className="md:hidden p-2 text-gray-600 hover:text-gray-900"
            >
              <Menu className="w-6 h-6" />
            </button>
          </div>
        </div>
      </header>

      {/* Mobile Navigation Overlay */}
      {mobileMenuOpen && (
        <div className="fixed inset-0 bg-black bg-opacity-50 z-40 md:hidden" onClick={() => setMobileMenuOpen(false)}>
          <div className="fixed left-0 top-16 bottom-0 w-80 bg-white overflow-y-auto" onClick={(e) => e.stopPropagation()}>
            <div className="p-6">
              <nav className="space-y-1">
                {sectionsData?.map((section) => (
                  <div key={section.id} className="space-y-1">
                    <button
                      onClick={() => {
                        setActiveContent(section.id);
                        setMobileMenuOpen(false);
                      }}
                      className={`block w-full text-left px-1 py-3 rounded-md transition-colors text-sm font-medium ${
                        activeContent === section.id
                          ? 'bg-blue-50 text-blue-700'
                          : 'text-gray-900 hover:bg-gray-100'
                      }`}
                    >
                      <div className="truncate" title={section.title}>
                        {section.title}
                      </div>
                    </button>
                    
                    {section.subsections && section.subsections.length > 0 && (
                      <div className="ml-8 space-y-1">
                        {section.subsections.map((subsection) => (
                          <button
                            key={subsection.id}
                            onClick={() => {
                              setActiveContent(subsection.id);
                              setMobileMenuOpen(false);
                            }}
                            className={`block w-full text-left px-3 py-2 rounded-md text-sm transition-colors ${
                              activeContent === subsection.id
                                ? 'bg-blue-25 text-blue-600'
                                : 'text-gray-800 hover:bg-gray-50'
                            }`}
                          >
                            <div className="truncate" title={subsection.title}>
                              {subsection.title}
                            </div>
                          </button>
                        ))}
                      </div>
                    )}
                  </div>
                ))}
              </nav>
            </div>
          </div>
        </div>
      )}

      {/* Main Content */}
      <main className="flex-grow">
        <div className="max-w-full mx-auto px-4">
          <div className="flex min-h-screen">
            {/* Left Sidebar - Navigation */}
            <aside className="w-72 bg-white flex-shrink-0 fixed top-16 bottom-0 overflow-y-auto scrollbar-hide hidden md:block md:left-16 lg:left-32">
              <div className="p-6">
                <nav className="space-y-1">
              {sectionsData?.map((section) => (
                  <div key={section.id} className="space-y-1">
                    {/* Main Section */}
                <button
                      onClick={() => setActiveContent(section.id)}
                      className={`block w-full text-left px-1 py-3 rounded-md transition-colors text-sm font-medium ${
                        activeContent === section.id
                          ? 'bg-blue-50 text-blue-700'
                      : 'text-gray-900 hover:bg-gray-100'
                  }`}
                >
                      <div className="truncate" title={section.title}>
                  {section.title}
                      </div>
                    </button>
                    
                    {/* All Subsections */}
                    {section.subsections && section.subsections.length > 0 && (
                      <div className="ml-8 space-y-1">
                        {section.subsections.map((subsection) => (
                          <button
                            key={subsection.id}
                            onClick={() => setActiveContent(subsection.id)}
                            className={`block w-full text-left px-3 py-2 rounded-md text-sm transition-colors ${
                              activeContent === subsection.id
                                ? 'bg-blue-25 text-blue-600'
                                : 'text-gray-800 hover:bg-gray-50'
                            }`}
                          >
                            <div className="truncate" title={subsection.title}>
                              {subsection.title}
                            </div>
                </button>
                        ))}
                      </div>
                    )}
                  </div>
                              ))}
                </nav>
              </div>
            </aside>

            {/* Center Content Area */}
            <div className="flex-1 bg-white px-6 py-6 overflow-y-auto main-content">
              {currentContent && (
                <>
                  <h3 className="text-2xl font-semibold text-gray-900 mb-6">
                    {currentContent.content.title}
                  </h3>
                  
                  {/* Content - Proper Markdown rendering */}
                  <MarkdownContent content={currentContent.content.content} />
                  
                  {/* Mobile PDF, Images, and Sources - Only visible on small screens */}
                  <div className="lg:hidden mt-8 space-y-6">
                    {/* PDF Section */}
                    <div>
                      <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                        <FileText className="w-4 h-4 mr-2" />
                        PDF Original
                      </h4>
                      {currentContent?.content?.page_number ? (
                        <div className="space-y-3">
                          <button
                            onClick={() => handlePdfPageView(currentContent.content.page_number!)}
                            className="w-full bg-blue-50 p-3 rounded-lg hover:bg-blue-100 transition-colors text-left"
                          >
                            <div className="flex items-center space-x-2">
                              <FileText className="w-4 h-4 text-blue-600" />
                              <div>
                                <p className="text-sm font-medium text-blue-700">
                                  Page {currentContent.content.page_number}
                                </p>
                                <p className="text-xs text-blue-600">
                                  Click to view full page
                                </p>
                              </div>
                            </div>
                          </button>
                        </div>
                      ) : (
                        <div className="text-center py-4">
                          <FileText className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                          <p className="text-xs text-gray-500 mb-2">No page reference available</p>
                          <button
                            onClick={() => window.open(`https://arxiv.org/pdf/${paperData.arxiv_id}.pdf`, '_blank', 'noopener,noreferrer')}
                            className="px-3 py-2 bg-blue-600 text-white text-xs rounded-lg hover:bg-blue-700 transition-colors"
                          >
                            View Full PDF
                          </button>
                        </div>
                      )}
                    </div>

                    {/* Images Section */}
                    <div>
                      <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                        <ImageIcon className="w-4 h-4 mr-2" />
                        Images
                      </h4>
                      {imagesLoading ? (
                        <div className="text-center py-4">
                          <div className="animate-spin rounded-full h-6 w-6 border-b-2 border-blue-600 mx-auto"></div>
                          <p className="text-xs text-gray-500 mt-2">Loading images...</p>
                        </div>
                      ) : relevantImages.length > 0 ? (
                        <div className="grid grid-cols-2 gap-2">
                          {relevantImages.map((image, index) => (
                            <div
                              key={image.id || index}
                              className="aspect-square bg-gray-200 rounded-lg flex items-center justify-center cursor-pointer hover:bg-gray-300 transition-colors overflow-hidden group"
                              onClick={() => setSelectedImage(image)}
                            >
                              <img
                                src={image.url || `/api/image/${image.id}`}
                                alt={`Figure ${index + 1}`}
                                className="max-w-full max-h-full object-contain p-1 group-hover:scale-105 transition-transform"
                              />
                            </div>
                          ))}
                        </div>
                      ) : (
                        <div className="text-center py-4">
                          <ImageIcon className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                          <p className="text-xs text-gray-500">No images for this content</p>
                        </div>
                      )}
                    </div>

                    {/* Sources Section */}
                    <div>
                      <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                        <ExternalLink className="w-4 h-4 mr-2" />
                        Sources
                      </h4>
                      {contentCitations.length > 0 ? (
                        <div className="space-y-2">
                          {contentCitations.map((citation, index) => (
                            <div
                              key={index}
                              className="bg-gray-50 p-3 rounded-lg hover:bg-gray-100 transition-colors"
                            >
                              <div className="flex items-start space-x-2">
                                <div className="flex-1 min-w-0">
                                  <p className="text-xs font-medium text-gray-900 mb-1">
                                    Reference {index + 1}
                                  </p>
                                  <p className="text-xs text-gray-800 break-words">
                                    {citation}
                                  </p>
                                  <button
                                    onClick={() => handleCitationClick(citation)}
                                    className="inline-flex items-center text-xs text-blue-600 hover:text-blue-800 hover:underline mt-2"
                                  >
                                    {isYouTubeUrl(citation) ? (
                                      <Play className="w-3 h-3 mr-1" />
                                    ) : (
                                      <ExternalLink className="w-3 h-3 mr-1" />
                                    )}
                                    {isYouTubeUrl(citation) ? 'Watch Video' : 'View Source'}
                                  </button>
                                </div>
                              </div>
                            </div>
                          ))}
                        </div>
                      ) : (
                        <div className="text-center py-4">
                          <ExternalLink className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                          <p className="text-xs text-gray-500">No citations for this content</p>
                        </div>
                      )}
                    </div>
                  </div>
                </>
              )}
            </div>

            {/* Right Sidebar - PDF, Images, and Sources */}
            <aside className="w-96 bg-white flex-shrink-0 fixed top-16 bottom-0 overflow-y-auto scrollbar-hide hidden lg:block lg:right-32">
              <div className="p-6 space-y-6">
              
              {/* PDF Section */}
              <div>
                <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                  <FileText className="w-4 h-4 mr-2" />
                  PDF Original
                </h4>
                {currentContent?.content?.page_number ? (
                  <div className="space-y-3">
                    <button
                      onClick={() => handlePdfPageView(currentContent.content.page_number!)}
                      className="w-full bg-blue-50 p-3 rounded-lg hover:bg-blue-100 transition-colors text-left"
                    >
                      <div className="flex items-center space-x-2">
                        <FileText className="w-4 h-4 text-blue-600" />
                        <div>
                          <p className="text-sm font-medium text-blue-700">
                            Page {currentContent.content.page_number}
                          </p>
                          <p className="text-xs text-blue-600">
                            Click to view full page
                          </p>
                        </div>
                      </div>
                    </button>
                    <div className="p-3 bg-gray-50 rounded-lg">
                      <p className="text-xs text-gray-600 mb-2">
                        <strong>PDF Reference:</strong>
                      </p>
                      <p className="text-xs text-gray-700">
                        This content is sourced from page {currentContent.content.page_number} of the original PDF. 
                        Click above to view the full page with figures, tables, and original formatting.
                      </p>
                    </div>
                  </div>
                ) : (
                  <div className="text-center py-4">
                    <FileText className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                    <p className="text-xs text-gray-500 mb-2">No page reference available</p>
                    <button
                      onClick={() => window.open(`https://arxiv.org/pdf/${paperData.arxiv_id}.pdf`, '_blank', 'noopener,noreferrer')}
                      className="px-3 py-2 bg-blue-600 text-white text-xs rounded-lg hover:bg-blue-700 transition-colors"
                    >
                      View Full PDF
                    </button>
                  </div>
                )}
              </div>

              {/* Images Section */}
              <div>
                <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                  <ImageIcon className="w-4 h-4 mr-2" />
                  Images
                </h4>
                {imagesLoading ? (
                  <div className="text-center py-4">
                    <div className="animate-spin rounded-full h-6 w-6 border-b-2 border-blue-600 mx-auto"></div>
                    <p className="text-xs text-gray-500 mt-2">Loading images...</p>
                  </div>
                ) : relevantImages.length > 0 ? (
                  <div className="grid grid-cols-2 gap-2">
                    {relevantImages.map((image, index) => (
                      <div
                        key={image.id || index}
                        className="aspect-square bg-gray-200 rounded-lg flex items-center justify-center cursor-pointer hover:bg-gray-300 transition-colors overflow-hidden group"
                        onClick={() => setSelectedImage(image)}
                      >
                        <img
                          src={image.url || `/api/image/${image.id}`}
                          alt={`Figure ${index + 1}`}
                          className="max-w-full max-h-full object-contain p-1 group-hover:scale-105 transition-transform"
                        />
                      </div>
                    ))}
                  </div>
                ) : (
                  <div className="text-center py-4">
                    <ImageIcon className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                    <p className="text-xs text-gray-500">No images for this content</p>
                  </div>
                )}
                {relevantImages.length > 0 && (
                  <p className="text-xs text-gray-500 mt-2 text-center">
                    Click on an image to enlarge.
                  </p>
                )}
              </div>

              {/* Sources Section */}
              <div>
                <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                  <ExternalLink className="w-4 h-4 mr-2" />
                  Sources
                </h4>
                {contentCitations.length > 0 ? (
                  <div className="space-y-2">
                    {contentCitations.map((citation, index) => (
                      <div
                        key={index}
                        className="bg-gray-50 p-3 rounded-lg hover:bg-gray-100 transition-colors"
                      >
                        <div className="flex items-start space-x-2">
                          <div className="flex-1 min-w-0">
                            <p className="text-xs font-medium text-gray-900 mb-1">
                              Reference {index + 1}
                            </p>
                            <p className="text-xs text-gray-800 break-words">
                              {citation}
                            </p>
                            <button
                              onClick={() => handleCitationClick(citation)}
                              className="inline-flex items-center text-xs text-blue-600 hover:text-blue-800 hover:underline mt-2"
                            >
                              {isYouTubeUrl(citation) ? (
                                <Play className="w-3 h-3 mr-1" />
                              ) : (
                                <ExternalLink className="w-3 h-3 mr-1" />
                              )}
                              {isYouTubeUrl(citation) ? 'Watch Video' : 'View Source'}
                            </button>
                          </div>
                        </div>
                      </div>
                    ))}
                  </div>
                ) : (
                  <div className="text-center py-4">
                    <ExternalLink className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                    <p className="text-xs text-gray-500">No citations for this content</p>
                  </div>
                )}
                </div>
                
              </div>
            </aside>
          </div>
        </div>
      </main>

      {/* Image Modal with Close Button */}
      {selectedImage && (
        <div 
          className="fixed inset-0 bg-black bg-opacity-75 flex items-center justify-center z-50 p-4"
          onClick={() => setSelectedImage(null)}
        >
          <div className="relative max-w-4xl max-h-full" onClick={(e) => e.stopPropagation()}>
            <button
              onClick={() => setSelectedImage(null)}
              className="absolute top-4 right-4 text-white hover:text-gray-300 z-10 bg-black bg-opacity-50 rounded-full p-2"
            >
              <X className="w-6 h-6" />
            </button>
            <img
              src={selectedImage.url || `/api/image/${selectedImage.id}`}
              alt="Enlarged figure"
              className="max-w-full max-h-full object-contain rounded-lg"
            />
          </div>
        </div>
      )}

      {/* YouTube Modal */}
      {youtubeModal.isOpen && youtubeModal.videoId && (
        <div className="fixed inset-0 bg-black bg-opacity-75 flex items-center justify-center z-50 p-4">
          <div className="relative bg-white rounded-lg max-w-4xl w-full max-h-full">
            <button
              onClick={() => setYoutubeModal({ isOpen: false, videoId: null })}
              className="absolute top-4 right-4 text-gray-600 hover:text-gray-800 z-10"
            >
              <X className="w-8 h-8" />
            </button>
            <div className="p-4">
              <iframe
                width="100%"
                height="480"
                src={`https://www.youtube.com/embed/${youtubeModal.videoId}`}
                title="YouTube video player"
                frameBorder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowFullScreen
                className="rounded-lg"
              ></iframe>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}
