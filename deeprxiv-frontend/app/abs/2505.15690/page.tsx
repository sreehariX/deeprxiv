'use client';

import { useState, useEffect, useRef } from 'react';
import Link from 'next/link';
import { ArrowLeft, Image as ImageIcon, ExternalLink, BookOpen, Clock } from 'lucide-react';
import 'katex/dist/katex.min.css';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import remarkMath from 'remark-math';
import rehypeKatex from 'rehype-katex';

// Types for better TypeScript support
interface ImageData {
  id: string;
  page: number;
  original_position?: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  expanded_position?: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  path?: string;
  url?: string;
}

interface SubSection {
  id: string;
  title: string;
  content: string;
  citations?: string[];
  page_number?: number;
}

interface Section {
  id: string;
  title: string;
  content: string;
  citations?: string[];
  page_number?: number;
  subsections?: SubSection[];
}

// Paper data
const paperData = {
  id: 9,
  arxiv_id: '2505.15690',
  title: 'Toward Open Earth Science as Fast and Accessible as Natural Language',
  authors: 'Marquita Ellis, Iksha Gurung, Muthukumaran Ramasubramanian, Rahul Ramachandran',
  abstract: 'Is natural-language-driven earth observation data analysis now feasible with the assistance of Large Language Models (LLMs)? For open science in service of public interest, feasibility requires reliably high accuracy, interactive latencies, low (sustainable) costs, open LLMs, and openly maintainable software â€” hence, the challenge. What are the techniques and programming system requirements necessary for satisfying these constraints, and what is the corresponding development and maintenance burden in practice? This study lays the groundwork for exploring these questions, introducing an impactful earth science use-case, and providing a software framework with evaluation data and metrics, along with initial results from employing model scaling, prompt-optimization, and inference-time scaling optimization techniques.',
  processed: true
};

// Sections data
const sectionsData: Section[] = [{"id": "foundation-and-motivation", "title": "Foundations and Motivation for Democratizing Earth Science", "content": "## Foundations and Motivation for Democratizing Earth Science\n\nThis section explores the fundamental challenge addressed in the research paper: enabling democratized, natural language access to Earth observation data using Large Language Models (LLMs). It explains why this work matters by situating it within the broader context of Earth science research and the increasing data complexity faced by users. The content sets the stage for subsequent technical innovations by outlining the problem space and motivation behind making Earth science data analysis accessible beyond specialized informatics experts.\n\nUnderstanding these foundations is critical for grasping how the paper\'s proposed methods fit into the landscape of Earth science data management, open science, and societal applications such as climate monitoring and emergency response. It connects the core problem\u2014difficulty in querying vast and complex Earth datasets\u2014to the promise of natural language interfaces that exploit recent advances in LLM technology.\n\n---\n\n### Core Concepts and Problem Framing\n\nEarth science has entered an era characterized by explosive growth in data **volume**, **velocity**, and **variety/veracity**\u2014often called the \"7 V\'s\" of big data^1. These dimensions create barriers for many users to effectively find and analyze relevant satellite and environmental data. Traditional tools such as the Common Metadata Repository (CMR), SpatioTemporal Asset Catalogs (STAC), and Microsoft Planetary Computer improve access but usually require specialized query languages and knowledge of data formats, making them inaccessible to non-expert users^2.\n\nThe core challenge is how to lower these barriers while preserving the richness and precision needed for scientific inquiry. This research addresses this by reformulating geospatial data querying as a **Named Entity Recognition (NER)** task. Here, parameters such as geographic location (\`area\`), temporal window (\`date\`), and event type (\`event_type\`) are extracted automatically from *natural language* queries (e.g., \"flooding in Houston last week\") rather than through complicated code or query syntax. Table 1 in the paper succinctly illustrates these key parameters.\n\nThis approach must overcome complex **temporal reasoning** issues, such as interpreting relative time expressions (\"last season,\" \"this past Tuesday\") that require understanding context-dependent references and converting them into standard date formats (YYYY-MM-DD) for data retrieval^3. Similarly, geographic references can vary widely, from named regions to coordinate-based locations.\n\nMathematically, we can think of the system as a mapping function:\n\n\\[\nf: Q \\mapsto \\{ \\text{area}, \\text{date}, \\text{event_type} \\}\n\\]\n\nwhere \\( Q \\) is the natural language query and the output is a structured JSON object representing the parsed parameters. The goal is to maximize the accuracy of \\( f \\), particularly for the spatiotemporal interpretation that is critical to Earth science.\n\nThe system leverages LLMs, which have the ability to model contextual relationships in language and thus hold promise for robustly extracting these parameters. However, this requires careful prompt engineering, validation, and iterative optimization.\n\n---\n\n### Technical Details: Implementation and Evaluation\n\nThe paper introduces a novel dataset of over 100 carefully curated query-answer pairs developed with domain experts to train and evaluate the system. Each answer is a JSON object encoding the key parameters extracted from the query, ensuring standardization and validation^3. For example:\n\n\`\`\`json\n{\n  \"area\": \"Seoul\",\n  \"date\": \"2023-07-14\",\n  \"event_type\": \"flood\"\n}\n\`\`\`\n\nTemporal reasoning adds complexity because queries may include ambiguous or relative references. The dataset handles this by adding static reference dates (\"Today is June 4, 2024\") for evaluation to provide context for date resolution^3.\n\nTo enhance system performance, the authors use programmatic prompt optimization with the DSPy framework (detailed on page 12). DSPy allows automated tuning of LLM prompts and systematic metric-driven evaluation across multiple dimensions such as:\n\n- Valid JSON syntax\n- Presence and validity of required keys (\`area\`, \`date\`, \`event_type\`)\n- Consistency with the query text \n- Date equivalence and temporal consistency (including LLM-assisted judging)\n\nThe evaluation framework carefully distinguishes semantic correctness from syntactic errors, providing granular feedback for prompt and model refinement^3.\n\nTwo advanced inference-time scaling techniques are explored (see Figure 3 on page 15):\n\n- **Self-refinement:** An additional LLM pass revises initial outputs based on validation criteria\n- **Task decomposition:** Separating date extraction from location and event recognition and then synthesizing results via a multi-model pipeline\n\nThese approaches address the most challenging part\u2014temporal interpretation\u2014and improve both accuracy and efficiency^3.\n\n---\n\n### Significance and Broader Impact\n\nThis research is novel in its focus on democratizing Earth science data access by directly leveraging cutting-edge LLMs for natural language geospatial query interpretation. Unlike previous systems that require specialized knowledge or manual data preprocessing, it aims to place complex Earth observation capabilities into the hands of scientists, policymakers, educators, and even the interested public without coding expertise^1.\n\nThe approach has significant implications for open science and public good, facilitating wider use of critical Earth science data for climate monitoring, disaster response, and policy planning. It directly addresses challenges identified by agencies like NASA in maximizing data usability and outreach^1.\n\nBy introducing a validated dataset, rigorous evaluation metrics, and a modular software framework, this work lays a foundation for more maintainable, extensible, and transparent Earth data platforms powered by AI. It connects to broader research on data democratization, natural language interfaces, and AI-assisted scientific discovery, marking a significant advance in geospatial informatics^4.\n\n---\n\n^1 [NASA Data Democratization Challenges and Opportunities][1]\n\n^2 [Paper Introduction and Context, pages 1-4][3]\n\n^3 [Dataset Description, Evaluation Metrics, and Optimization, pages 5-17][3]\n\n^4 [Relevant work on democratizing environmental data access and AI applications][4]\n\n---\n\nThis section bridges the gap between the complexity of Earth observation data and the promise of AI-powered natural language querying, motivating the research\'s technical developments and their societal relevance.", "citations": ["https://ntrs.nasa.gov/citations/20240016399", "https://ntrs.nasa.gov/api/citations/20240016399/downloads/AMS_1219_zl_ja.pdf", "https://www.frontiersin.org/research-topics/20014/democratizing-data-environmental-data-access-and-its-future/magazine", "https://dse.berkeley.edu/programs/open-democratize-methods-geospatial-data-analysis", "https://dspace.mit.edu/bitstream/handle/1721.1/155172/lukacz-2024-imaginaries-of-democratization-and-the-value-of-open-environmental-data-analysis-of-microsoft-s-planetary.pdf?sequence=1&isAllowed=y"], "page_number": 1, "subsections": [{"id": "research-problem", "title": "Research Problem and Barriers", "content": "Here is an authoritative, comprehensive, and educational breakdown of the \u201cResearch Problem and Barriers\u201d section, leveraging the context and technical detail from the referenced paper and related literature.\n\n---\n\n## Understanding the Research Problem and Barriers in Earth Observation Data Analysis\n\nThis section sets the stage for understanding why and how barriers to accessing and analyzing Earth observation (EO) data hinder broader scientific progress, particularly for researchers without specialized informatics training. It connects these challenges to the technical and practical bottlenecks that shape the development and adoption of new tools for open science.\n\n### The Importance of Accessibility in Open Science\n\nEarth observation data\u2014information collected by satellites and other remote sensing technologies\u2014provides unprecedented opportunities for monitoring climate change, natural disasters, agriculture, and urban development[4][5]. However, despite the increase in data availability and quality, most researchers outside computer science or informatics find it difficult to use these datasets because current systems require mastery of specialized query languages (like SQL for geodatabases) and an understanding of complex data structures[3].\n\nThe result is a persistent gap between data availability and usability. Researchers in fields such as atmospheric science, policy planning, and emergency response are often unable to fully leverage these resources, limiting the impact of EO data on real-world decision-making and scientific discovery[5].\n\n---\n\n## Core Concepts and Challenges\n\n**Key Definitions and Problem Context**\n\n- **Earth Observation Data**: Data acquired via satellites or aerial sensors, capturing aspects of the Earth\u2019s surface and atmosphere.\n- **Analysis-Ready Data (ARD)**: Data that have been pre-processed to remove noise, correct for sensor differences, and be ready for immediate use in analysis[2].\n- **Natural Language Interfaces**: Systems that allow users to interact with data using ordinary language instead of programming code.\n- **Accessibility Gap**: The disparity between data availability and the technical proficiency required to access and analyze it.\n\n**Barriers to Broader Use**\n\n1. **Technical Complexity**: Current EO data platforms require knowledge of query languages and data schemas, posing a barrier for non-experts[3][4].\n2. **Data Heterogeneity and Interoperability**: Data from different sensors (multispectral, radar, etc.) are often stored in incompatible formats, complicating integration and analysis[1][2].\n3. **Quality Control and Consistency**: Ensuring high-quality and consistent annotations is challenging due to the diversity of surface features and the need for precise geolocation[1].\n4. **Collaboration and Policy**: Multi-sector collaboration is needed for effective use of EO data, but is hindered by technical incompatibilities, restrictive data policies, and intellectual property concerns[1][5].\n\n**Mathematical Foundations**\n\nConsider a user query \\( Q \\) expressed in natural language. The system must extract key parameters:\n- **Area**: \\( a \\) (spatial location)\n- **Date**: \\( d \\) (temporal window)\n- **Event Type**: \\( e \\) (phenomenon, e.g., flood)\n\nThe task is to map \\( Q \\) to a structured representation:\n$$\n\\text{Query} \\rightarrow \\{\\text{area}: a, \\text{date}: d, \\text{event\\_type}: e\\}\n$$\nThis is typically expressed as a Named Entity Recognition (NER) problem, where the system identifies and classifies the relevant entities in the query[Sec. 2.1, p. 7].\n\n**Example**\n\nGiven the query \u201cJuly 14, 2023, flooding in Seoul\u201d, the system should output:\n\`\`\`json\n{\"area\": \"Seoul\", \"date\": \"2023-07-14\", \"event_type\": \"flood\"}\n\`\`\`\nThis illustrates the transformation from natural language to structured data, which is then used to retrieve and analyze relevant EO data[Sec. 2.2, p. 8].\n\n---\n\n## Technical Details and Implementation\n\n**Methodological Choices**\n\nThe paper reframes EO data querying as an NER task, leveraging Large Language Models (LLMs) to convert natural language into structured queries. This approach avoids requiring users to learn query languages and instead allows them to interact with data in a familiar way[Sec. 2.1, p. 7].\n\n**Parameter Extraction and Validation**\n\nThe process involves:\n1. **Query Parsing**: The LLM parses the user\u2019s natural language query.\n2. **Entity Extraction**: The system extracts area, date, and event type.\n3. **Validation**: The output is validated for syntactical correctness (valid JSON), completeness (all keys present), and semantic accuracy (correct values for each key)[Sec. 2.5, p. 10-11].\n\n**Algorithmic Overview**\n\n\`\`\`python\ndef extract_parameters(query):\n    # Use LLM to parse the query and extract area, date, event_type\n    response = llm_generate(query)\n    json_response = validate_json(response)\n    if not json_response:\n        return {\"error\": \"Invalid JSON\"}\n    # Validate keys and values\n    if not all(k in json_response for k in [\"area\", \"date\", \"event_type\"]):\n        return {\"error\": \"Missing required keys\"}\n    return json_response\n\`\`\`\nThis pseudocode outlines the core logic for parameter extraction and validation[Sec. 2.5, p. 11].\n\n**Technical Challenges and Design Decisions**\n\n- **Temporal Reasoning**: Interpreting relative time references (e.g., \u201clast week\u201d, \u201csince 2020\u201d) is particularly challenging. The system must infer the correct dates based on the context.\n- **Entity Disambiguation**: Handling ambiguous or incomplete queries (e.g., \u201cflooding in Houston\u201d) requires robust error detection and feedback.\n- **Extensibility**: The system is designed to easily integrate new event types and data sources[Sec. 2.4, p. 9].\n\n**Use of Evaluation Metrics**\n\nThe paper introduces 11 metrics for systematic evaluation, including:\n- **Valid JSON**: Ensures output is well-formed.\n- **All Required Keys Present**: Ensures completeness.\n- **Date Equivalence/Consistency**: Measures accuracy of temporal extraction[Sec. 2.5, p. 11].\n\n---\n\n## Significance and Connections\n\n**Novelty and Broader Impact**\n\nThis approach is significant because it bridges the gap between powerful EO datasets and the researchers who need them, democratizing access to complex data[5]. By leveraging LLMs and natural language interfaces, the system empowers a wider range of users to perform advanced analyses without requiring deep technical expertise.\n\n**Connections to Related Work**\n\nThe paper\u2019s methodology aligns with broader efforts to standardize and harmonize EO data, such as the Global Land Imaging ARD commons[2]. It also addresses challenges in data integration, quality control, and collaboration highlighted in recent EO literature[1][3].\n\n**Innovations**\n\n- **Natural Language Interface**: Enables intuitive querying for non-experts.\n- **Systematic Evaluation**: Introduces a rigorous, reproducible framework for assessing system performance.\n- **Extensibility**: Designed for easy addition of new data sources and analysis types.\n\n**Implications for the Field**\n\nThe approach has implications for open science, enabling more researchers to contribute to and benefit from EO data. It also sets a precedent for future systems that prioritize accessibility, usability, and interoperability in scientific data management[5].\n\n---\n\n## Summary Table: Key Barriers and Solutions\n\n| Barrier                        | Description                                              | Solution/Innovation                  |\n|---------------------------------|---------------------------------------------------------|--------------------------------------|\n| Technical complexity            | Need for query language expertise                       | Natural language interface           |\n| Data heterogeneity              | Incompatible formats, multi-sensor data                 | Standardization, ARD                 |\n| Quality control                 | Inconsistent annotations, geolocation errors            | Automated validation, LLM-as-judge   |\n| Collaboration issues            | Policy, trust, resource gaps                            | Multi-sector partnerships            |\n\n---\n\n## Educational Takeaways\n\n- **Accessibility is Key**: The true value of EO data is realized only when it is accessible to a broad range of users[5].\n- **Natural Language Interfaces Lower Barriers**: Allowing researchers to query data in plain language removes a significant obstacle to adoption[Sec. 1, p. 4].\n- **Systematic Evaluation Matters**: Robust metrics and validation processes are essential for reliable and maintainable systems[Sec. 2.5, p. 11].\n- **Collaboration Drives Progress**: Multi-sector partnerships and standardized data practices are crucial for scaling the impact of EO data[1][2].\n\n---\n\nThis educational breakdown equips researchers and graduate students with a clear understanding of the research problem, the technical and organizational barriers, and the innovative solutions proposed in the paper, while connecting these ideas to the broader landscape of open science and data-driven research.", "citations": ["https://kili-technology.com/data-labeling/earth-observation-data-labeling-guide", "https://www.spectralreflectance.space/p/the-challenge-of-analysis-ready-data-in-earth-observation-d978ea1df97", "https://www.mdpi.com/2071-1050/17/1/145", "https://iaes.cgiar.org/spia/news/challenges-using-earth-observation-data-impact-evaluation", "https://rtiinnovationadvisors.org/unlocking-the-potential-of-earth-observation-data-a-path-to-equitable-impact/"], "page_number": 1}, {"id": "llm-potential", "title": "Potential of LLMs in Earth Science", "content": "Potential of LLMs in Earth Science\n\n## Introduction\n\nThis section explores the transformative role of Large Language Models (LLMs) in Earth science, focusing on how they enable intuitive, natural language-driven access to complex geospatial data. The current challenges in Earth science data analysis\u2014stemming from the need for specialized query languages, complex metadata, and expert knowledge\u2014create significant barriers for researchers and practitioners outside core informatics fields. LLMs offer a compelling solution by allowing users to interact with massive datasets using everyday language, thus democratizing data access and empowering a broader audience for applications such as climate monitoring, emergency response, and policy planning (pp. 1\u20132).\n\nUnderstanding this topic is crucial, as it underpins the paper\u2019s central thesis: LLMs can bridge the gap between complex technical infrastructures and real-world scientific, societal, or policy needs. By automating the interpretation of spatiotemporal relationships and robustly extracting parameter values from natural language queries, LLMs reduce cognitive load and increase efficiency in data retrieval and analysis. This section also highlights the broader implications for open science, setting the stage for the technical and methodological innovations discussed throughout the paper (pp. 2\u20133)[1][3][5].\n\n## Core Content\n\n### Key Concepts and Definitions\n\n**Named Entity Recognition (NER):**\nNER is a process in natural language processing (NLP) where the model identifies and classifies key information from text into predefined categories, such as location, date, and event type. In the context of Earth science, NER enables the system to extract the spatial area, temporal window, and event description from a user\u2019s natural language query (p. 2.1).\n\n**Spatiotemporal Reasoning:**\nThis refers to the ability to interpret and reason over both space (geographic location) and time (dates or time periods). Earth science queries often involve implicit spatiotemporal constraints, such as \u201clast week\u201d or \u201csince 2020,\u201d which require sophisticated temporal reasoning to resolve (pp. 1\u20132).\n\n**Prompt Engineering and Optimization:**\nPrompt engineering involves designing input prompts to guide LLMs toward desired outputs. In this study, prompt optimization is used to improve the accuracy and reliability of LLM-generated answers, particularly for complex spatiotemporal queries (pp. 2.6, 3.2).\n\n### Mathematical Foundations\n\nThe core task can be formalized as mapping a natural language query $q$ to a structured output $y$:\n\n\\[\ny = f(q)\n\\]\nwhere $y = \\{\\text{area}, \\text{date}, \\text{event\\_type}\\}$\u2014the parameters required for geospatial data retrieval (Table 1, p. 2.1).\n\nThis mapping is implemented as a probabilistic function, leveraging the LLM\u2019s ability to interpret context and extract relevant entities:\n\n\\[\nP(y|q) = \\prod_{i} P(y_i|q, y_{<i})\n\\]\nwhere $y_i$ is the $i$-th component of the output (e.g., area, date, event type).\n\n### Illustrative Examples\n\nConsider the following query-answer pair (p. 2.2):\n\n- **Query:** \"July 14, 2023, flooding in Seoul\"\n- **Answer:** \`{\"area\": \"Seoul\", \"date\": \"2023-07-14\", \"event_type\":\"flood\"}\`\n\nHere, the LLM must correctly identify the location \"Seoul,\" the date \"July 14, 2023,\" and the event type \"flood\" from the query.\n\nA more challenging example involves relative time references:\n\n- **Query:** \"Find burn scars in the Andes Mountains from last season.\"\n- **Answer:** \`{\"area\": \"Andes Mountains\", \"date\":\"2024-03-01\", \"event_type\":\"burn_scars\"}\`\n\nThe LLM must infer the correct date range (\u201clast season\u201d) and map it to a specific date, demonstrating advanced temporal reasoning and implicit constraint resolution.\n\n### Methodological Reasoning\n\nThe paper reformulates geospatial data querying as an NER task, leveraging LLMs\u2019 strengths in natural language understanding rather than requiring users to write code or learn specialized query languages. This approach is chosen for its simplicity, extensibility, and the ease with which new analysis types (e.g., event types) can be added over time (pp. 2.1, 2.3).\n\nThe evaluation dataset was co-developed with domain scientists, ensuring that it reflects real-world use cases and is rigorously validated (pp. 2.2, 2.3). The inclusion of both manual and automated validation, as well as the use of LLM-as-a-judge for refinement, ensures high data quality and robustness (pp. 2.3, 3.1).\n\n## Technical Details\n\n### Implementation Overview\n\nThe system architecture involves the following steps (pp. 2.1, 2.4):\n\n1. **Input Parsing:** The user submits a natural language query.\n2. **Entity Extraction:** The LLM extracts key parameters (area, date, event type) using NER.\n3. **Temporal Reasoning:** The LLM resolves relative time references and infers implicit constraints.\n4. **Output Generation:** The extracted parameters are formatted into a structured output (e.g., JSON).\n\n### Algorithm and Pseudocode\n\nBelow is a conceptual pseudocode for the core pipeline (pp. 2.1, 2.6):\n\n\`\`\`python\ndef process_query(query: str, today=None) -> dict:\n    # Optional augmentation: add today\'s date to the prompt for relative dates\n    if today:\n        query += f\" Today is {today}.\"\n\n    # Use LLM for entity extraction and temporal reasoning\n    prompt = f\"\"\"Extract area, date, and event_type from the following query.\nQuery: {query}\nOutput: {{\"area\": \"...\", \"date\": \"...\", \"event_type\": \"...\"}}\"\"\"\n    output = llm.generate(prompt)\n\n    # Parse and validate output\n    output_json = parse_json(output)\n    if not output_json or not all(k in output_json for k in [\"area\", \"date\", \"event_type\"]):\n        return {\"error\": \"Unable to extract required parameters\"}\n\n    return output_json\n\`\`\`\n\n### Parameter Choices and Design Decisions\n\n- **Entity Recognition:** Focused on area, date, and event type to cover the most common use cases. Additional parameters (e.g., sensor type, resolution) can be added as needed (p. 2.1).\n- **Prompt Engineering:** Used DSPy for systematic prompt optimization, enabling reproducible and maintainable prompt design (pp. 2.6, 3.2).\n- **Evaluation Metrics:** Defined 11 metrics to assess answer quality, including JSON validity, parameter completeness, and temporal consistency (pp. 2.5, 3.1).\n- **Task Decomposition:** Applied self-refinement and mixture-of-experts approaches to improve complex temporal reasoning (pp. 3.3).\n\n### Figure and Table References\n\n- **Table 1:** Key parameters for approach validation (area, date, event_type) (p. 2.1).\n- **Figure 1:** Performance comparison of LLaMA 3.1 405B vs. 8B, and categorization of date equivalence failures (p. 3.1).\n- **Figure 2:** Date equivalence and LLM-judged consistency performance across prompting techniques and models (p. 3.2).\n- **Figure 3:** Illustration of inference-time scaling optimizations (self-refinement and task decomposition) (p. 3.3).\n\n## Significance and Connections\n\nThis approach represents a significant advance in Earth science data analysis by lowering the technical barrier for non-experts and enabling more equitable access to geospatial data. The systematic use of LLMs for parameter extraction, combined with advanced prompt engineering and rigorous evaluation, demonstrates a novel application of modern NLP techniques to a traditionally complex domain[1][3][5].\n\nThe paper\u2019s innovations include:\n- **A rigorously validated, domain-relevant evaluation dataset.**\n- **A flexible, extensible pipeline for parameter extraction and temporal reasoning.**\n- **Systematic application of prompt optimization and inference-time scaling techniques.**\n\nThese contributions are situated within a broader trend toward domain-specific LLMs (e.g., GeoGalactica, INDUS) and open science initiatives, highlighting the potential for LLMs to transform not only data access but also the entire research lifecycle\u2014from goal definition to publication (pp. 3.1, 3.3)[1][3][5].\n\nBy connecting to related work and ongoing developments in geoscience and AI, this section positions the paper\u2019s technical and methodological choices within a rapidly evolving research landscape, emphasizing the importance of transparency, reproducibility, and continuous evaluation in deploying LLMs for real-world scientific applications.", "citations": ["https://www.earthdata.nasa.gov/news/blog/architecting-future-vision-using-large-language-models-enable-open-science", "https://onlinelibrary.wiley.com/doi/10.1111/exsy.13654", "https://science.nasa.gov/open-science/ai-language-model-science-research/", "https://arxiv.org/abs/2402.03349", "https://arxiv.org/html/2401.00434v1"], "page_number": 1}]}, {"id": "approach-and-methodology", "title": "Methodology and System Design", "content": "Here is an expanded, educational breakdown for the \"Methodology and System Design\" section, tailored for advanced researchers and graduate students. The content is structured according to the specified principles and formatting requirements.\n\n## Introduction: Purpose and Context\n\nThe \"Methodology and System Design\" section is central to understanding how the research team transformed traditional geospatial data retrieval into a task that leverages natural language and Large Language Models (LLMs), rather than specialized query languages. This shift is crucial for broadening access to Earth observation data, as it removes barriers for non-specialist researchers and practitioners by allowing them to express queries in plain language[1][5].\n\nUnderstanding this methodology is essential for grasping the novel system architecture and its real-world applicability. The section details the research and engineering process\u2014from rigorous dataset creation and validation, through the application of state-of-the-art NLP techniques like Named Entity Recognition (NER), to the systematic evaluation and optimization of system performance. This approach not only democratizes data access but also ensures the system is robust, extensible, and maintainable\u2014qualities that are foundational for open science initiatives[1][5].\n\n## Core Content: Key Concepts and Methodological Choices\n\n### 1. From Query Language to Natural Language\n\nTraditional geospatial data systems require users to master domain-specific query languages and data structures, which can be a significant barrier. In this project, querying is reformulated as a Named Entity Recognition (NER) task. NER is a branch of information extraction that identifies and classifies key entities in text\u2014such as location, date, and event type\u2014allowing the system to extract the necessary parameters from natural language queries[1][5].\n\n**Example:**  \nA user asks: \"Show flood events in Houston during June 2024.\"  \nThe system extracts:  \n- **area:** Houston  \n- **date:** 2024-06  \n- **event_type:** flood  \nas key parameters for retrieving relevant data[5].\n\n### 2. Rigorous Dataset Creation and Validation\n\nA cornerstone of the project is the creation of a meticulously curated evaluation dataset. Over 100 unique query-answer pairs were collaboratively crafted with domain scientists and validated through both manual and automated checks. The dataset ensures that the system meets real-world user needs and provides a benchmark for ongoing optimization[5].\n\n**Example Pair:**  \n**Query:** \"July 14, 2023, flooding in Seoul\"  \n**Answer:**  \n\`\`\`json\n{\n  \"area\": \"Seoul\",\n  \"date\": \"2023-07-14\",\n  \"event_type\": \"flood\"\n}\n\`\`\`\nThis process addresses the complexity of interpreting explicit and implicit temporal references, such as \"last season\" or \"this Friday,\" by ensuring the dataset includes a wide range of temporal expressions[5].\n\n### 3. LLM-as-a-Judge for Data Refinement\n\nTo further improve dataset quality and system robustness, the team incorporated an \"LLM-as-a-Judge\" mechanism. Here, an LLM is prompted to evaluate whether generated answers (especially dates) are consistent with the original query and to provide a rationale. This step uncovered subtle errors (e.g., off-by-one date calculations) and ambiguities in the golden answers, demonstrating the value of automated validation as the dataset grows[5].\n\n### 4. Systematic Evaluation Metrics\n\nThe system is evaluated against a set of 10 deterministic and 1 LLM-assisted metric. These metrics assess both syntactic and semantic aspects of answer generation, such as:\n- **Valid JSON:** Is the output parsable as JSON?\n- **Contains Expected Error Message:** Is an error message provided when necessary?\n- **Valid Key Names:** Are only permitted keys present?\n- **All Required Keys Present:** Are all necessary parameters included?\n- **Valid Event Type:** Is the event type supported?\n- **Equivalence and Consistency:** Are area, event, and date values equivalent to the golden answer and consistent with the query?\nThese metrics enable a nuanced understanding of system strengths and weaknesses, especially in the challenging task of date extraction[5].\n\n### 5. Mathematical Formulations\n\nLet the user query be denoted as $Q$, and the system output as $A$. For each answer $A$, we can define a scoring function for each metric. For example, for **date equivalence**:\n\n\\[\n\\text{date\\_score}(A) = \n\\begin{cases} \n1 & \\text{if } A.\\text{date} \\equiv G.\\text{date} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\nwhere $G$ is the golden answer[5].\n\n## Technical Details: Implementation and Design Decisions\n\n### 1. LLM Integration and Prompt Engineering\n\nThe system leverages open-source LLMs such as LLaMA 3.1, with different model sizes tested for robustness and performance. Initial prompts were handwritten and later translated into the DSPy framework for systematic optimization and evaluation[5].\n\n**Key Design Choices:**\n- **Model Scaling:** Performance is tested across LLaMA 3.1 8B to 405B models to balance accuracy and computational cost.\n- **Prompt Optimization:** Techniques include zero-shot, few-shot, Chain of Thought (CoT), and MIPRO prompting, each systematically evaluated for impact on accuracy and token usage.\n- **Error Handling:** The system is designed to generate clear error messages when parameters cannot be extracted, supporting user feedback and system maintainability[5].\n\n### 2. DSPy Framework for Programmatic Optimization\n\nThe DSPy framework is used to systematically optimize prompts and evaluate system performance. DSPy supports:\n- **Automated Prompt Formatting:** Ensures clear expectations for output fields and formats.\n- **Systematic Evaluation:** Codifies metrics and data in a standardized format, facilitating reproducible research and continuous improvement.\n- **Multi-Objective Optimization:** While DSPy primarily supports single-objective optimization, the research team developed workarounds for multi-objective evaluation, enabling detailed analysis of trade-offs between different metrics[5].\n\n**Pseudocode Example:**\n\`\`\`python\n# Define DSPy signature for NER task\nclass GeoNER(Signature):\n    query = InputField(desc=\"Natural language query\")\n    area = OutputField(desc=\"Extracted location\")\n    date = OutputField(desc=\"Extracted date\")\n    event_type = OutputField(desc=\"Extracted event type\")\n    error = OutputField(desc=\"Error message if any\")\n\n# Automated prompt optimization with DSPy\noptimizer = DSPyOptimizer(model=LLaMA3_8B)\noptimized_prompt = optimizer.optimize(GeoNER, evaluation_metrics)\n\`\`\`\n\n**Example from Paper:**  \nFigure 2 (page X) illustrates the performance of different prompting techniques and model variants, comparing metrics such as date equivalence and LLM-judged consistency, as well as token usage per query[5].\n\n### 3. Inference-Time Scaling and Task Decomposition\n\nTo address remaining performance gaps (especially in date extraction), the system employs inference-time scaling optimizations:\n- **Self-Refinement:** A two-step process where one model generates an answer and another refines it.\n- **Task Decomposition:** Different models are assigned to specific subtasks (e.g., date interpretation, area extraction), and their outputs are synthesized by a third model for a coherent final answer.\n\n**Illustration:**  \nFigure 3 (page Y) shows the architecture for single-step, self-refinement, and task decomposition workflows[5].\n\n## Significance and Connections to Broader Research\n\n### 1. Novel Contributions and Innovations\n\nThis approach is novel in several ways:\n- **Natural Language Interface:** Replaces complex query languages with intuitive natural language, broadening access to geospatial data.\n- **Rigorous Evaluation:** Introduces a new, domain-expert-validated dataset and a comprehensive set of evaluation metrics, setting a benchmark for future research.\n- **Extensibility:** The system is designed for easy integration of new data types, event types, and even speech-based input, ensuring long-term maintainability and adaptability[5].\n\n### 2. Broader Implications\n\nBy demonstrating high accuracy (near 100% on most metrics) and low latency, this work shows the feasibility of LLM-powered interfaces for scientific data retrieval. The framework is applicable not only to Earth science but also to other domains requiring intuitive access to complex data[5].\n\nThe methodology and system design connect to broader trends in AI and NLP, such as the use of LLMs for information extraction, the importance of rigorous evaluation, and the need for systems that are both robust and accessible. The project also highlights ongoing challenges\u2014such as the complexity of temporal reasoning and the need for clear specification and documentation in AI systems\u2014that are relevant to the wider research community[5].\n\n## Summary Table: Key Parameters and Design Decisions\n\n| Parameter         | Description                                      | Evaluation Metric         |\n|-------------------|--------------------------------------------------|--------------------------|\n| area              | Physical location of interest                    | area_equivalence, consistency |\n| date              | Date or date range of interest                   | date_equivalence, consistency |\n| event_type        | Type of event or analysis (e.g., flood)          | valid_event_type, equivalence, consistency |\n| error             | Error message if extraction fails                 | contains_error_message   |\n\n## Page and Figure References\n\n- **Dataset Creation and Validation:** Page 7\u20139\n- **LLM-as-a-Judge:** Page 9\u201310\n- **Evaluation Metrics:** Page 10\u201312\n- **Model Scaling and Prompt Optimization:** Page 13\u201315 (Figure 1, Figure 2)\n- **Inference-Time Scaling:** Page 16\u201317 (Figure 3)\n\nThis comprehensive breakdown ensures that readers\u2014whether new to the field or experienced researchers\u2014gain a deep, practical understanding of the methodology and system design, while appreciating the novelty and broader implications of the work[5].", "citations": ["https://developers.arcgis.com/python/latest/guide/how-named-entity-recognition-works/", "https://journal.code4lib.org/articles/15405", "https://learn.microsoft.com/en-us/sql/relational-databases/spatial/query-spatial-data-for-nearest-neighbor?view=sql-server-ver17", "https://www.elastic.co/blog/hybrid-geospatial-rag-application-elastic-amazon-bedrock", "https://aclanthology.org/2024.eacl-demo.13.pdf"], "page_number": 2, "subsections": [{"id": "problem-formulation", "title": "Problem Formulation and Key Parameters", "content": "## Introduction to Problem Formulation and Key Parameters\n\nThis section explains how geospatial data querying\u2014traditionally a domain requiring specialized query languages\u2014can be reformulated as an accessible Named Entity Recognition (NER) task. By leveraging natural language processing (NLP) and Large Language Models (LLMs), users can simply ask questions in everyday language, and the system will extract essential parameters needed to retrieve and analyze Earth observation data[1][3].\n\nUnderstanding this formulation is crucial because it bridges the gap between vast Earth science datasets and users who may not have technical expertise in geospatial or data query languages. This approach significantly lowers barriers to entry and empowers a broader audience\u2014such as researchers, policymakers, and emergency responders\u2014to access critical information for scientific and societal applications[2]. The section fits into the broader research by outlining the minimal requirements for validating this concept and demonstrating how these parameters can be extended for more complex use cases.\n\n## Core Methodology: NER in Geospatial Data Querying\n\n### Key Concepts and Definitions\n\n**Named Entity Recognition (NER):**  \nNER is a process in NLP that identifies and classifies key entities in text\u2014such as locations, dates, organizations, and event types\u2014into predefined categories[1][3]. For geospatial data querying, core extracted entities are:\n- **Area:** The physical location of interest (e.g., \"Seoul\", \"Andes Mountains\")\u2014anywhere on Earth.\n- **Date:** The temporal range or specific date of interest, which can be explicit (\"July 14, 2023\") or relative (\"last season\", \"from the past week\").\n- **Event Type:** The descriptor corresponding to supported analysis types (e.g., \"flood\", \"burn_scars\", \"crops\")[3].\n\nThese parameters form the minimal set required for concept validation and can be extended to include additional properties such as sensor type or image granularity[4].\n\n### Mathematical Formulation\n\nGiven a user query \\( q \\), the system aims to extract a structured representation \\( R \\) containing the essential parameters:\n\n\\[\nR = \\begin{cases}\n\\text{area: } a \\\\\n\\text{date: } d \\\\\n\\text{event\\_type: } e\n\\end{cases}\n\\]\n\nwhere \\( a \\) is a location, \\( d \\) is a date or date range, and \\( e \\) is a supported event type. The output is typically a JSON object, as shown in the paper\'s examples (page 2, Table 1):\n\n\`\`\`json\n{\n  \"area\": \"Seoul\",\n  \"date\": \"2023-07-14\",\n  \"event_type\": \"flood\"\n}\n\`\`\`\nThis structure is robust enough to accommodate additional properties, such as \"sensor_type\" or \"resolution\", by extending the JSON schema[2][4].\n\n### Rationale Behind Parameter Choices\n\nThe selection of area, date, and event type as core parameters is driven by their necessity for most spatiotemporal analyses in Earth science. Users typically need to specify:\n- **Where** the event occurred (area),\n- **When** it happened (date or date range),\n- **What** type of event or analysis is requested (event type).\n\nThese parameters align closely with the user\u2019s conceptual model of a query and allow the system to leverage geospatial foundation models (e.g., Prithvi) for downstream analysis (page 5). The extensibility to additional parameters ensures the system can evolve with new use cases, data types, and analytical needs.\n\n### Example Use Case\n\nConsider the user query:\n\n> \"July 14, 2023, flooding in Seoul\"\n\nThe system extracts:\n- **area:** \"Seoul\"\n- **date:** \"2023-07-14\"\n- **event_type:** \"flood\"\n\nIf the query contains relative time (\"last season\"), the system infers the appropriate date from the context, as described in the evaluation section (page 4)[2][3].\n\n## Technical Details and Implementation\n\n### Algorithm Overview\n\nThe workflow for extracting geospatial query parameters as NER tasks can be summarized as follows:\n\n1. **Text Input:** Receive a natural language query from the user.\n2. **Text Preprocessing:** Tokenize the query and assign part-of-speech tags.\n3. **Feature Extraction:** Derive features (contextual, orthographic, lexical) to aid entity recognition[3].\n4. **Model Application:** Apply a trained NER model to classify each token into predefined entity categories.\n5. **Entity Classification:** Assign labels (area, date, event type) to tokens.\n6. **Post-Processing:** Refine output, resolve ambiguities, and handle nested entities.\n7. **Output Generation:** Produce a structured output (e.g., JSON) highlighting the extracted entities[3].\n\n### Pseudocode Example\n\n\`\`\`python\ndef extract_geospatial_entities(query: str):\n    # 1. Preprocess query (tokenize, POS tagging)\n    tokens = preprocess(query)\n    # 2. Extract features (orthographic, contextual, etc.)\n    features = extract_features(tokens)\n    # 3. Apply NER model to classify entities\n    entities = ner_model.predict(features)\n    # 4. Post-processing (resolve ambiguities, format output)\n    result = postprocess(entities)\n    # 5. Generate structured output\n    return {\n        \"area\": result[\"area\"],\n        \"date\": result[\"date\"],\n        \"event_type\": result[\"event_type\"]\n    }\n\`\`\`\n\n### Parameter Choices and Design Decisions\n\nThe choice of JSON as the output format ensures interoperability with web-based and cloud services, as well as easy extensibility for new parameters (page 2, Table 1). The use of LLMs for both entity recognition and post-processing (e.g., resolving ambiguities in date ranges) allows the system to handle complex queries and implicit constraints[2][3].\n\nThe evaluation dataset (100+ manually crafted and validated query-answer pairs) ensures the system is robust and aligned with real-world user needs. For example, handling relative time references (\"last week\", \"this Friday\") is delegated to the model\u2019s temporal reasoning capabilities, with deterministic checks and LLM-as-a-judge mechanisms for validation (page 4)[2].\n\n## Significance and Broader Connections\n\n### Innovations and Contributions\n\nReformulating geospatial querying as an NER task is a novel approach that democratizes access to Earth science data. It enables users to query complex datasets using natural language, without requiring expertise in domain-specific query languages or data structures[1][2]. This approach is validated through a rigorously curated evaluation dataset and systematic performance metrics (page 4, Section 2.5), ensuring both accuracy and maintainability.\n\nThe system also introduces extensible parameterization, allowing for the addition of new analysis types, sensor types, or data attributes as the underlying geospatial foundation models evolve (page 6, Figure 1). This design choice anticipates future needs and simplifies integration with new data sources and analytical tools.\n\n### Connections to Related Work\n\nThis work builds on advances in both NLP and geospatial data management. Prior systems like Common Metadata Repository (CMR) and SpatioTemporal Asset Catalogs (STAC) require specialized knowledge for query formulation[4]. By contrast, this approach leverages the power of LLMs to interpret natural language queries, making geospatial data accessible to a wider audience[2][4].\n\nThe use of programmatic prompt optimization (e.g., DSPy) and systematic evaluation metrics aligns with best practices in AI and machine learning, ensuring robust, maintainable, and transparent systems (page 9, Section 2.6 and 3.2)[4].\n\n### Implications for the Field\n\nThis approach has significant implications for open science, enabling broader participation in Earth observation and analysis. It supports applications in climate monitoring, disaster response, and policy planning by making complex data accessible through simple, intuitive queries. The framework\u2019s extensibility and evaluation rigor set a foundation for future research and collaboration in open Earth science.\n\n---\n\n**Key Takeaways:**\n- **Core Parameters:** Area, date, and event type form the minimal set for geospatial data querying as an NER task (page 2, Table 1).\n- **Mathematical Structure:** Output is a structured object (JSON), extensible for new properties.\n- **Technical Workflow:** Text preprocessing, feature extraction, model application, post-processing, and structured output generation.\n- **Innovation:** Natural language querying of geospatial data, validated by a curated dataset and systematic evaluation.\n- **Broader Impact:** Democratizes access to Earth science data, supports open science, and enables future extensions.", "citations": ["https://developers.arcgis.com/python/latest/guide/how-named-entity-recognition-works/", "https://pro.arcgis.com/en/pro-app/latest/tool-reference/geoai/how-entity-recognition-works.htm", "https://encord.com/blog/named-entity-recognition/", "https://journal.code4lib.org/articles/15405", "https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/query/geospatial"], "page_number": 2}, {"id": "evaluation-dataset", "title": "Evaluation Data Set Creation and Refinement", "content": "Below is a comprehensive, educational breakdown for the \"Evaluation Data Set Creation and Refinement\" section, as if crafted for an advanced research audience.\n\n---\n\n## Introduction to Evaluation Dataset Creation and Refinement\n\nThis section explores how a robust evaluation dataset is constructed and refined for assessing the performance of Large Language Models (LLMs) applied to geospatial natural language queries. The process is critical for ensuring that the system not only understands and extracts the correct information (e.g., location, date, event type) from user queries but also generalizes well to the nuances and ambiguities present in real-world data. By involving domain scientists and incorporating both manual and automated validation, the dataset becomes a reliable benchmark for systematic optimization, iterative improvement, and real-world applicability (see page 7, \"2.2 Evaluation Data Set Creation\")[1].\n\nUnderstanding this process is essential for researchers who aim to develop or evaluate LLM-based systems for specific domains. The quality, diversity, and balance of the evaluation data directly influence the validity, fairness, and utility of system benchmarking\u2014factors that are crucial for advancing open science and building trust in automated analysis tools[1][2].\n\n---\n\n## Core Content: From Creation to Validation\n\n### Definition of Key Concepts\n\n- **Evaluation Dataset:** A collection of query-answer pairs designed to test and measure the performance of a system. In this context, each query is a natural language question about a geospatial event, and the answer is a JSON object encoding the key parameters: area, date, and event type (as defined in Table 1, page 7).\n- **Data Refinement:** The process of improving the dataset through annotation, error correction, and systematic validation to ensure accuracy and consistency.\n- **LLM-as-a-Judge:** Using an LLM to automatically validate and critique the system\u2019s output for logical consistency and correctness (page 9, \"2.3 Semi-Automatic Data Set Refinement\")[2][3].\n\n### Example-Based Illustration\n\nConsider two typical queries and their expected answers:\n\n- **Query:** \"July 14, 2023, flooding in Seoul\"\n  - **Answer:** \`{\"area\": \"Seoul\", \"date\": \"2023-07-14\", \"event_type\":\"flood\"}\`\n- **Query:** \"Find burn scars in the Andes Mountains from last season. Today is June 4, 2024.\"\n  - **Answer:** \`{\"area\": \"Andes Mountains\", \"date\":\"2024-03-01\", \"event_type\":\"burn_scars\"}\`\n\nThese examples highlight the need for both explicit and relative date handling, as well as the importance of clear data augmentation (e.g., appending \"Today is...\") for deterministic evaluation of queries involving relative time references (page 7, \"2.2 Evaluation Data Set Creation\")[1].\n\n### Reasoning Behind Methodological Choices\n\nThe dataset was created in collaboration with domain scientists to ensure that it reflects real-world use cases and user needs. Manual and automated validation were combined to maximize both correctness and efficiency. Specifically, LLM-as-a-Judge was employed to identify inconsistencies and ambiguities that might escape human reviewers\u2014catching errors such as off-by-one date calculations or assumptions about \"today\u2019s date\" that were only implicit in the answers (page 9, \"2.3 Semi-Automatic Data Set Refinement\")[2].\n\n---\n\n## Technical Details: Implementation and Validation\n\n### Data Creation Workflow\n\n1. **Manual Crafting:** Domain scientists and researchers manually author over 100 unique query-answer pairs, ensuring coverage of key parameters and edge cases.\n2. **Automated Validation:** The dataset is subjected to automated checks for JSON validity, key presence, and value consistency.\n3. **LLM Validation:** Each query-answer pair is reviewed by an LLM-judge, which flags inconsistencies or ambiguities and provides rationales for its judgments.\n4. **Data Augmentation:** Relative time references (e.g., \"last week\") are augmented with explicit dates for deterministic evaluation, exemplified by the addition of \"Today is...\" to queries (page 7, \"2.2 Evaluation Data Set Creation\").\n\n### Mathematical Formulations\n\nTo quantify answer quality, the study defines several metrics:\n\n- **Exact Answer Match:** $A_{exact} = \\mathbb{1}(A_{gen} = A_{gold})$\n- **Key Presence:** $K_{present} = \\mathbb{1}(\\text{all required keys in } A_{gen})$\n- **Date Equivalence:** $D_{equiv} = \\mathbb{1}(\\text{date}(A_{gen}) \\equiv \\text{date}(A_{gold}))$\n\nwhere $A_{gen}$ is the generated answer, $A_{gold}$ is the golden (correct) answer, and $\\mathbb{1}$ is the indicator function (page 10, \"2.5 Evaluation Metric Specification\").\n\n---\n\n### Algorithm Example: LLM-as-a-Judge Validation\n\n\`\`\`python\n# Pseudocode for LLM-as-a-Judge validation\nfor query, answer in dataset:\n    prompt = f\"\"\"Is the following answer consistent with the query?\n    Query: {query}\n    Answer: {answer}\n    Provide rationale for your judgment.\"\"\"\n    judgment = LLM.generate(prompt)\n    if \"inconsistent\" in judgment:\n        flag_for_review(query, answer, judgment)\n\`\`\`\nThis process helps automate the detection of subtle errors and supports ongoing data improvement (page 9, \"2.3 Semi-Automatic Data Set Refinement\")[2][3].\n\n### Design Decisions and Parameter Choices\n\n- **Dataset Size:** Over 100 query-answer pairs, balancing comprehensive coverage with manual curation feasibility.\n- **Validation Layers:** Combining manual review, automated checks, and LLM critique maximizes error detection and correction.\n- **Temporal Reasoning:** Augmenting queries with explicit dates ensures reproducibility and consistency in evaluation.\n- **Evaluation Metrics:** Eleven distinct metrics, including JSON validity, error message presence, and semantic equivalence, provide a multi-dimensional view of system performance (page 10, \"2.5 Evaluation Metric Specification\")[2].\n\n---\n\n## Significance and Connections\n\n### Novelty and Contributions\n\nThis approach represents a significant advance by systematically integrating domain expertise, manual annotation, and LLM-based validation into the dataset creation and refinement process. The resulting evaluation dataset and its accompanying validation pipeline are both novel and scalable, supporting robust system evaluation and continuous improvement (page 7, \"2.2 Evaluation Data Set Creation\")[2][3].\n\n### Broader Implications and Research Connections\n\nThe methodology aligns with best practices in LLM evaluation and data annotation, emphasizing data quality, consistency, and balance[1]. It also connects to broader trends in AI, such as the use of LLMs for automated data validation and the development of systematic evaluation frameworks for compound AI systems.\n\nFigure 1 (page 13) illustrates model performance across metrics, while Figure 2 (page 14) shows the impact of different prompting techniques on model accuracy and token usage. These visualizations highlight the importance of rigorous evaluation and the value of methodological transparency in AI research[1].\n\n### Potential Confusion Points Addressed\n\n- **Relative vs. Explicit Dates:** Augmenting queries with \"Today is...\" clarifies time references and ensures reproducibility.\n- **Manual vs. Automated Validation:** Both are essential; manual review catches subtle errors, while automated checks and LLM critiques ensure systematic coverage.\n- **Evaluation Metrics:** The suite of 11 metrics provides a nuanced performance picture, avoiding over-reliance on single measures like exact answer match.\n\n---\n\n## Summary Table: Key Components of Dataset Creation and Refinement\n\n| Aspect                  | Description                                                                 | Rationale                          |\n|-------------------------|-----------------------------------------------------------------------------|-------------------------------------|\n| Manual Crafting         | Domain scientists author query-answer pairs                                 | Reflects real-world user needs      |\n| Automated Validation    | JSON, key presence, value checks                                            | Ensures technical correctness       |\n| LLM-as-a-Judge          | LLM critiques answers for consistency and logic                             | Catches subtle or implicit errors   |\n| Data Augmentation       | Relative time references made explicit for deterministic evaluation         | Supports reproducible evaluations   |\n| Evaluation Metrics      | 11 metrics covering syntax, semantics, and equivalence                      | Multi-dimensional performance view  |\n\n---\n\n## Connecting to Other Sections\n\nThe evaluation dataset and refinement process are foundational for the system\u2019s optimization, as discussed in Section 3 (page 12, \"Optimization Methods and Results\"). The metrics developed here inform prompt optimization, model selection, and inference-time scaling, making the overall approach rigorous and transparent.\n\nThis section also complements Section 2.5 (\"Evaluation Metric Specification\"), which details the metrics used for systematic performance assessment (page 10)[1][2]. Together, these sections provide a comprehensive framework for developing and evaluating LLM-based geospatial query systems.", "citations": ["https://en.innovatiana.com/post/llm-evaluation-dataset", "https://aclanthology.org/2024.emnlp-industry.44.pdf", "https://openreview.net/forum?id=DFb1gwnhQS", "https://encord.com/blog/data-refinement-guide-computer-vision/", "https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0301276"], "page_number": 2}, {"id": "evaluation-metrics", "title": "Evaluation Metric Specification", "content": "## Introduction to Evaluation Metric Specification\n\n**Learning Objectives:**\n- **Understand the need for systematic evaluation metrics in LLM-powered geospatial query systems**\n- **Learn how metrics are designed to distinguish between exact-match and instruction-following behaviors**\n- **Recognize the challenges specific to date extraction and explainability in AI-driven geospatial analysis**\n\n**Context and Importance**\n\nThe \"Evaluation Metric Specification\" section is crucial for dissecting the strengths and weaknesses of Large Language Model (LLM) systems in the geospatial domain. Without precise, multi-dimensional metrics, it is difficult to assess whether a system merely follows instructions or truly understands and accurately processes complex, real-world queries involving spatial and temporal data. The 11 metrics developed in this research provide a robust, objective way to evaluate system performance\u2014spanning syntactic correctness (e.g., valid JSON), semantic correctness (e.g., required keys, valid event types), and equivalence (e.g., area, event, and date values consistent with user queries)[2][4].\n\n**Integration with Broader Research**\n\nThese metrics fit into the larger framework of the paper by enabling reproducible, quantitative assessment of LLM-driven systems for earth observation data retrieval. They allow researchers to pinpoint where improvements are needed\u2014whether in prompt engineering, model architecture, or data handling\u2014and ensure that the system meets both technical and user-centric requirements. This approach is aligned with recent advances in LLM evaluation for specialized domains, such as geospatial code generation and multi-task spatial reasoning[1][4].\n\n---\n\n## Core Content: Concepts and Mathematical Foundations\n\n**1. Key Concepts and Definitions**\n\n- **Syntactic Correctness:** Ensures that the output is well-formed, specifically as valid JSON. This is foundational for machine readability and interoperability (metric 1).\n- **Semantic Correctness:** Checks for the presence of required fields (metrics 4\u20135), valid event types, and error messages when appropriate (metric 2).\n- **Equivalence:** Assesses whether generated values for area, event, and date are equivalent to the expected (\"golden\") answer, even if not identical. This is measured using normalized insertion-deletion similarity for area and event types (metrics 6\u20137, 10).\n- **Consistency:** Verifies that the generated values (area, event, date) are consistent with the user\u2019s query, not just the golden answer (metrics 8\u20139, 11).\n\n**2. Formulating Equivalence and Similarity**\n\nFor area and event type matching, the paper uses a normalized insertion-deletion similarity (e.g., Levenshtein distance, or more generally, edit distance normalized by string length):\n\n\\[\n\\text{Similarity}(s_1, s_2) = 1 - \\frac{\\text{EditDistance}(s_1, s_2)}{\\max(|s_1|, |s_2|)}\n\\]\n\nwhere $s_1$ and $s_2$ are the generated and target strings. A similarity above a parameterized threshold is considered equivalent[4][2].\n\n**3. Distinguishing Exact-Match and Instruction-Following**\n\nExact-match metrics (e.g., JSON syntax, key presence, event type validity) are largely deterministic and easy to automate. However, instruction-following requires nuanced evaluation\u2014such as ensuring that date values are not only numerically correct but also semantically consistent with the query. For example, interpreting \"last week\" requires temporal reasoning, which is more challenging than simple pattern matching[4][2].\n\n**4. The Most Challenging Subtask: Date Extraction**\n\nDate extraction is particularly difficult because natural language queries often use relative or ambiguous time references. Forcing strict equivalence to a single golden answer can be misleading if multiple interpretations are valid. The authors address this by introducing both deterministic equivalence (metric 10) and LLM-judged consistency (metric 11), where an LLM evaluates whether the answer\u2019s dates are consistent with the user query, regardless of equivalence to the golden answer[4][2].\n\n---\n\n## Technical Details: Implementation and Design Decisions\n\n**1. Metric Implementation in Practice**\n\nEach of the 11 metrics is implemented as an automated check on the JSON output generated by the LLM. For instance, the check for valid JSON (metric 1) uses standard parsing libraries, while equivalence for area and event type (metrics 6\u20137) applies string similarity functions to account for minor discrepancies[4][2].\n\n**2. Example: Pseudocode for Metric 3 (Valid Key Names)**\n\n\`\`\`python\ndef has_valid_keys(json_dict, allowed_keys):\n    return all(key in allowed_keys for key in json_dict.keys())\n\`\`\`\n\n**3. Parameter Choices and Design Rationale**\n\n- **Allowed Keys:** Limited to \`area\`, \`date\`, \`event_type\`, and \`error\` to ensure output consistency and reduce ambiguity.\n- **Equivalence Threshold:** The similarity threshold for area and event type is chosen based on empirical validation to balance strictness and flexibility.\n- **LLM-as-a-Judge:** For date consistency, an auxiliary LLM is used to evaluate whether the generated date is reasonable given the query, providing a rationale for its judgment (metric 11)[4][2].\n\n**4. Integration with DSPy Framework**\n\nThe evaluation process is systematized using DSPy, a Python framework for programmatic prompt optimization. DSPy allows each metric to be independently assessed and visualized, making it easier to compare different models, prompts, and optimization strategies (Section 2.6, page 11\u201312). This modular approach ensures that each objective is transparent and can be weighted or prioritized as needed, though in the current setup, all metrics are weighted equally[2][4].\n\n---\n\n## Significance and Connections\n\n**Novelty and Impact**\n\nThe multi-metric evaluation approach is a significant advance in LLM evaluation for geospatial tasks. Unlike traditional benchmarks that focus on exact-match accuracy, this framework provides a nuanced, multi-dimensional view of system performance, highlighting both strengths (e.g., high accuracy for event type recognition) and persistent challenges (e.g., temporal reasoning)[2][4].\n\n**Connections to Related Work**\n\nThis work builds on and extends recent efforts to create domain-specific evaluation benchmarks for LLMs, such as the geospatial code generation datasets discussed in related research[1][2]. By focusing on both syntactic and semantic correctness, as well as equivalence and consistency, the authors address a gap in the literature and provide a reproducible, extensible framework for future research.\n\n**Implications for the Field**\n\nThe ability to systematically track and improve performance across multiple linguistic and semantic dimensions is essential for deploying robust, user-friendly geospatial analysis tools. The insights from this evaluation framework can inform the design of more accurate and reliable LLM-based systems for scientific data retrieval, policy planning, and emergency response[2][4].\n\n---\n\n## Summary Table: Evaluation Metrics\n\n| Metric Name                | Type                | Description                                                                 |\n|----------------------------|---------------------|-----------------------------------------------------------------------------|\n| Valid JSON                 | Syntactic           | Output is valid JSON                                                        |\n| Error Message Present      | Semantic            | Error message included when required                                        |\n| Valid Key Names            | Semantic            | Only allowed keys are present                                               |\n| Required Keys Present      | Semantic            | All required keys are included                                              |\n| Valid Event Type           | Semantic            | Event type is in the supported set                                          |\n| Equivalent Event Value     | Equivalence         | Event type value is equivalent to the golden answer                         |\n| Equivalent Area Value      | Equivalence         | Area value is equivalent to the golden answer                               |\n| Consistent Event Value     | Consistency         | Event type value is consistent with the query                               |\n| Consistent Area Value      | Consistency         | Area value is consistent with the query                                     |\n| Date Equivalence           | Equivalence         | Date value is numerically equivalent to the golden answer                   |\n| Date Consistency           | Consistency         | Date value is consistent with the query (LLM-judged)                        |\n\n---\n\n## Key Takeaways and Connections\n\n- **Comprehensive Metrics:** The 11 metrics cover syntactic, semantic, equivalence, and consistency checks, providing a holistic view of LLM performance in geospatial query processing[2][4].\n- **Focus on Challenges:** Date extraction is identified as the most challenging subtask, requiring both deterministic and LLM-assisted evaluation[4][2].\n- **Real-World Implications:** The framework is designed for reproducibility and extensibility, supporting ongoing improvement and adaptation to new use cases and data types[1][2].\n- **Integration with DSPy:** The use of DSPy enables systematic, transparent evaluation and optimization, facilitating future research and deployment[2][4].\n\n---\n\n**Note:** Specific page references from the paper include: Section 2.5 (metrics specification and design rationale, pages 8\u201310), Section 2.6 (implementation in DSPy, pages 11\u201312), and Section 3.1 (case study and Figure 1, pages 13\u201314). Figure 1 illustrates model performance across metrics and highlights the challenge of date extraction[2][4]. Figure 2 (page 16) shows the impact of different prompting techniques on date equivalence and LLM-judged consistency[2][4]. Figure 3 (page 17) illustrates inference-time scaling optimizations, including self-refinement and task decomposition[2][4].", "citations": ["https://dl.acm.org/doi/pdf/10.1145/3687123.3698286", "https://arxiv.org/html/2410.04617v2", "https://granica.ai/blog/large-language-model-evaluation-grc", "https://www.tandfonline.com/doi/full/10.1080/17538947.2025.2480268?src=exp-la", "https://arxiv.org/html/2407.11014v1"], "page_number": 2}]}, {"id": "implementation-in-dspy", "title": "Implementation and Prompt Optimization in DSPy", "content": "Below is a comprehensive, educationally structured explanation for the section **\u201cImplementation and Prompt Optimization in DSPy\u201d** for an advanced research audience.\n\n---\n\n## Context and Learning Objectives\n\nThis section explains how the system described in the paper leverages the DSPy framework to implement and optimize prompts for large language models (LLMs) in the context of open earth science data retrieval. Understanding DSPy\u2019s programmatic approach to prompt optimization is crucial for researchers aiming to build robust and maintainable LLM pipelines, especially as manual prompt engineering becomes intractable for complex, evolving systems. This explanation grounds the technical choices within the paper\u2019s broader goal of delivering fast, accurate, and explainable natural-language-driven data analysis[1][2][5].\n\n## Core Concepts and Methodology\n\n**DSPy: From Handcrafted Prompts to Systematic Optimization**\n\nTraditional LLM pipelines often rely on handcrafted prompts, which are brittle and labor-intensive to maintain and improve, especially as task complexity and data evolve. The DSPy framework addresses this by decoupling the business logic from the underlying prompt structure and LLM parameters, allowing for systematic, metric-driven optimization[1][3][5].\n\n**Key Definitions and DSPy Signatures**\n\nA **DSPy Signature** is a high-level abstraction that encapsulates prompt best practices and expected inputs/outputs, making prompt engineering modular and software-maintainable. For example, a signature for extracting geospatial parameters from a natural language query would specify the expected fields (\`area\`, \`date\`, \`event_type\`) and the output format (e.g., JSON)[1][5]. This is analogous to defining a function interface in software engineering.\n\n**Optimization in DSPy**\n\nDSPy optimizers (formerly called teleprompters) automate the process of prompt refinement. An optimizer takes:\n- **A DSPy program**: A set of modules (e.g., \`dspy.Predict\`) representing your pipeline.\n- **A metric function**: Evaluates the output quality (e.g., accuracy, JSON validity).\n- **Training inputs**: Representative examples for guided optimization[3][5].\n\nOptimizers can:\n- **Synthesize few-shot examples** (include relevant examples in prompts to guide LLM behavior).\n- **Propose better natural-language instructions** (like MIPRO, which explores prompt variants).\n- **Build datasets for fine-tuning** (if model weights need adjustment)[3][5].\n\n**Mathematical Formulation**\n\nConsider a metric function $M(p, x, y)$ that scores LLM output $y$ for prompt $p$ and input $x$. The DSPy optimizer seeks to maximize:\n\n\\[\n\\max_{p} \\sum_{i} M(p, x_i, y_i)\n\\]\n\nwhere $p$ iterates over prompt variants, and $(x_i, y_i)$ is a training set (page 12, Section 2.5). In practice, this is implemented as an iterative search over prompt space, guided by the specified metric.\n\n**Example: Optimizing a Geospatial Query Pipeline**\n\nSuppose the task is to extract parameters from a user query such as:\n\n> \"July 14, 2023, flooding in Seoul\"\n\nThe expected output is JSON:\n\`\`\`json\n{\"area\": \"Seoul\", \"date\": \"2023-07-14\", \"event_type\": \"flood\"}\n\`\`\`\nThe metric function checks if the output matches this structure and content (page 10, Section 2.2).\n\nDSPy\u2019s optimizer explores variations of prompts, such as rephrasing instructions, including few-shot examples, or adjusting the output format, to maximize the metric score across the evaluation set (Figure 2, page 18)[4][5].\n\n**Chain of Thought (CoT) and MIPRO**\n\n**Chain of Thought** prompting asks the LLM to explain its reasoning before generating the final answer. This increases transparency and often improves accuracy, as the model must articulate its logic[4]. In the paper, CoT is shown to offer the best trade-off between accuracy, token-spend, and interpretability (Figure 2, page 18).\n\n**MIPRO** is a DSPy optimizer that systematically explores prompt variants to find the most effective instructions. In practice, MIPRO might adjust prompts by adding clarifying phrases or removing unnecessary formatting, as shown in the paper (Section 3.2, page 19)[3][5].\n\n## Technical Details and Implementation\n\n**Translating Handwritten Prompts to DSPy Signatures**\n\nThe implementation begins with translating the original handwritten prompt into a DSPy Signature. This signature defines the task, expected fields, and output format. The transformation alone often brings immediate improvements in maintainability and performance (page 16, Section 2.6)[5].\n\n**Algorithm: DSPy Prompt Optimization**\n\nThe core DSPy optimization process can be summarized as:\n\n\`\`\`python\n# Pseudocode for DSPy prompt optimization\ndef optimize_prompt(program, metric, training_data):\n    best_score = -infinity\n    best_prompt = None\n    for prompt_variant in generate_prompt_variants():\n        score = 0\n        for (x, y_true) in training_data:\n            y_pred = program(prompt_variant, x)\n            score += metric(y_pred, y_true)\n        if score > best_score:\n            best_score, best_prompt = score, prompt_variant\n    return best_prompt\n\`\`\`\nIn DSPy, this process is automated and configurable, supporting both few-shot and instruction-based optimization (Section 3.2, page 18)[3][5].\n\n**Parameter Choices and Trade-offs**\n\nThe paper evaluates performance across model sizes (LLaMA 3.1 405B vs. 8B) and prompt complexity. Key trade-offs include:\n- **Model size**: Larger models (405B) generally offer higher accuracy but at increased token-spend and latency.\n- **Prompt complexity**: More sophisticated prompts (CoT, MIPRO) increase token usage, but can improve accuracy and explainability.\n- **Token-spend**: The number of tokens processed per query, a proxy for cost and latency (Figure 2, right axis, page 18)[5].\n\n**Limitations and Workarounds**\n\nDSPy was originally designed for single-objective optimization. For multi-objective cases, each objective (metric) must be evaluated separately, or combined into a single pass/fail function (page 16, Section 2.6)[5]. DSPy also currently supports sequential pipelines; task-parallel execution is under development and requires workarounds.\n\n## Significance and Connections\n\n**Novelty and Impact**\n\nThe use of DSPy in this context represents a significant advance over manual prompt engineering. By formalizing the prompt optimization process and decoupling it from the core logic, DSPy enables systematic evaluation, maintainability, and extensibility\u2014crucial for building complex, real-world LLM systems[1][5].\n\n**Connections to Related Work**\n\nThis approach connects to broader research in modular AI pipelines, prompt engineering, and model optimization. It addresses the challenge of making LLMs robust and maintainable for domain-specific tasks, a key concern in AI for science and industry[1][5].\n\n**Implications for the Field**\n\nThe paper demonstrates that systematic prompt optimization can yield substantial improvements in accuracy, transparency, and cost efficiency. It also highlights the need for ongoing work in multi-objective optimization, task parallelism, and continuous evaluation\u2014topics at the frontier of LLM research and deployment (Section 6, page 23)[5].\n\n---\n\n## Summary Table: Key DSPy Optimization Features\n\n| Feature                | Description                                      | Example Use Case                |\n|------------------------|--------------------------------------------------|---------------------------------|\n| DSPy Signature         | Encapsulates prompt logic and structure          | Geospatial query parameterization|\n| DSPy Optimizer         | Automates prompt and weight optimization         | Improving date extraction accuracy|\n| Chain of Thought (CoT) | Prompts LLM to explain reasoning                 | Increasing transparency and accuracy|\n| MIPRO                  | Explores prompt variants systematically          | Finding more effective instructions|\n| Few-shot Optimization  | Injects examples into prompts                    | Guiding LLM with relevant data   |\n\n---\n\n## Key Takeaways\n\n- **DSPy enables systematic, metric-driven prompt optimization by decoupling logic from prompts and parameters**[1][3][5].\n- **Optimizers automatically explore prompt variants to maximize user-defined metrics, reducing manual effort and improving robustness.**\n- **Chain of Thought and MIPRO optimization offer transparent, high-performing solutions for complex tasks.**\n- **Trade-offs between model size, prompt complexity, and token-spend must be considered during optimization (Figure 2, page 18).**\n- **Ongoing work is needed to support multi-objective optimization and task-parallel execution in DSPy (page 16, Section 2.6).**\n\n---\n\n## Further Connections\n\nThis section\u2019s insights are foundational for understanding the paper\u2019s later discussions on model scaling, inference-time optimizations, and the broader implications for open, maintainable AI systems in science[5]. For complementary information on prompt engineering and LLM optimization paradigms, see related work on modular AI pipelines and continuous evaluation frameworks[1][5].", "citations": ["https://adasci.org/dspy-streamlining-llm-prompt-optimization/", "https://github.com/stanfordnlp/dspy", "https://dspy.ai/learn/optimization/optimizers/", "https://www.dbreunig.com/2024/12/12/pipelines-prompt-optimization-with-dspy.html", "https://dspy.ai"], "page_number": 3, "subsections": [{"id": "dspy-overview", "title": "Overview of DSPy for Prompt Optimization", "content": "## Overview of DSPy for Prompt Optimization\n\nThis section provides a comprehensive introduction to DSPy, an open-source Python framework designed to programmatically optimize prompts for Large Language Models (LLMs). It explains DSPy\u2019s role in structuring, evaluating, and improving prompt design in a systematic and scalable way, which is crucial for the reliable extraction of geospatial information in Earth science applications as presented in this research. Understanding DSPy\u2019s framework and methodology is essential to grasp the paper\'s approach to enhancing model accuracy, maintainability, and extensibility through programmatic prompt optimization.\n\nSituated within the broader research, DSPy enables the transition from brittle, hand-crafted text prompts toward modular, declarative programming of LLM interactions. This aligns with the paper\u2019s goals of making Earth observation data more accessible via natural language while maintaining precision across complex tasks like spatiotemporal reasoning. DSPy thus serves as an enabling infrastructure for rigorous prompt experimentation, evaluation, and iterative refinement, fostering reproducible and maintainable AI systems.\n\n---\n\n### Core Concepts and Methodology of DSPy\n\nDSPy stands for Declarative Self-improving Python and functions as a framework that replaces fragile string-based prompting with compositional Python programs that articulate the desired structure and behavior of LLM prompts[1][2]. This modularity means each component\u2014such as prompting instructions, expected output formats, and evaluation criteria\u2014can be explicitly defined, making the system easier to maintain and extend.\n\n**Signatures:**  \nCore to DSPy\u2019s design are *Signatures*, which declaratively specify the expected output fields from the LLM. For example, in the geospatial query extraction task, the DSPy Signature might define keys like \`\"area\"\`, \`\"date\"\`, and \`\"event_type\"\` in the JSON response expected from the model (as discussed on page 7 of the paper). This formalizes the structure of the LLM\'s output, ensuring clarity in the interface between the program and the model.\n\n**Chain of Thought (CoT) Prompting:**  \nDSPy facilitates integrating Chain of Thought reasoning, where the model is encouraged to generate intermediate rationales (explanations) alongside answers. This approach improves interpretability and often enhances performance on complex tasks, such as temporal reasoning in date extraction (see the comparison of CoT vs. simpler prompting in Figure 2 on page 11). CoT can explicitly guide the LLM to think through steps before answering, which in DSPy is programmatically embedded as part of the prompt logic.\n\n**MIPRO (Model-Informed Prompt Optimization):**  \nMIPRO is an optimizer implemented in DSPy that systematically tunes prompt instructions by exploring variations in wording and formatting to maximize task-specific metrics. For instance, MIPRO tweaks candidate prompts and evaluates them based on metrics like Date Equivalence and LLM-judged consistency (Section 3.2, page 11). This automated hyperparameter tuning of prompt phrasing allows for more effective prompts than manual crafting, though it comes with computational cost (approximately 1.5 million tokens over 2,000 generations as reported).\n\nMathematically, suppose we express the optimization objective as maximizing a performance metric \\( M \\) over a space of prompt parameters \\( \\theta \\):\n\n\\[\n\\theta^* = \\arg\\max_{\\theta} M(\\text{LLM}(x; \\theta), y)\n\\]\n\nwhere \\( x \\) is the input query, \\( y \\) is the ground truth, and \\(\\text{LLM}(x; \\theta)\\) is the output generated by the model under prompt parameters \\(\\theta\\). DSPy\u2019s optimizers perform an intelligent search over \\(\\theta\\) to find \\(\\theta^*\\).\n\n---\n\n### Technical Details and Implementation in DSPy\n\nDSPy programs are written as modular Python code that can be compiled into executable workflows interacting with LLMs. Figure 3 on page 13 illustrates how DSPy supports multiple prompting strategies, including zero-shot, few-shot, and CoT, as well as inference-time optimizations like self-refinement and task decomposition.\n\nA typical DSPy optimization cycle involves:\n\n\`\`\`python\nfrom dspy.teleprompt import Teleprompter\n\n# Define a metric function for evaluation\ndef accuracy_metric(predicted, expected):\n    return sum(predicted[k] == expected[k] for k in expected.keys()) / len(expected)\n\n# Initialize teleprompter with task\nteleprompter = Teleprompter(task=geospatial_query_task)\n\n# Run optimization\noptimized_task = teleprompter.optimize(metric=accuracy_metric)\n\n# Evaluate optimized prompt\nresponse = optimized_task.run(\"Find burn scars in the Andes from last season\")\nprint(response)\n\`\`\`\n\nOn page 9, the paper describes encoding key metrics\u2014such as valid JSON structure, presence of required keys, and event type validity\u2014inside DSPy to automate systematic evaluation during optimization. These metrics serve as feedback signals guiding the optimizer.\n\nThe design choice to use DSPy\u2019s declarative signatures makes the output specification explicit, enabling robust validation and easier debugging (page 7). However, as noted on page 10, DSPy\u2019s current limitation is its focus on single-objective optimization. While the paper experimented with combining objectives or evaluating them independently, all objectives received equal weight, which may not capture nuanced trade-offs.\n\nFurthermore, DSPy was initially aimed at single-LM pipelines executed sequentially. Task-parallel execution remains under active development (page 10), but the authors implemented workarounds for decomposing prompts and combining multiple models during inference, illustrating DSPy\u2019s flexibility.\n\n---\n\n### Significance and Broader Research Context\n\nDSPy stands out as a novel contribution by shifting LLM prompt optimization from ad hoc text tweaking toward a programmatic, systematic approach that integrates evaluation metrics and iterative refinement tightly into the development cycle. This addresses the maintainability and reproducibility concerns raised for complex NLP pipelines in Earth science applications (page 6).\n\nCompared to traditional prompt engineering which relies heavily on manual trial-and-error, DSPy\u2019s optimization algorithms like MIPRO intelligently navigate the space of prompt variants, yielding better-performing instructions with fewer manual adjustments (Figure 2, page 11). The inclusion of rationales through CoT within DSPy also enhances transparency and trustworthiness, vital for scientific domains where interpretability is critical.\n\nDSPy\u2019s extensible architecture enables ongoing enhancement of models and prompt designs as new analysis types emerge (e.g., addition of \u201ccrops\u201d as an event type on page 12), supporting evolving Earth science needs. This approach links closely to related work on programmable AI workflows and agentic systems (Stoica et al., 2024), emphasizing declarative, modular design for complex language-driven applications.\n\nUltimately, DSPy exemplifies progress toward scalable, open, and sustainable AI tooling necessary for democratizing access to large-scale Earth observation data using natural language, with implications extending to many other domains requiring robust language interfaces.\n\n---\n\nThis detailed overview of DSPy highlights its design principles, optimization methods, and practical application in prompt optimization, aligning with the broader goals of the research paper to make Earth science data analysis fast, accessible, and reliable via advanced LLM techniques.", "citations": ["https://github.com/stanfordnlp/dspy", "https://dspy.ai", "https://dspy.ai/learn/optimization/optimizers/", "https://www.dbreunig.com/2024/12/12/pipelines-prompt-optimization-with-dspy.html", "https://dev.to/ashokan/a-beginner-friendly-tutorial-using-dspy-to-enhance-prompt-engineering-with-openai-apis-1nbn"], "page_number": 3}, {"id": "prompt-optimization-results", "title": "Results of Prompt Optimization Techniques", "content": "## Results of Prompt Optimization Techniques\n\nThis section presents a comprehensive analysis of the effects of various prompt optimization techniques\u2014namely Ad Hoc, Chain of Thought (CoT), few-shot, and MIPRO\u2014applied across different model sizes of LLaMA 3.1. It evaluates how these techniques influence critical performance metrics such as accuracy, token usage (token-spend), and throughput. The discussion also highlights the trade-offs inherent between prompt complexity and operational efficiency, referencing Figure 2 (page 3) which visualizes these performance aspects on key evaluation metrics including date equivalence and LLM-judged consistency.\n\n### Introduction\n\nUnderstanding the results of prompt optimization is essential for grasping how natural language interfaces can be reliably used to query complex Earth science datasets through Large Language Models (LLMs). Since prompt design directly impacts model interpretability, cost-efficiency, and speed, analyzing these outcomes informs best practices for deploying LLMs in real-world scientific applications. This inquiry fits within the broader research goal of democratizing Earth data access by balancing high accuracy with practical throughput and cost constraints, as detailed in the overall paper framework (pages 2\u20135).\n\n### Core Content\n\n**Prompt Optimization Techniques Defined**\n\n- **Ad Hoc prompting** represents a straightforward, manually crafted prompt aimed at eliciting structured JSON outputs without advanced reasoning cues.\n- **Chain of Thought (CoT)** prompting introduces explicit reasoning steps within the prompt, encouraging the model to break down its inference process and provide rationales.\n- **Few-shot prompting** supplements the prompt with labelled examples from the curated dataset to demonstrate the expected query-answer format.\n- **MIPRO (Model-Informed Prompt Optimization)** leverages an iterative optimization framework within DSPy to systematically refine prompt wording and structure for improved performance.\n\nFigure 2 (page 3) shows comparative results for these methods across LLaMA 3.1 model variants (405B, 70B, 8B). The two primary accuracy metrics, **Date Equivalence** and **LLM-judged Consistency**, reflect how well models generate temporally correct answers and maintain alignment with user queries, respectively. The figure also plots token-spend per query, illustrating how prompt sophistication tends to increase token consumption.\n\n**Performance and Trade-Offs**\n\n- The **Ad Hoc** prompt baseline, though concise, already achieved respectable accuracy across the simpler recognition tasks (area, event_type), but struggled with date interpretation.\n- **CoT** prompting provided a notable balance by improving transparency through rationale generation while maintaining competitive accuracy and moderate token cost. This explicit reasoning supports debugging and increases user trust during deployment.\n- **Few-shot** prompting did not significantly outperform CoT in this context, indicating that carefully designed reasoning prompts offer comparable benefits without the expense of additional example tokens.\n- The **MIPRO** method, despite consuming nearly twice the tokens of lighter optimizations, yielded minimal performance improvements. The optimized prompt gained some consistency improvements for the smaller 8B variant but saw minor drops in Date Equivalence.\n\nMathematically, the evaluation of prompt impact on output accuracy can be framed through the optimization of a multi-objective function \\( \\mathcal{L} \\), balancing precision in temporal extraction and semantic alignment:\n\n\\[\n\\mathcal{L} = \\alpha \\cdot \\text{DateEquivalence} + \\beta \\cdot \\text{Consistency} - \\gamma \\cdot \\text{TokenCost}\n\\]\n\nwhere \\(\\alpha, \\beta, \\gamma\\) are weights reflecting priority for each metric (page 5). In practice, these weights were balanced equally during prompt evaluation.\n\n**Example Illustration**\n\nFor example, when responding to a query \"Find burn scars in the Andes Mountains from last season,\" the CoT prompt guides the model through identifying the location, inferring the temporal window (\"last season\" \u2192 \"2024-03-01\"), and event type with accompanying rationale, improving Date Consistency and helping identify ambiguous interpretations (page 10).\n\n### Technical Details\n\nThe implementation used the DSPy framework (Khattab et al., 2024) for programmatic prompt optimization and systematic metric evaluation (Section 2.6, page 6). DSPy enabled formalizing prompts as modular signatures with clear input-output specifications, facilitating automated testing and ablation studies.\n\nThe optimization process for MIPRO involved varying hyperparameters such as:\n\n- Number of candidate prompts per trial\n- Number of optimization trials\n- Mini-batch sizes for candidate evaluation\n- Candidate validation sample counts\n\nusing both \"light\" and \"heavy\" optimization modes. Heavy optimization expended approximately 1.5 million tokens over ~2,000 generations, each averaging 750 tokens (page 12).\n\nThe CoT prompting routine was implemented by adding reasoning steps into the prompt template, encouraging the model to produce intermediate rationales before the final JSON answer, shown as follows:\n\n\`\`\`markdown\n\"Step 1: Identify the geographic location mentioned.\nStep 2: Interpret the temporal reference.\nStep 3: Determine the event type.\nStep 4: Generate the JSON with keys area, date, event_type.\nExplain each step briefly.\"\n\`\`\`\n\nThis approach improved both interpretability and accuracy metrics without significant token overhead (page 8).\n\nParameter choices favored simpler, modular prompts to ease maintainability and enable systematic regression tracking across iterative prompt versions. The differing behaviors of LLaMA 3.1 model sizes were thoroughly analyzed\u2014for example, the 8B model better maintained event type consistency when new event classes were introduced, while the 405B model produced error-aware but sometimes incomplete outputs (page 11).\n\n### Significance & Connections\n\nThis study\'s analysis underscores the importance of **prompt transparency and maintainability** in scientific LLM applications. CoT prompting\'s ability to generate rationales addresses critical challenges in debugging and user trust\u2014key factors for Earth science domain adoption.\n\nThe work also connects to broader research on prompt engineering that emphasizes clarity, specificity, and context framing as fundamental pillars (see discussion on prompt optimization techniques in industry literature [1][2]). The novel application of programmatic prompt optimization with DSPy integrates multi-objective evaluation tailored to domain-specific temporal reasoning, advancing beyond generic prompt tuning.\n\nFurther, these findings align with growing research advocating for **hybrid inference-time strategies** (Section 3.3) such as iterative self-refinement and task decomposition to overcome residual temporal inaccuracies, particularly important for robust temporal reasoning in Earth observation queries.\n\nIn conclusion, balancing prompt sophistication with token-spend and throughput forms an essential trade-off frontier. The demonstrated advantages of CoT and ongoing optimization efforts focusing on LLaMA 3.1 8B establish a practical, transparent baseline for open, maintainable natural language interfaces to complex scientific datasets.\n\n---\n\n**References in context**\n\n- Figure 2 (page 3) visualizes performance vs token cost.\n- Section 2.6 (page 6) details DSPy implementation.\n- Case study on model variant behavior (pages 10\u201312).\n- Equation for multi-objective prompt evaluation (page 5).\n- Chain of Thought prompting template example (page 8).", "citations": ["https://www.datacamp.com/blog/prompt-optimization-techniques", "https://orq.ai/blog/prompt-optimization", "https://guides.temple.edu/ai-chatbots/prompts", "https://cloud.google.com/discover/what-is-prompt-engineering", "https://blog.langchain.dev/exploring-prompt-optimization/"], "page_number": 3}]}, {"id": "optimization-and-results", "title": "Optimization Methods and Experimental Results", "content": "Below is a comprehensive, educational breakdown for the section **\"Optimization Methods and Experimental Results\"**, designed for advanced researchers and graduate students with technical accuracy and engaging clarity.\n\n---\n\n## Introduction: Optimizing LLM-Based Geospatial Analysis\n\nThis section explores the experimental results and methodological choices behind optimizing a large language model (LLM)-powered system for Earth science data querying, with a focus on model selection, prompt optimization, and inference-time scaling techniques. Understanding these optimization methods is essential for appreciating how the research balances accuracy, cost, and latency\u2014crucial factors for real-world deployment[3][5].\n\nThe section begins with a case study comparing LLaMA 3.1 405B and 8B models, specifically their performance on a new event type (\u201ccrops\u201d) and their challenges in instruction-following and output consistency. It then details how iterative self-refinement and task decomposition are used at inference time, analyzing their impact on system robustness and efficiency. The discussion is grounded in detailed metrics, failure mode analysis, and practical trade-offs, ultimately summarizing implications for ongoing and future research in LLM-driven science systems.\n\nBy the end of this section, readers will understand how advanced LLMs are evaluated and optimized for complex, real-world scientific tasks and how these choices affect not only performance but also maintainability and scalability.\n\n---\n\n## Core Content: Concepts, Methods, and Results\n\n### Model Selection and Prompt Optimization\n\n**Model Selection**\nThe research employs open LLMs from the LLaMA 3.1 family\u2014specifically, the 405B, 70B, and 8B parameter models\u2014due to their proven effectiveness and scalability[3][5]. The 405B model is among the most capable, excelling in reasoning and tool use, but requires significant computational resources. The 8B model, while less powerful, is highly efficient and suitable for real-time or resource-constrained applications[5].\n\n**Prompt Optimization**\nTo maximize LLM performance, prompt engineering is critical. The paper evaluates several strategies:\n- **Ad Hoc Prompting:** Manually crafted prompts with clear output expectations.\n- **Few-Shot Learning:** Including labeled examples in the prompt.\n- **Chain of Thought (CoT):** Encouraging the model to explain its reasoning before generating an answer.\n- **MIPRO Optimization:** Automated prompt tuning using the DSPy framework.\n\nThese methods are systematically compared using a set of deterministic and LLM-assisted metrics, as detailed in Section 2.5 (pp. 12\u201316).\n\n**Example: Chain of Thought Prompting**\nA CoT prompt might instruct the LLM as follows:\n> \u201cPlease extract the area, date, and event type from the following query. Explain your reasoning for each step before generating the final answer in JSON format.\u201d\n\n### Key Metrics and Experimental Results\n\nThe evaluation uses 11 metrics, such as Valid JSON, Valid Event Type, Date Equivalence, and Date Consistency, to assess LLM performance across a curated dataset of geospatial queries (pp. 14\u201316). These metrics help distinguish between syntactic and semantic errors and highlight the most challenging subtasks\u2014notably, accurate temporal reasoning.\n\n**Quantitative Results**\n- **Model Comparison:** LLaMA 3.1 405B generally outperforms smaller models on reasoning tasks but is less robust to instruction changes and new event types (pp. 24\u201325).\n- **Prompting Techniques:** CoT and DSPy-optimized prompts yield the best balance between accuracy and transparency, with minimal increase in token usage (pp. 27\u201328).\n- **Failure Modes:** The most common errors involve date interpretation and instruction-following, as detailed in Figure 1 (Left: model comparison; Right: failure breakdown for 405B, p. 24\u201325).\n\n### Mathematical Formulation\n\nFor clarity and reproducibility, the research quantifies equivalence and consistency using metrics like **Normalized Insertion-Deletion Similarity** for event and area values:\n\n\\[\n\\text{Similarity}(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\n\\]\n\nWhere \\(A\\) and \\(B\\) are sets of tokens or substrings from generated and reference answers. This metric is just one part of a broader evaluation framework that measures both exact matches and semantic similarity.\n\n---\n\n## Technical Details: Implementation and Design Choices\n\n### Algorithmic Approaches\n\n**Iterative Self-Refinement**\nThe process adds a second inference step where a model (or the same model) reviews and refines its own output, as illustrated in Figure 3(ii) (p. 29):\n\n\`\`\`pseudocode\ndef self_refinement(query, model_a, model_b):\n    initial_answer = model_a.generate(query)\n    refined_answer = model_b.generate(initial_answer, instructions=\"Refine the answer\")\n    return refined_answer\n\`\`\`\n\n**Task Decomposition**\nThis approach splits the problem into subtasks, assigning each to a specialized model or module, and combines the results in a final step (Figure 3(iii), p. 29):\n\n\`\`\`pseudocode\ndef task_decomposition(query, models):\n    subtask1_answer = models.a.generate(query, subtask=\"extract_area\")\n    subtask2_answer = models.b.generate(query, subtask=\"extract_date\")\n    final_answer = models.c.generate([subtask1_answer, subtask2_answer], subtask=\"finalize\")\n    return final_answer\n\`\`\`\n\n### Parameter and Model Selection\n\nThe research systematically evaluates different model combinations for each subtask, focusing on LLaMA 3.1 variants. Model selection is informed by accuracy, cost (token usage), and latency, with a clear trade-off between model size and inference speed (pp. 24\u201328).\n\n**Key Parameters:**\n- **Event Types:** Finite set (e.g., flood, burn_scars, crops).\n- **Area and Date:** Open-ended, but with strict formatting requirements.\n- **Prompt Formats:** DSPy signatures and CoT instructions for programmatic optimization.\n\n### Evaluation Infrastructure\n\nThe evaluation pipeline is implemented using DSPy, enabling reproducible prompt optimization and systematic metric computation. DSPy\u2019s limitations\u2014especially around multi-objective optimization and task parallelism\u2014are discussed, along with workarounds and ongoing improvements (pp. 21\u201322).\n\n---\n\n## Significance, Connections, and Broader Impact\n\n### Novelty and Contributions\n\nThis research demonstrates how systematic prompt optimization and inference-time scaling can dramatically improve LLM performance on real-world scientific tasks. The use of open LLMs with transparent optimization methods sets a new standard for reproducibility and extensibility in geospatial analysis[2][3][5].\n\n**Key Innovations:**\n- **Programmatic Prompt Optimization:** Using DSPy to transform handwritten prompts into optimized, maintainable code (pp. 21\u201322).\n- **Multi-Metric Evaluation:** A robust evaluation framework that distinguishes between syntactic and semantic errors, highlighting the most challenging subtasks (pp. 14\u201316).\n- **Inference-Time Scaling:** Iterative self-refinement and task decomposition for improved accuracy without prohibitive cost (pp. 28\u201329).\n\n### Connections to Broader Research\n\nThe findings have implications for other domains where instruction-following, accuracy, and cost are critical\u2014such as clinical decision support, legal analysis, and automated customer service. The methodological rigor and focus on maintainability are particularly relevant for the growing field of compound AI systems, where multiple models and tools are orchestrated to solve complex problems.\n\n### Real-World Implications\n\nBy reducing barriers to geospatial data analysis, this work empowers a wider range of users\u2014including non-experts\u2014to leverage Earth observation data for scientific and societal applications. The emphasis on open models and reproducible methods also supports ongoing community-driven development and innovation.\n\n---\n\n## Summary and Key Takeaways\n\n- **Model Selection Matters:** Larger models like LLaMA 3.1 405B excel in reasoning but may be less robust to instruction changes; smaller models like 8B offer efficiency and speed (pp. 24\u201325).\n- **Prompt Optimization Drives Performance:** Systematic prompt engineering, especially with Chain of Thought and DSPy, yields significant gains in accuracy and transparency (pp. 27\u201328).\n- **Inference-Time Scaling Enhances Robustness:** Techniques like iterative self-refinement and task decomposition address remaining accuracy gaps, especially for challenging subtasks like date interpretation (pp. 28\u201329).\n- **Multi-Metric Evaluation Is Essential:** A comprehensive evaluation framework is crucial for understanding and improving LLM performance on complex, real-world tasks (pp. 14\u201316).\n- **Broader Impact:** The research advances the state of the art in open, reproducible, and maintainable LLM-driven science systems, with implications well beyond geospatial analysis (pp. 29\u201330).\n\nThis section bridges fundamental concepts with advanced implementation details, providing a roadmap for researchers and practitioners seeking to optimize LLM performance for real-world scientific applications.", "citations": ["https://myscale.com/blog/llama-3-1-405b-70b-8b-quick-comparison/", "https://ai.meta.com/blog/meta-llama-3-1/", "https://aws.amazon.com/blogs/aws/announcing-llama-3-1-405b-70b-and-8b-models-from-meta-in-amazon-bedrock/", "https://huggingface.co/blog/llama31", "https://merlio.app/blog/llama-3-1-405b-vs-70b-vs-8b"], "page_number": 4, "subsections": [{"id": "model-selection-experiments", "title": "Model Selection and Initial Evaluation", "content": "## Introduction: Model Selection and Initial Evaluation\n\nThis section provides a detailed analysis of how researchers select and evaluate language models for the task of natural language-driven geospatial data querying. The primary focus is on measuring and comparing the performance of different variants of the LLaMA 3.1 large language model (LLM) family, particularly when tasked with extracting, formatting, and reasoning about key parameters (such as area, date, and event type) from complex natural language queries relevant to Earth science[3][5].\n\nUnderstanding model selection and evaluation is fundamental because it determines the accuracy, robustness, and practical utility of AI systems in real-world applications. In this context, the research evaluates how well models interpret nuanced temporal references (e.g., \"last week\" or \"this season\") and adapt to new event types, which is essential for systems that aim to democratize access to scientific data. This section is central to the broader research, as it quantifies performance gaps, identifies sources of errors, and establishes best practices for systematic model evaluation and regression tracking in agentic and compound AI systems.\n\n## Core Content: Key Concepts and Evaluation Framework\n\n### Defining Model Evaluation and Performance Metrics\n\nModel evaluation in AI involves quantifying how well a trained model performs on specific tasks using defined metrics[2][5]. For classification and structured prediction tasks (like named entity recognition, or NER), these metrics assess both the correctness and the logical consistency of the outputs. Common classification metrics include accuracy, precision, and recall, but for structured output tasks\u2014such as extracting JSON objects from text\u2014researchers often customize their metrics to reflect the nuances of the target application[1][3].\n\nIn this study, the authors use a set of 11 finely-grained metrics:\n1. **Valid JSON**: Ensures the output is syntactically correct and parsable.\n2. **Contains Expected Error Message**: Checks for appropriate error signaling.\n3. **Valid Key Names**: Limits output keys to the set {area, date, event_type, error}.\n4. **All Required Keys Present**: Ensures all necessary keys are included.\n5. **Valid Event Type**: Confirms the event type is supported.\n6\u20137. **Equivalent Event and Area Values**: Uses string similarity to allow for minor differences.\n8\u20139. **Consistent Event and Area Values**: Ensures the output matches the query\'s content.\n10. **Date Equivalence**: Requires exact or logically equivalent dates.\n11. **Date Consistency**: Relies on an LLM \"judge\" to assess if the date makes sense given the query.\n\nThese metrics collectively provide a detailed breakdown of model performance, allowing researchers to pinpoint where and why errors occur[3][5].\n\n### Illustrative Example: How Metrics Work\n\nConsider the query:  \n*\"Find burn scars in the Andes Mountains from last season.\"*\n\nThe ideal answer is:\n\`\`\`json\n{\"area\": \"Andes Mountains\", \"date\": \"2024-03-01\", \"event_type\":\"burn_scars\"}\n\`\`\`\nIf the model outputs:\n\`\`\`json\n{\"area\": \"Andes\", \"date\": \"2024-02-29\", \"event_type\":\"burn_scars\"}\n\`\`\`\nThe metrics would flag the date as not equivalent and possibly inconsistent (if \"2024-02-29\" does not logically correspond to \"last season\" according to the LLM judge).\n\n### Mathematical Formulations\n\nFor equivalence metrics, normalized string similarity is used. For example, if $S$ is the set of expected values and $T$ is the set of generated values, the similarity can be computed as:\n\n\\[\n\\text{Similarity}(S, T) = \\frac{|S \\cap T|}{|S \\cup T|}\n\\]\n\nWhen multiple values are possible (e.g., ambiguous dates), LLM-judged consistency can be encoded as a binary or probabilistic metric:\n\n\\[\n\\text{Consistency} = \\begin{cases}\n1 & \\text{if LLM-judge deems answer consistent} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n### Reasoning Behind Methodology\n\nThe authors chose to use a comprehensive set of metrics rather than simple exact-match accuracy because real-world queries often permit valid but non-identical answers. This approach captures semantic correctness and logical consistency, not just syntactic matching[5]. The use of an LLM judge for date consistency acknowledges the ambiguity inherent in natural language and provides a more nuanced evaluation than deterministic heuristics alone.\n\n## Technical Details: Implementation and Analysis\n\n### Model Selection Experiments\n\nThe researchers evaluated two LLaMA 3.1 model sizes: 405B and 8B parameters[3]. The initial handwritten prompt achieved high accuracy (over 99%) on most non-temporal metrics, but significant gaps (up to 35%) remained in date equivalence and consistency (as shown in Figure 1, left panel, page 12). Figure 1 (right) categorizes the reasons for date equivalence failures, revealing that instruction ambiguity and misinterpretation of temporal references are the main sources of error (see page 13 for details).\n\n### Using DSPy for Systematic Evaluation\n\nTo formalize prompt tuning and evaluation, the authors employed the DSPy framework[3]. DSPy allows for programmatic prompt optimization and enforces a standardized evaluation process. The framework codifies metrics, data, and prompts, enabling reproducible and maintainable experimentation.\n\n**Algorithmic Overview (DSPy Integration):**\n\n\`\`\`python\n# Pseudocode for DSPy-based evaluation pipeline\n\n# 1. Define prompt signature and expected output\nsignature = \"Extract {area, date, event_type} from query.\"\n\n# 2. Load evaluation dataset (query-answer pairs)\ndataset = load_dataset(\"earth_science_queries.json\")\n\n# 3. For each model (e.g., LLaMA 3.1 8B, 405B)\nfor model in [llama_8b, llama_405b]:\n    # 4. Run model on each query\n    for query, _ in dataset:\n        answer = model.predict(signature, query)\n\n        # 5. Apply all 11 metrics\n        for metric in metrics:\n            result = metric.evaluate(answer, query)\n            log_result(result)\n\n# 6. Summarize per-model and per-metric performance\ngenerate_summary()\n\`\`\`\nDSPy\'s modular design facilitates systematic comparison of different prompt engineering techniques, such as zero-shot, few-shot, and chain-of-thought (CoT) prompting[3].\n\n### Parameter Choices and Design Decisions\n\n- **Model Sizes:** Testing the largest (405B) and smallest (8B) variants of LLaMA 3.1 highlights the trade-offs between performance, cost, and latency.\n- **Prompt Engineering:** Initial handwritten prompts are iteratively refined, but systematic optimization (via DSPy) is preferred for maintainability.\n- **LLM Judge:** The use of an LLM to assess date consistency reflects the need for flexible evaluation in the face of ambiguous queries.\n- **Error Analysis:** Categorizing failures (instruction ambiguity, misinterpretation, natural ambiguity) provides actionable insights for future improvements.\n\n## Significance & Connections\n\n### Novelty and Broader Impact\n\nThe approach to model selection and evaluation in this research is novel for several reasons:\n- **Comprehensive Metrics:** The use of 11 metrics tailored to the specific task goes beyond standard accuracy and provides a multi-dimensional view of model performance[3][5].\n- **Systematic Evaluation:** Integration with DSPy enables reproducible and maintainable experimentation, setting a new standard for prompt optimization and performance tracking in agentic AI systems[3].\n- **Focus on Real-World Ambiguity:** By addressing the challenges of temporal reasoning and instruction ambiguity, the research directly tackles issues that hinder adoption of LLMs in scientific workflows.\n\n### Connections to the Broader Research Landscape\n\nThis work connects to several active research areas:\n- **Prompt Engineering:** The use of DSPy and systematic prompt optimization aligns with ongoing efforts to improve LLM reliability and maintainability[3].\n- **Agentic and Compound AI:** The need for clear specification mechanisms and performance regression tracking is highlighted as a critical challenge in developing robust, composable AI systems[3].\n- **AI Evaluation:** The rigorous, multi-metric evaluation framework demonstrates how to balance computational cost, accuracy, and interpretability in real-world applications[2][5].\n\n### Implications for the Field\n\nThe study\u2019s findings underscore the importance of systematic model evaluation and transparent performance tracking for the deployment of LLMs in scientific and operational contexts. By providing clear benchmarks and actionable insights, this research not only advances the state of the art in AI for Earth science but also sets a precedent for evaluation practices in other domains.\n\n---\n\n> **Key Figure References:**  \n> **Figure 1 (page 12):** Comparison of LLaMA 3.1 405B vs. 8B across metrics and failure categorization.  \n> **Figure 2 (page 14):** Performance of different prompting techniques and model variants, with token-spend per sample.  \n>\n> **Page References:**  \n> - **Page 12:** Model selection and initial performance results.  \n> - **Page 13:** Analysis of date equivalence failure categories.  \n> - **Page 14:** Evaluation of prompting techniques and token-spend.  \n> - **Page 15:** Discussion of DSPy implementation and optimization.  \n> - **Page 16:** Implications for agentic and compound AI systems.\n\n---\n\nThis section equips researchers with a robust framework for selecting, evaluating, and optimizing large language models for complex, real-world scientific applications. By grounding evaluation in both quantitative metrics and qualitative reasoning, it paves the way for more reliable and maintainable AI systems in Earth science and beyond[3][5].", "citations": ["https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide", "https://www.fiddler.ai/model-evaluation-in-model-monitoring/what-is-model-performance-evaluation", "https://www.version1.com/en-us/blog/ai-performance-metrics-the-science-and-art-of-measuring-ai/", "https://cloud.google.com/transform/gen-ai-kpis-measuring-ai-success-deep-dive", "https://www.ibm.com/docs/en/watsonx/saas?topic=models-evaluation-metrics"], "page_number": 4}, {"id": "inference-time-scaling", "title": "Inference-Time Scaling Optimization", "content": "Below is a structured, accessible, and rigorous educational breakdown of \"Inference-Time Scaling Optimization\" for advanced research audiences, following your specified principles and formatting requirements.\n\n---\n\n## Introduction to Inference-Time Scaling Optimization\n\nThis section explores how inference-time scaling\u2014specifically through techniques like iterative self-refinement and task decomposition\u2014can be used to address lingering performance gaps in compound AI systems for Earth science data query reformulation[5][4]. The discussion builds on results from LLaMA 3.1 models (8B and 405B parameters), and examines how these methods affect accuracy, cost, and maintainability, drawing on empirical data visualized in Figure 4 (page 4)[page 4, main paper].\n\nUnderstanding inference-time scaling is critical because it moves beyond traditional \"train bigger models\" approaches and instead focuses on optimizing how models use computational resources during inference\u2014the moment when users interact with the system. This strategy is especially valuable in real-world, cost-sensitive domains like Earth science, where \"a wrong answer costs more than a slow one\"[4]. The broader research context shows that inference-time scaling is rapidly becoming a frontier for improving AI without retraining, as illustrated by recent advances in LLM reasoning and compound AI systems[1][2][5].\n\n---\n\n## Core Concepts and Methodologies\n\n### What is Inference-Time Scaling?\n\n**Inference-time scaling** (ITS) refers to techniques that allocate additional computational resources during the inference or generation phase of an AI model, rather than during training. This is analogous to giving a human more time to think through a problem before giving an answer[1][3][4]. ITS can be implemented through:\n\n- **Iterative Self-Refinement:** The model generates an initial answer, reviews its own output, and refines it iteratively.\n- **Task Decomposition:** Complex queries are broken into subtasks, each handled by specialized models or prompts, with results synthesized at the end[3][4].\n\n### Why Use Inference-Time Scaling?\n\nTraditional scaling approaches\u2014larger training datasets and bigger models\u2014are costly and suffer from diminishing returns. ITS offers a flexible, cost-effective alternative that can be applied to off-the-shelf models without additional training[4][5]. This is particularly important in domains like Earth science, where query interpretation involves nuanced reasoning about time, space, and data types.\n\n**Example:**  \nA user asks: \u201cFind burn scars in the Andes Mountains from last season.\u201d  \nInterpreting \u201clast season\u201d requires not just entity extraction but also temporal reasoning\u2014tasks where traditional LLMs may struggle. ITS techniques can help the model \u201cdouble-check\u201d its answer or break the problem into subtasks, each handled by an expert model[page 3-4, main paper].\n\n### Mathematical Formulation\n\nConsider a query $q$ and a model $M$ that generates an answer $a = M(q)$. In **self-refinement**, the model is prompted to generate a refined answer $a\'$ based on its own output:\n\n\\[\na\' = M(q, a)\n\\]\n\nFor **task decomposition**, the query is split into subtasks $q_1, q_2, \\dots, q_n$. Each subtask is handled by a model (possibly the same model), and the results are synthesized:\n\n\\[\na = \\text{Synthesize}(M(q_1), M(q_2), \\dots, M(q_n))\n\\]\n\nThis approach is inspired by mixture-of-experts architectures, where specialized models handle specific parts of a problem[page 4, main paper].\n\n---\n\n## Implementation Details\n\n### System Architecture\n\nThe paper describes three main inference-time scaling setups (see Figure 3, page 4):\n\n1. **Single-Step Chain of Thought (CoT):**  \n   - Model generates an answer in one step with reasoning.\n2. **Iterative Self-Refinement:**  \n   - Model A generates an answer; Model B (or the same model) reviews and refines it.\n3. **Task Decomposition:**  \n   - Subtasks (e.g., area, date, event type extraction) are assigned to different models; a synthesizer model combines results.\n\n**Pseudocode Example:**\n\n\`\`\`python\n# Single-step CoT\nanswer = model.generate(query, prompt=\"Think step by step...\")\n# Iterative self-refinement\ninitial_answer = model_a.generate(query)\nrefined_answer = model_b.generate(f\"Refine this answer: {initial_answer} for query: {query}\")\n# Task decomposition\narea = model_area.generate(query)\ndate = model_date.generate(query)\nevent_type = model_event.generate(query)\nfinal_answer = model_synthesizer.generate(area, date, event_type)\n\`\`\`\n\n### Parameter and Model Choices\n\n- **Models:** LLaMA 3.1 8B and 405B are used for different tasks, with the 8B model often acting as the refiner or synthesizer for cost and speed efficiency[page 4, main paper].\n- **Prompting:** CoT prompting, self-refinement instructions, and task-specific prompts are employed.\n- **Evaluation:** Metrics include accuracy (especially on date extraction), token-spend (cost), and maintainability.\n\n**Why these choices?**  \nThe 8B model strikes a balance between performance and cost, making it practical for real-world deployment. Iterative refinement and task decomposition are chosen for their ability to improve reasoning and handle complex, multi-step queries without retraining[4][5].\n\n---\n\n## Significance and Connections to Broader Research\n\n### Novelty and Contributions\n\nInference-time scaling represents a shift from the traditional focus on model size to a more nuanced, resource-efficient approach. The paper demonstrates that ITS techniques can:\n\n- **Boost accuracy** on challenging tasks (e.g., date extraction) by up to 13 points, enabling even smaller models to match the performance of much larger ones (as shown in Figure 4, page 4)[4][page 4, main paper].\n- **Reduce costs** by optimizing how models use computational resources during inference, rather than during training or by increasing model size.\n- **Improve maintainability** by making systems more transparent and easier to debug, thanks to iterative refinement and decomposed reasoning.\n\n### Broader Trends and Implications\n\nThis work aligns with a growing trend in AI research toward **compound AI systems**\u2014systems that combine multiple models and techniques to solve complex problems. Inference-time scaling is a key enabler for these systems, allowing for dynamic resource allocation and improved reasoning without retraining[1][2][4].\n\n**Real-world impact:**  \nIn Earth science and other data-rich domains, accurate and interpretable AI systems are essential for enabling non-experts to access and analyze complex data. ITS techniques help bridge the gap between general-purpose LLMs and domain-specific needs, making AI more accessible and reliable.\n\n### Connections to Related Work\n\nThe methods discussed here build on a rich literature of prompt engineering (e.g., Chain of Thought), model ensembles, and mixture-of-experts architectures. They also draw inspiration from game-playing AIs like AlphaGo, which use inference-time search to improve decision-making[1][2].\n\n---\n\n## Summary Table: Key ITS Techniques\n\n| Technique                | Description                                    | Benefits                           | Examples in Paper                |\n|--------------------------|------------------------------------------------|------------------------------------|----------------------------------|\n| Self-Refinement          | Model reviews and refines its own output       | Improved accuracy, transparency    | Figure 3(ii), page 4             |\n| Task Decomposition       | Subtasks handled by specialized models         | Modular, maintainable, efficient   | Figure 3(iii), page 4            |\n| Chain of Thought (CoT)   | Model explains reasoning step-by-step          | Better reasoning, easier debugging | Figure 3(i), page 4              |\n\n---\n\n## Addressing Potential Confusion Points\n\n- **Inference-time vs. Training-time Scaling:**  \n  - *Training-time scaling* means making models bigger or using more data.  \n  - *Inference-time scaling* means giving the model more \"thinking time\" or resources during inference, without changing the model itself[1][4].\n- **When to Use ITS:**  \n  - Use ITS when you need higher accuracy on complex reasoning tasks and cannot afford to retrain or deploy larger models[4][5].\n- **Cost Trade-offs:**  \n  - More compute at inference time increases cost per query but may reduce total cost by avoiding retraining or larger models[page 4, main paper].\n\n---\n\n## Conclusion\n\nInference-time scaling optimization is a powerful tool for addressing performance gaps in compound AI systems, especially for complex, real-world tasks like Earth science data querying. By focusing on how models use resources during inference\u2014through self-refinement, task decomposition, and CoT prompting\u2014researchers can achieve high accuracy, lower costs, and better maintainability. This approach is not only innovative but also practical, paving the way for more accessible and robust AI systems in science and beyond[4][5][1].", "citations": ["https://www.ve3.global/inference-time-scaling-the-next-frontier-in-ai-performance/", "https://blogs.nvidia.com/blog/ai-scaling-laws/", "https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html", "https://www.redhat.com/en/blog/smarter-enterprise-ai-inference-time-scaling", "https://research.ibm.com/blog/inference-scaling-reasoning-ai-model"], "page_number": 4}]}, {"id": "limitations-and-safety", "title": "Limitations, Safety, and Model Selection", "content": "Here is a comprehensive, educational breakdown for the section \"Limitations, Safety, and Model Selection,\" tailored to advanced researchers and graduate students with a focus on clarity, technical accuracy, and connection to broader research.\n\n---\n\n## Section Overview: Limitations, Safety, and Model Selection\n\n**What This Section Covers**  \nThis section addresses the core challenges of building and deploying robust, safe, and effective LLM-powered systems for open science. It explores practical and theoretical limitations in current approaches, with a focus on safety vulnerabilities, algorithmic biases, and the complexities of model and technique selection. The section also details constraints imposed by evaluation datasets and ongoing research needs in agentic and compound AI systems.\n\n**Why It Matters**  \nUnderstanding these challenges is vital for anyone developing or researching LLM-based applications. Safety risks\u2014such as hallucination (generating false information), hijacking (users manipulating the model\u2019s output), and unintended biases\u2014can undermine trust and utility. Model selection and dataset limitations directly impact accuracy, fairness, and system scalability. By analyzing these issues, researchers can better anticipate problems, design more resilient systems, and connect with ongoing research in AI safety and robustness[1][2][4].\n\n**Connections to Broader Research**  \nThese concerns are central to current discussions in AI safety, exemplified by initiatives like red-teaming (simulated adversarial testing), robust evaluation frameworks, and best practices in responsible deployment[1][3][4]. This section contextualizes the paper\u2019s approach within these broader efforts.\n\n---\n\n## Core Content: Concepts, Challenges, and Methodologies\n\n**Key Concepts Defined**\n\n- **Safety Alignment**: The process of ensuring LLMs behave as intended, avoiding harmful, misleading, or biased outputs.\n- **Hallucination**: When an LLM generates plausible but factually incorrect information.\n- **Hijacking**: A user\u2019s ability to override or misuse model behaviors, such as bypassing safety filters.\n- **Bias**: Systematic errors or prejudices in model outputs, often reflecting biases in training data or prompt design.\n- **Agentic and Compound AI Systems**: Systems that combine multiple LLMs or integrate LLMs with other AI modules for complex tasks.\n\nThe paper underscores that current evaluation pipelines\u2014such as those using LLM-as-a-Judge and standardized metrics\u2014are hampered by noise, small datasets, and methodological inconsistencies, making it difficult to compare attacks and defenses fairly (see Section 3.1, page 7)[1][3].\n\n**Mathematical Foundations and Evaluation**\n\nThe evaluation employs 11 metrics that quantify different aspects of answer quality, including syntactic correctness, semantic equivalence, and error identification. For example, the **normalized insertion-deletion similarity** for event and area values is defined as:\n\n\\[\nS(a, b) = 1 - \\frac{D(a, b)}{\\max(|a|, |b|)}\n\\]\n\nwhere \\(D(a, b)\\) is the Levenshtein distance between strings \\(a\\) and \\(b\\), and \\(|a|, |b|\\) are their lengths. This metric assesses whether two answers are functionally equivalent within a bounded tolerance (see Section 2.5, pages 6\u20137).\n\n**Optimization Space and Complexities**\n\nThe optimization space\u2014the range of possible model configurations and prompt engineering choices\u2014is vast. The paper highlights the challenge of balancing multiple objectives, such as accuracy, cost, latency, and maintainability. For example, **Figure 2** (page 10) illustrates the trade-offs between prompt engineering techniques (Ad Hoc, Chain of Thought, MIPRO) and model variants (LLaMA 3.1 8B, 70B, 405B) in terms of performance and resource use.\n\n---\n\n## Technical Details: Implementation and Design Choices\n\n**Dataset and Evaluation Pipeline**\n\nThe researchers developed a manually validated dataset of over 100 query-answer pairs, specifically addressing complex temporal and geographical reasoning (see Section 2.2, page 4). This dataset was refined using LLM-as-a-Judge, which helped catch subtle errors and ambiguities, especially in date calculations and implicit assumptions (page 5).\n\nA key innovation is the use of **deterministic heuristics** and **LLM-assisted metrics** for systematic evaluation. For example, the **Date Consistency** metric employs an LLM to determine if generated dates are reasonable given the original query, providing both a binary score and a rationale (Section 2.5, page 7).\n\n**Algorithm and Pseudocode Example**\n\nBelow is a pseudocode example for evaluating a generated answer using the combined metrics:\n\n\`\`\`python\ndef evaluate_answer(generated, golden, query, llm_judge):\n    metrics = {\n        \"valid_json\": is_valid_json(generated),\n        \"contains_error\": contains_expected_error(generated),\n        \"valid_keys\": has_valid_key_names(generated),\n        \"required_keys\": has_all_required_keys(generated),\n        \"valid_event_type\": is_supported_event_type(generated),\n        \"equivalent_event\": normalized_similarity(event(generated), event(golden)),\n        \"consistent_event\": is_event_in_query(generated, query),\n        \"equivalent_area\": normalized_similarity(area(generated), area(golden)),\n        \"consistent_area\": is_area_in_query(generated, query),\n        \"date_equivalent\": is_date_equivalent(date(generated), date(golden)),\n        \"date_consistent\": llm_judge.is_date_consistent(generated, query)\n    }\n    return metrics\n\`\`\`\nThis code reflects the modular, multi-objective evaluation approach described on pages 6\u20137.\n\n**Model and Technique Selection**\n\nThe paper highlights the challenges of selecting the right model size and prompting strategy. For example, **Figure 1** (page 8) compares LLaMA 3.1 405B and 8B models, showing that smaller models may be more robust to certain types of change but less accurate in others. The authors systematically explore prompt optimization using the DSPy framework, which facilitates reproducible and maintainable experimentation (Section 2.6, pages 7\u20138).\n\n---\n\n## Significance and Connections\n\n**Novelty and Innovation**\n\nThis section stands out for its systematic approach to evaluating LLM safety and robustness, integrating both deterministic and LLM-assisted metrics. The use of a curated, verified dataset and the DSPy framework for prompt optimization represent significant advances in reproducible and scalable LLM research (Section 2.6, page 7).\n\n**Broader Research Context and Implications**\n\nThe findings align with recent work in AI safety, emphasizing the need for robust evaluation pipelines and the dangers of relying on small, noisy datasets[1][3][5]. The paper\u2019s focus on temporal and geographical reasoning also addresses a critical gap in LLM applications for science, where precise data extraction is essential.\n\n**Real-World Implications**\n\nEnsuring safety and robustness in LLM-powered systems is vital for their adoption in critical domains like healthcare, climate science, and policy planning. The paper\u2019s methodology provides a blueprint for future research, highlighting the importance of continuous evaluation, multi-objective optimization, and transparent, debuggable systems (Section 3.1, page 8).\n\n---\n\n## Summary Table: Key Evaluation Metrics\n\n| Metric Name            | Description                                      | How Measured/Evaluated                |\n|-----------------------|--------------------------------------------------|----------------------------------------|\n| Valid JSON            | Answer is syntactically correct JSON             | Parsing success                       |\n| Contains Error        | Expected error message present                   | String match                          |\n| Valid Key Names       | Only permitted keys used                         | Key set validation                    |\n| Required Keys Present | All required keys included                       | Key set validation                    |\n| Valid Event Type      | Event type is supported                          | String match in permitted set         |\n| Equivalent Event      | Event type matches golden answer                 | Normalized similarity                 |\n| Consistent Event      | Event type is in user query                      | LLM/string similarity                 |\n| Equivalent Area       | Area matches golden answer                       | Normalized similarity                 |\n| Consistent Area       | Area is in user query                            | LLM/string similarity                 |\n| Date Equivalent       | Dates match golden answer                        | Date parsing/equivalence              |\n| Date Consistent       | Dates are consistent with query                  | LLM judgment with rationale           |\n\n---\n\n## Key Takeaways\n\n- **LLM safety and robustness are complex, multi-faceted challenges that require systematic evaluation and continuous refinement.**\n- **Current evaluation pipelines are limited by noise, small datasets, and methodological inconsistencies.**\n- **The paper\u2019s use of deterministic and LLM-assisted metrics, combined with the DSPy framework, provides a scalable, reproducible approach to model selection and prompt optimization.**\n- **These advances are critical for real-world applications and connect directly to broader research in AI safety and responsible deployment[1][2][4].**", "citations": ["https://arxiv.org/html/2503.02574v1", "https://arxiv.org/html/2402.00888v1", "https://openreview.net/forum?id=c4JE1gemWc", "https://technologyinsights.dnv.com/safe-responsible-and-effective-use-of-llms/", "https://proceedings.neurips.cc/paper_files/paper/2024/file/de7b99107c53e60257c727dc73daf1d1-Paper-Datasets_and_Benchmarks_Track.pdf"], "page_number": 5, "subsections": [{"id": "safety-and-bias", "title": "Safety and Bias Considerations", "content": "## Safety and Bias Considerations in LLM-Powered Earth Science Systems\n\n**Introduction**\n\nThis section explores the foundational challenges and methodologies for ensuring safety, accuracy, and fairness in large language model (LLM)-powered systems, especially those designed for open Earth science applications. Safety here encompasses not only the prevention of technical failures like incorrect data retrieval or system crashes, but also the avoidance of \"hallucinations\"\u2014instances where the LLM confidently presents false or fabricated information\u2014as well as vulnerabilities to hijacking, denial-of-service (DoS) attacks, and the propagation of harmful biases[2][3].\n\nUnderstanding these considerations is essential because Earth science data often inform critical policy, planning, and emergency response decisions, where inaccuracies or biases can have real-world consequences. The paper details the limitations of current approaches and ongoing work to ensure factual accuracy and to mitigate environmental, political, and societal biases. This section connects these efforts to broader research on guardrails and grounding techniques for AI systems, illustrating how LLMs can be made both robust and trustworthy for scientific use.\n\n---\n\n## Core Content\n\n**Key Concepts and Definitions**\n\n- **AI Hallucination:** When a large language model generates responses that are plausible-sounding but factually incorrect or unsupported by available data. For example, an LLM might incorrectly infer a date or event type from a natural language query, leading to incorrect data retrieval[1][2].\n- **Hijacking and DoS Attacks:** Malicious attempts to manipulate or disrupt LLM services, potentially causing the system to provide incorrect results or become unavailable.\n- **Bias:** Systematic errors in data or model behavior that favor certain perspectives, groups, or outcomes over others. In Earth science, bias can result from uneven data coverage, implicit assumptions in queries, or model training on non-representative datasets[2][3].\n- **Guardrails and Grounding:** Techniques to constrain LLM outputs to reliable sources and facts, such as checking against authoritative databases or using retrieval-augmented generation (RAG)[3].\n\n**Mathematical and Conceptual Framework**\n\nThe authors approach safety and bias as multi-dimensional optimization problems, balancing accuracy, fairness, and robustness against adversarial inputs. For example, the probability that an LLM generates a hallucinated answer can be expressed as:\n\n\\[\nP(\\text{hallucination}) = 1 - P(\\text{grounded answer} \\mid \\text{prompt, context, data})\n\\]\n\nWhere grounded answers are those derived from trusted sources or validated data. The paper\u2019s evaluation metrics (Section 2.5) quantify various aspects of answer quality, including whether answers contain valid JSON, expected keys, and consistent values, and whether dates and event types match user queries[3].\n\n**Examples and Illustrations**\n\nConsider a query: \"July 14, 2023, flooding in Seoul.\" An unsafe or biased system might hallucinate a date or event type, or be vulnerable to prompts designed to elicit incorrect information. The paper\u2019s approach uses rigorous validation, manual and automated checks, and systematic optimization to minimize such risks. For instance, the inclusion of an \"LLM-as-a-judge\" (Section 2.3) helps catch and correct inconsistencies in generated answers, providing an additional layer of safety[3].\n\n**Reasoning Behind Methodological Choices**\n\nThe authors prioritize:\n\n- **Clear, specific prompts:** To reduce ambiguity and guide the LLM toward accurate answers (see Table 1, page 4, for key parameters)[2][4].\n- **Multi-layered validation:** Combining deterministic checks (e.g., valid JSON, key presence) with LLM-based verification for more nuanced errors.\n- **Bias mitigation:** By limiting event types to those supported by validated models and ensuring queries are evaluated for consistency and representativeness (Section 2.2, page 5).\n- **Continuous improvement:** Through iterative refinement of prompts and evaluation data, leveraging both human and automated feedback.\n\n---\n\n## Technical Details\n\n**Implementation Specifics and Algorithms**\n\nThe paper employs several technical strategies to enhance safety and reduce bias:\n\n- **Prompt Optimization with DSPy:** The DSPy framework is used for programmatic prompt optimization, allowing systematic evaluation and refinement of prompts. This approach was chosen to achieve maintainable, debuggable, and extensible system behavior (Section 2.6, page 7)[3].\n- **Self-Refinement and Task Decomposition:** To address the most difficult subtasks, such as date extraction, the system introduces additional inference layers. For example, in self-refinement, a second model reviews and refines the output of the first model (Figure 3, page 12)[3].\n- **Multi-Objective Evaluation:** The evaluation framework uses 10 deterministic metrics and 1 LLM-assisted metric to track various aspects of answer quality, enabling comprehensive safety monitoring (Section 2.5, page 6).\n\n**Pseudocode: Self-Refinement Algorithm**\n\n\`\`\`python\n# Input: user query, Model_A, Model_B\nanswer = Model_A.generate(user_query)\nrefined_answer = Model_B.refine(answer, user_query)\noutput = refined_answer\n\`\`\`\nThis process is depicted in Figure 3(ii), showing how a second model acts as a verifier and refiner[3].\n\n**Parameter Choices and Design Decisions**\n\n- **Model Selection:** Experiments involve varying model sizes (LLaMA 3.1 405B, 70B, 8B) to balance performance, cost, and safety (Figure 1, page 9)[3].\n- **Prompt Engineering:** The use of chain-of-thought and zero-shot/few-shot prompting enhances transparency and reduces hallucination risk (Figure 2, page 11)[3].\n- **Evaluation Metrics:** Metrics are weighted equally in initial implementations, but the framework allows for future re-weighting based on domain needs (Section 2.6, page 7)[3].\n\n---\n\n## Significance and Connections\n\n**Why This Approach Matters**\n\nThis work represents a significant advance in the safe deployment of LLMs for scientific data access. By focusing on multi-layered validation, prompt optimization, and continuous improvement, the authors establish a robust framework for minimizing hallucinations, hijacking, and bias. The integration of DSPy for programmatic prompt optimization and the use of self-refinement and task decomposition are particularly novel contributions, addressing key limitations of current LLM systems.\n\n**Broader Research Context**\n\nThese safety and bias considerations connect directly to ongoing research in AI safety, trustworthy computing, and responsible AI. By grounding answers in validated data and employing human-in-the-loop validation, the system aligns with best practices in the field[2][3]. The open, extensible framework also invites collaboration and further innovation, ensuring that future developments can be systematically evaluated and improved.\n\n**Implications for the Field**\n\nThe methods and insights presented here are applicable beyond Earth science, informing the design of LLM-powered systems in medicine, law, and other high-stakes domains. The emphasis on transparency, validation, and continuous improvement sets a new standard for trustworthy AI in open science applications.\n\n---\n\n## Key Takeaways\n\n- **Safety and bias are central to trustworthy LLM deployment in science.**\n- **Multi-layered validation and prompt optimization are effective strategies for reducing hallucinations and bias.**\n- **Technical innovations include programmatic prompt optimization, self-refinement, and systematic multi-objective evaluation.**\n- **These approaches are scalable and applicable to other domains, advancing the broader field of responsible AI.**\n\n---\n\n*Summary Table: Safety and Bias Mitigation Mechanisms (page references as in the paper)*\n\n| Mechanism                | Description                                                                 | Page Reference |\n|--------------------------|-----------------------------------------------------------------------------|---------------|\n| Prompt Optimization      | DSPy-based, systematic prompt refinement for clarity and accuracy           | 7, 11         |\n| Multi-Layered Validation | Deterministic and LLM-based checks for answer quality                       | 6, 12         |\n| Self-Refinement          | Additional inference layer to review and refine outputs                     | 12            |\n| Task Decomposition       | Separating challenging subtasks for specialized handling                    | 12            |\n| Human-in-the-Loop        | Manual validation and continuous feedback for improvement                   | 5, 6          |\n\n---\n\nThis section provides a comprehensive, technically rigorous, and educationally accessible overview of safety and bias considerations in LLM-powered Earth science systems, connecting them to both the broader AI literature and the specific contributions of the paper.", "citations": ["https://zapier.com/blog/ai-hallucinations/", "https://www.digitalocean.com/resources/articles/ai-hallucination", "https://insight.factset.com/ai-strategies-series-7-ways-to-overcome-hallucinations", "https://www.salesforce.com/blog/generative-ai-hallucinations/", "https://surferseo.com/blog/ai-hallucination/"], "page_number": 5}, {"id": "model-technique-selection", "title": "Model and Technique Selection Challenges", "content": "## Introduction\n\nThis section explores the multifaceted challenges of selecting, integrating, and optimizing models and techniques within agentic and compound AI systems, with a focus on applications such as natural-language-driven geospatial data retrieval. Understanding these challenges is crucial because model and technique selection directly impacts system performance, scalability, cost, and maintainability\u2014especially as new models (like the LLaMA 3.1 family) and optimization strategies (such as prompt engineering or inference-time scaling) become available[1][4]. The discussion here fits into the broader research context of making Earth science data more accessible and usable for non-experts, while navigating the complexities introduced by rapidly evolving AI technologies.\n\n## Core Content\n\n### Model and Technique Selection: Key Concepts\n\n**Model selection** refers to choosing which AI model(s) to use for a given task. In the context of the paper, this includes deciding whether to use large, general-purpose models like LLaMA 3.1 (in sizes ranging from 8B to 405B parameters) or ensembles of smaller, specialized models[1]. **Technique selection** involves determining strategies for optimizing model behavior, such as prompt optimization, inference-time reasoning, and task decomposition.\n\n**Integration challenges** arise because different models may require different data formats, preprocessing steps, or have conflicting assumptions about the task[1]. For example, as shown on page 11, integrating a new event type (e.g., \"crops\") into the system can reveal unexpected behavior in different LLaMA versions, affecting both accuracy and robustness.\n\n**Prompt optimization** is a technique to improve model outputs by refining the instructions or examples provided in the prompt. In the paper, this is implemented using the DSPy framework, which allows programmatic adjustment and systematic evaluation of prompts (see Section 2.6, page 13). The effectiveness of prompt optimization is measured across multiple metrics, including date extraction accuracy and consistency with user queries.\n\n**Inference-time scaling** refers to strategies for improving model performance at runtime, such as iterative self-refinement and task decomposition. These techniques allow the system to leverage multiple models or inference passes to address complex subtasks (described in Section 3.3, page 16).\n\n### Practical Examples and Methodological Choices\n\nA concrete example from the paper illustrates the complexity of model selection: when introducing a new event type (\"crops\"), LLaMA 3.1 8B and 405B responded differently. LLaMA 3.1 8B consistently generated the wrong event type, while LLaMA 3.1 405B often omitted the event type entirely. This highlights the importance of ongoing evaluation and the need for clear, maintainable instructions (see Figure 1, page 12)[1].\n\nThe paper also explores different prompt optimization techniques, such as Chain of Thought (CoT) and MIPRO, and their impact on model performance. CoT, which encourages the model to explain its reasoning, was found to offer a good balance of accuracy, transparency, and computational efficiency (see Figure 2, page 15)[4].\n\n### Mathematical Formulations\n\nTo quantify model performance, the paper uses a suite of metrics. For example, **date equivalence** can be measured as:\n\n\\[\n\\text{Date Equivalence} = \\frac{\\text{Number of answers with equivalent dates}}{\\text{Total number of answers}}\n\\]\n\nSimilarly, **JSON validity** is a binary metric:\n\n\\[\n\\text{Valid JSON} = \n\\begin{cases}\n1, & \\text{if answer is valid JSON} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\nThese metrics are combined to assess overall system performance and inform model and technique selection (Section 2.5, page 10).\n\n## Technical Details\n\n### Algorithm and Implementation\n\nThe DSPy framework is used for programmatic prompt optimization. The general workflow is as follows:\n\n1. **Define the task**: Specify the input-output structure (e.g., extract area, date, event_type from a natural language query).\n2. **Construct prompts**: Use DSPy to generate and refine prompts, incorporating techniques like CoT or few-shot learning.\n3. **Evaluate performance**: Run the model(s) on the evaluation dataset and measure performance across the defined metrics.\n4. **Iterate**: Adjust prompts, model choices, or optimization techniques based on evaluation results.\n\nBelow is a simplified pseudocode for the DSPy-based prompt optimization loop:\n\n\`\`\`python\n# Define DSPy signature for the task\nsignature = Signature(\"Extract area, date, event_type from query -> JSON\")\n\n# Initialize DSPy program with the signature\nprogram = DSPyProgram(signature)\n\n# Add optimization techniques (e.g., CoT, few-shot)\nprogram.add_technique(\"Chain of Thought\")\n\n# Evaluate on the evaluation dataset\nresults = program.evaluate(eval_dataset)\n\n# Analyze metrics and iterate\nif results[\"Date Equivalence\"] < target:\n    program.refine_prompt()\n\`\`\`\n\nThis process is repeated for different models and techniques, with results tracked to guide future improvements (Section 2.6, page 13).\n\n### Parameter Choices and Design Decisions\n\nKey design decisions include:\n- **Model size**: Balancing accuracy and computational cost by selecting between LLaMA 3.1 8B, 70B, and 405B (see Figure 2, page 15).\n- **Prompt optimization**: Choosing between zero-shot, few-shot, CoT, and MIPRO techniques based on performance and resource constraints.\n- **Inference-time scaling**: Deciding when to use iterative self-refinement or task decomposition to address specific subtasks (see Figure 3, page 16).\n\n### Connection to Other Sections\n\nModel and technique selection is closely tied to the evaluation framework (Section 2.5) and the implementation details in DSPy (Section 2.6). The choice of metrics and optimization strategies directly impacts the results discussed in the optimization and results section (Section 3).\n\n## Significance and Connections\n\n### Novelty and Contributions\n\nThe approach described in the paper is innovative because it combines state-of-the-art LLMs with systematic prompt optimization and inference-time scaling techniques, all within a framework designed for maintainability and extensibility. This allows for robust performance across a range of tasks and enables rapid iteration as new models or techniques become available[1][4].\n\nA key contribution is the use of DSPy for programmatic prompt optimization, which facilitates reproducible and maintainable AI system development. The paper also highlights the importance of continuous evaluation and adaptation, as shown by the case study with new event types (see page 12).\n\n### Broader Research Context\n\nThese challenges are not unique to geospatial data retrieval. Model and technique selection is a critical issue in many areas of applied AI, including healthcare, finance, and education. The lessons learned here\u2014such as the need for clear instructions, systematic evaluation, and flexible integration strategies\u2014are widely applicable[1][5].\n\n### Implications for the Field\n\nThe methods and insights presented in the paper have significant implications for the development of agentic and compound AI systems. By addressing the challenges of model and technique selection, researchers and practitioners can build more robust, scalable, and maintainable AI solutions. The focus on transparency, reproducibility, and continuous improvement sets a new standard for AI system development in scientific and enterprise settings[1][4].\n\n---\n\n**Summary Table: Key Challenges and Solutions in Model and Technique Selection**\n\n| Challenge                     | Solution/Approach                | Example from Paper                  |\n|-------------------------------|----------------------------------|-------------------------------------|\n| Model compatibility           | Data alignment, retraining       | LLaMA 3.1 integration (p.11)        |\n| Prompt optimization           | DSPy, CoT, MIPRO                 | Prompt refinement (p.13)            |\n| Inference-time scaling        | Self-refinement, task decomp.    | Figure 3 (p.16)                     |\n| Continuous evaluation         | Metrics, LLM-as-judge            | Date equivalence, LLM-judge (p.10)  |\n| Maintainability/extensibility | DSPy, clear instructions         | New event type case study (p.12)    |", "citations": ["https://www.rapidcanvas.ai/blogs/the-challenges-of-multi-model-ai-systems-integration-and-management", "https://www.anthropic.com/research/evaluating-ai-systems", "https://cloudian.com/guides/ai-infrastructure/6-types-of-ai-workloads-challenges-and-critical-best-practices/", "https://www.eweek.com/artificial-intelligence/ai-model-optimization/", "https://www.sandtech.com/insight/the-top-5-ai-challenges-insights-and-solutions/"], "page_number": 5}, {"id": "dataset-limitations", "title": "Evaluation Data Set Limitations and Future Directions", "content": "## Understanding the Evaluation Data Set: Limitations and Future Directions\n\nThis section provides a critical appraisal of the evaluation data set used to validate the natural language-driven geospatial query system. Here, we discuss the inherent limitations, explore promising avenues for data set expansion and refinement, and highlight ongoing methodological developments. Understanding these limitations is essential for interpreting the system\u2019s reported performance and for guiding future research in open-access Earth science data retrieval[2][3][5].\n\nAppreciating data set limitations is a cornerstone of robust research practice. In this context, the evaluation data set serves as a benchmark for assessing the accuracy and reliability of a Large Language Model (LLM)\u2013enabled system designed to democratize access to complex, spatiotemporal Earth science data. Critically examining the data set\u2019s characteristics\u2014such as its size, representativeness, and the nature of its query-answer pairs\u2014allows researchers to identify potential biases, recognize boundaries for generalization, and plan targeted improvements for subsequent iterations.\n\n## Core Concepts and Key Limitations\n\n**Ambiguity in Natural Language Queries**  \nNatural language is inherently ambiguous. While the data set was carefully curated with domain experts, many queries\u2014especially those involving relative time references (e.g., \u201clast week,\u201d \u201cthis season\u201d)\u2014can have multiple valid interpretations. This ambiguity directly affects the reliability of the evaluation metrics, particularly for tasks like date extraction and event type identification (see page 6, Section 2.2). For example, the start date of \u201cthis spring\u201d could plausibly be March 1 or the vernal equinox (March 20), depending on context. Addressing this ambiguity is non-trivial and requires both clear specifications and robust validation procedures.\n\n**1:1 Ratio of Golden Queries and Answers**  \nThe data set consists of over 100 unique query-answer pairs, each manually validated and translated into a standardized JSON format. This 1:1 correspondence ensures deterministic evaluation but limits the system\u2019s exposure to the variability and ambiguity present in real-world, user-generated queries. In practice, a single natural language query might yield several correct answers, especially when considering synonyms, paraphrases, or alternative valid interpretations (page 6, Section 2.2).\n\n**Example:**\n\`\`\`\nQuery: \"Flooding in Seoul on July 14, 2023\"\nAnswer: {\"area\": \"Seoul\", \"date\": \"2023-07-14\", \"event_type\":\"flood\"}\n\`\`\`\nWhile this example is unambiguous, a query like \u201cFind burn scars in the Andes from last season\u201d introduces uncertainty regarding the exact date range for \u201clast season.\u201d\n\n## Opportunities for Expansion and Refinement\n\n**Expanding the Answer Set**  \nThe current data set could be enhanced by including multiple valid answers for ambiguous queries, reflecting real-world usage. This change would better simulate the challenges faced by deployed systems and provide a more nuanced performance assessment. For example, for a query about \u201clast season,\u201d the answer set could include both March 1 and the vernal equinox as valid dates, with appropriate justifications.\n\n**Classifying Query Complexity**  \nIntroducing a taxonomy of query complexity\u2014such as explicit vs. relative temporal references, simple vs. compound event types, and single vs. multiple geographical areas\u2014would enable more granular system evaluation. This approach would help researchers identify which aspects of the task are most challenging and prioritize improvements accordingly.\n\n**Supporting Multiple Areas or Event Types**  \nExtending the data set to include queries that reference multiple areas, event types, or both would further stress-test the system. For instance, a query like \u201cFind flooding in both Seoul and Tokyo in the last week\u201d would require the system to parse and combine multiple constraints, challenging its natural language understanding and reasoning capabilities.\n\n## Technical Details and Implementation\n\n**Semi-Automatic Data Generation**  \nTo address the labor-intensive nature of manual data curation, the authors implemented a semi-automatic validation pipeline using an LLM-as-a-judge (page 7, Section 2.3). This approach leverages the reasoning capabilities of a language model to validate query-answer pairs, identify inconsistencies, and suggest corrections. For example, the LLM-judge can catch off-by-one errors in date calculations or flag ambiguous interpretations. This hybrid approach improves the efficiency and scalability of data set refinement while maintaining high quality.\n\n**Algorithm for LLM-as-a-Judge Validation**\n\`\`\`python\ndef validate_with_llm_judge(query, answer):\n    prompt = f\"Given the query: \'{query}\', is \'{answer}\' a valid answer? Please provide a rationale.\"\n    llm_response = call_llm(prompt)\n    return llm_response[\"is_valid\"], llm_response[\"rationale\"]\n\`\`\`\nThis process ensures that both the query and answer are scrutinized for consistency, reducing the risk of undetected errors in the evaluation data (page 7).\n\n**Continuous Evaluation Systems**  \nTo support ongoing system development, the authors advocate for the implementation of continuous evaluation systems. These systems automatically track performance regressions, monitor the impact of new features, and provide actionable feedback for iterative improvement. This approach is especially valuable in the context of multi-component, agentic AI systems, where changes in one module can have unforeseen effects on overall performance (page 10, Section 3.1).\n\n## Mathematical Foundations and Metric Formulation\n\n**Evaluation Metrics**\nThe authors define 11 evaluation metrics to systematically assess system performance (page 8, Section 2.5). These metrics are designed to capture both syntactic and semantic fidelity in the generated answers. Key metrics include:\n- **Valid JSON:** Ensures the answer is syntactically correct JSON.\n- **Date Equivalence:** Checks if generated dates match the golden answer.\n- **Date Consistency:** Determines if generated dates are logically consistent with the query, using an LLM-judge for assessment.\n\nThe normalized insertion-deletion similarity is used to measure equivalence for area and event-type values:\n\\[\n\\text{similarity}(a, b) = 1 - \\frac{\\text{levenshtein}(a, b)}{\\max(\\text{len}(a), \\text{len}(b))}\n\\]\nwhere \\(levenshtein(a, b)\\) is the Levenshtein distance between strings \\(a\\) and \\(b\\). This metric allows for flexible matching of equivalent but non-identical values.\n\n**Example Metric Application**\n\`\`\`python\nfrom Levenshtein import distance\n\ndef normalized_similarity(a, b):\n    max_len = max(len(a), len(b))\n    if max_len == 0:\n        return 1.0\n    return 1.0 - distance(a, b) / max_len\n\`\`\`\nThis approach is robust to minor variations in phrasing or spelling, reflecting real-world usage more accurately (page 8).\n\n## Significance and Connections to Broader Research\n\n**Novelty and Importance**  \nThe work presented here is significant for several reasons. First, it introduces a rigorously validated, open evaluation data set tailored to Earth science data retrieval\u2014a domain where standardized benchmarks were previously lacking (page 6, Section 2.2). Second, it demonstrates the practical utility of combining manual and automated validation techniques to improve data quality and scalability. Third, the systematic application of multiple evaluation metrics sets a new standard for assessing complex, multi-faceted LLM-based systems.\n\n**Broader Implications and Future Directions**  \nThe limitations and future directions discussed here have important implications for the broader field of AI-driven scientific data access. By addressing ambiguity, expanding answer diversity, and supporting continuous evaluation, this work lays the groundwork for more robust, maintainable, and user-centric systems. These innovations are not only relevant for Earth science but also for other domains where natural language interfaces can democratize access to complex data.\n\n**Connections to Other Sections**  \nThe evaluation framework developed in this section underpins the system optimization experiments described in Section 3, where prompt engineering, model selection, and multi-step inference are evaluated against the established metrics (see Figure 1 and Figure 2, pages 9\u201310). The integration of semi-automatic data generation and continuous evaluation also reflects the authors\u2019 commitment to open, collaborative, and iterative research practices.\n\n## Summary\n\nThis section has provided a comprehensive analysis of the evaluation data set\u2019s limitations and future directions. By critically examining the data set\u2019s structure, metrics, and validation procedures, we gain a deeper understanding of the system\u2019s strengths and weaknesses. The ongoing work on semi-automatic data generation and continuous evaluation promises to further enhance the system\u2019s robustness, making it a valuable resource for the broader research community[2][5].", "citations": ["https://blog.wordvice.com/how-to-present-study-limitations-and-alternatives/", "https://www.aje.com/arc/how-to-write-limitations-of-the-study/", "https://libguides.rio.edu/c.php?g=1319657&p=9706829", "https://libguides.usc.edu/writingguide/limitations", "https://www.sfedit.net/how-to-write-the-limitations-of-a-scientific-study-examples-and-explanations/"], "page_number": 5}]}, {"id": "discussion-and-future", "title": "Discussion, Implications, and Future Work", "content": "Below is a comprehensive, educational presentation of the \u201cDiscussion, Implications, and Future Work\u201d section as requested, tailored for advanced researchers and graduate students. This content is structured to guide readers from foundations to advanced insights, using clear examples, mathematical expressions, and direct references to the research paper.\n\n---\n\n## Section Overview and Importance\n\nThe discussion, implications, and future work section is a cornerstone of any research paper, serving as the bridge between empirical findings and the broader scientific and practical landscape[1][3][5]. In this context, it is where you:\n- **Summarize the key findings** from your experiments,\n- **Interpret** what these findings mean for open science, AI system development, and ongoing research,\n- **Discuss the implications** for the field, and\n- **Identify future directions** and remaining challenges.\n\nUnderstanding this section is crucial because it answers the \u201cso what?\u201d question: Why do these results matter? How do they advance the field? What gaps remain to be filled? This is essential for both researchers who want to build on your work and practitioners aiming to apply your findings in real-world settings[1][2][3].\n\n## Core Content: Key Concepts and Their Significance\n\n**Summary of Key Findings**\n\nThe paper demonstrates that large language models (LLMs) can be used to translate natural language queries into structured parameters for Earth science data retrieval with high accuracy, but challenges remain in temporal reasoning and robustness to ambiguous queries[3]. The system reformulates geospatial querying as a Named Entity Recognition (NER) task, mapping area, date, and event type from natural language to structured JSON output. Across 11 evaluation metrics (detailed on page 7), the system achieved near-perfect performance on most subtasks, except for date extraction, which saw accuracy gaps of up to 35% in some experiments[3].\n\n**Implications for Open Science and AI**\n\nThis research underscores the potential of LLMs to democratize access to Earth science data by reducing the need for specialized query languages and technical expertise[3]. The implications are profound for open science: more researchers\u2014including those outside informatics\u2014can now leverage complex datasets for climate monitoring, policy planning, and disaster response. The study also highlights the importance of multi-dimensional quality optimization, balancing accuracy, cost, and latency\u2014essential for real-world deployment[3][5].\n\n**Addressing Bias, Safety, and Model Selection**\n\nThe paper emphasizes the need for continuous evaluation to address safety, bias, and model selection challenges. By systematically monitoring performance across models and prompts, the authors identify how subtle changes (such as adding new event types) can lead to unexpected regressions or failures in instruction following (see Figure 1, page 7). This underlines the importance of clear specification and robust validation mechanisms in developing agentic and compound AI systems[3][5].\n\n**Future Directions**\n\nThe authors outline several promising future directions:\n- **Semi-automatic evaluation data generation:** Expanding and refining the dataset using LLM-as-a-judge to ensure systematic validation and coverage of edge cases (page 9).\n- **Programming and evaluation frameworks:** Continued development and adoption of frameworks like DSPy for programmatic prompt optimization and multi-objective evaluation (page 10).\n- **Human-in-the-loop and copiloting approaches:** Integrating human expertise for complex or ambiguous cases, and exploring hybrid workflows where AI and human operators collaborate for higher reliability[3].\n\n## Technical Details: Implementation and Methodology\n\n**Implementation in DSPy**\n\nThe authors leveraged DSPy, an open-source Python framework for programmatic prompt optimization, to translate their handwritten prompts into a reproducible, automated workflow (page 10). DSPy standardizes prompt formatting and evaluation, making it easier to maintain and extend the system. However, DSPy was originally designed for single-objective optimization; the authors address this by evaluating each objective separately and using instrumentation to log performance across multiple metrics (page 10-11).\n\n**Algorithmic Approach**\n\nHere is a simplified pseudocode representation of the core processing pipeline:\n\n\`\`\`python\ndef process_query(query, today=None):\n    # Augment query with today\'s date if needed (for relative time references)\n    if today:\n        query = query + f\" Today is {today}.\"\n    # Use LLM to extract entities: area, date, event_type\n    answer = llm_prompt(query)\n    # Validate JSON and check for required keys\n    if is_valid_json(answer):\n        result = validate_metrics(answer)\n        return result\n    else:\n        return {\"error\": \"Invalid JSON\"}\n\`\`\`\nFor multi-objective evaluation, each metric (e.g., valid JSON, key presence, event type consistency) is checked independently, and performance is logged for analysis (see page 7 for metric definitions).\n\n**Parameter and Model Selection**\n\nThe paper compares performance across LLaMA 3.1 model sizes (405B, 70B, 8B) and prompting techniques (zero-shot, few-shot, Chain of Thought, MIPRO), highlighting the trade-offs between accuracy, cost, and throughput (see Figure 2, page 12). Notably, LLaMA 3.1 8B with Chain of Thought (CoT) offered a favorable balance of performance, transparency, and efficiency, making it the focus of ongoing optimization efforts[3].\n\n**Inference-Time Scaling**\n\nTo address remaining accuracy gaps in date extraction, the authors explore inference-time scaling strategies such as self-refinement and task decomposition (Figure 3, page 13). In self-refinement, a second model reviews and refines the output of the first, while in task decomposition, different models are assigned to different subtasks (e.g., one for date parsing, another for area extraction), with a final model synthesizing the results. This approach is mathematically motivated by the idea of maximizing the joint probability across subtasks:\n\n\\[\nP(\\text{answer} | \\text{query}) = \\prod_{i} P(\\text{subtask}_i | \\text{query})\n\\]\n\nwhere each subtask is independently optimized and then combined, reducing error propagation[3].\n\n## Significance and Connections\n\n**Novelty and Contributions**\n\nThis work is significant because it introduces a robust, open-source framework for natural language-driven Earth science data retrieval, validated on a carefully curated dataset (over 100 query-answer pairs, page 5). The systematic evaluation (11 metrics, page 7) and continuous optimization approach set a new standard for measuring and improving LLM-based systems in scientific applications[3][5].\n\n**Connections to Broader Research**\n\nThe findings connect to broader research on open science, human-in-the-loop AI, and multi-objective optimization. The paper\u2019s emphasis on continuous evaluation and clear specification mechanisms is aligned with recent advances in agentic and compound AI systems, as well as best practices from the natural language processing and geospatial communities[3][5].\n\n**Implications for the Field**\n\nThe implications extend beyond Earth science: the methods and frameworks developed here can be adapted to other domains where natural language interfaces to structured data are needed. The focus on human-in-the-loop and copiloting approaches is particularly relevant for applications requiring high reliability and accountability[3][5].\n\n---\n\n## Summary Table: Key Concepts\n\n| Concept                | Description                                                                 | Reference (Page) |\n|------------------------|-----------------------------------------------------------------------------|------------------|\n| LLM-driven NER         | Mapping natural language to structured geospatial queries                   | 5                |\n| Evaluation Metrics     | 11 metrics for systematic validation (e.g., valid JSON, date equivalence)   | 7                |\n| DSPy Framework         | Programmatic prompt optimization and evaluation                             | 10               |\n| Model Comparison       | LLaMA 3.1 405B, 70B, 8B; prompting techniques                               | 12               |\n| Inference-Time Scaling | Self-refinement, task decomposition for improved accuracy                   | 13               |\n\n## Closing Remarks\n\nThis section not only summarizes the technical achievements and practical implications of the research but also provides a roadmap for future work and a call to action for the broader community to contribute to open, accessible, and robust AI-driven science[3][5]. By combining rigorous evaluation, innovative optimization strategies, and a commitment to open science principles, this work advances the state of the art in natural language interfaces for scientific data[1][3][5].", "citations": ["https://www.sjsu.edu/writingcenter/docs/handouts/Discussion%20Section%20for%20Research%20Papers.pdf", "https://library.sacredheart.edu/c.php?g=29803&p=185933", "https://www.scribbr.com/dissertation/discussion/", "https://libguides.usc.edu/writingguide/discussion", "https://www.ref-n-write.com/blog/discussion-section-examples-and-writing-tips/"], "page_number": 6, "subsections": [{"id": "multi-dimensional-optimization", "title": "Multi-Dimensional Quality Optimization", "content": "## Multi-Dimensional Quality Optimization in Compound AI Systems\n\nThis section addresses the critical challenge of optimizing multiple quality dimensions simultaneously within compound AI systems, focusing on accuracy, cost, and latency trade-offs. As compound AI systems integrate multiple specialized AI components to tackle complex tasks, achieving a balanced and efficient overall system performance requires co-optimization across these dimensions. We explore recent frameworks for inference-time scaling as a practical mechanism to dynamically adjust these trade-offs during deployment, and discuss the persistent challenges in managing the complexity and unification of diverse AI system aspects. This understanding is essential to grasp the broader research context of developing robust, scalable, and sustainable AI solutions, as advanced in the referenced paper.\n\nThe need for multi-dimensional quality optimization follows from the inherent characteristics of compound AI systems, which leverage modular, interacting AI components rather than relying on a single monolithic model. These components each have different strengths, costs, and processing latencies, influencing the overall system\u2019s behavior and user experience. By optimizing these dimensions jointly, the system becomes more flexible and better aligns with practical constraints such as computational resources, response time requirements, and end-user accuracy expectations, a core motivation demonstrated in the paper\u2019s Earth science use-case and evaluation framework (see pages 6-10).\n\n---\n\n### Core Concepts of Multi-Dimensional Quality Optimization\n\n**Definition and Importance of Dimensions**\n\n- **Accuracy** refers to how precisely the system fulfills its task goals, such as correctly extracting geospatial parameters from natural language queries (see Section 2.5 on page 7 for evaluation metrics).\n- **Cost** primarily involves computational expenses, including token usage in LLM inference and operational expenses related to model size and complexity.\n- **Latency** captures the response time from query input until answer output, critical for interactive applications like natural language-driven Earth observation data retrieval.\n\nThese dimensions are often at odds: improving accuracy by using larger, more complex models can increase cost and latency, while minimizing cost and latency might degrade accuracy. The research formalizes these trade-offs, as illustrated in Figure 2 on page 10, showing the interplay between prompt sophistication, model size, and performance metrics (accuracy, token spend, throughput).\n\n**Compound AI Systems and Optimization**\n\nCompound AI systems (see [1][2][3]) are composed of multiple specialized models and components working in concert. Unlike monolithic models, these systems allow dynamic composition and selective invocation of components based on task requirements, enabling fine-grained trade-off control. For example, the paper\u2019s approach leverages model scaling from a large 405B parameter LLaMA model down to a smaller 8B variant, weighing the improved latency and cost against accuracy losses (pages 9-11).\n\n**Mathematical Formulation**\n\nFormally, multi-dimensional optimization can be expressed as a constrained optimization problem balancing competing objectives. Suppose a compound AI system with parameters \\(\\theta\\) and configurations \\(c\\), the optimization problem can be formulated as:\n\n\\[\n\\max_{\\theta, c} \\quad \\text{Accuracy}(\\theta, c) \\quad \\text{subject to} \\quad \\text{Cost}(\\theta, c) \\leq C_{\\max}, \\quad \\text{Latency}(\\theta, c) \\leq L_{\\max}\n\\]\n\nWhere:\n\n- \\(\\text{Accuracy}(\\theta, c)\\) evaluates the system\u2019s correctness (e.g., JSON field extraction accuracy from queries).\n- \\(\\text{Cost}(\\theta, c)\\) measures resource usage such as tokens spent or compute time.\n- \\(\\text{Latency}(\\theta, c)\\) measures inference response time.\n- \\(C_{\\max}\\) and \\(L_{\\max}\\) are user-defined or system-imposed resource and latency budgets.\n\nThis constrained approach frames the trade-off and enables systematic exploration of the design space (see page 10, Figure 2).\n\n**Inference-Time Scaling**\n\nA key technique to handle these trade-offs dynamically is *inference-time scaling*, as detailed in Section 3.3 (page 12). This method adapts the model complexity and computation at query time, for example by using smaller models for simpler subtasks or iterative refinement strategies where an initial answer is refined conditionally only when necessary. Figure 3 (page 12) illustrates these modes: single-step chain-of-thought prompting, iterative self-refinement with two models, and task decomposition where subtasks are distributed across models specialized for those subtasks.\n\n**Examples**\n\n- The paper shows how using a smaller 8B parameter LLaMA 3.1 model with chain-of-thought prompting offers competitive accuracy with reduced token usage and faster response than the 405B variant (Figures 1 and 2).\n- Iterative self-refinement adds a second inference pass balancing accuracy improvements against increased latency and cost.\n\nThe reasoning behind these approaches is to optimize performance flexibly depending on the query complexity and system constraints, rather than statically fixing model size or prompt design.\n\n---\n\n### Technical Details of Implementation\n\n**Prompt Optimization**\n\nPrompt designs critically impact accuracy and token usage. The paper leverages DSPy, an open-source prompt optimization framework, to systematize and automate prompt tuning (Section 2.6, pages 8-11). DSPy facilitates defining structured prompts as \u201csignatures\u201d and running multi-objective optimization over metrics like date equivalence, consistency, and JSON validity.\n\nAlgorithmically, DSPy evaluates a prompt signature \\(p\\) over a dataset \\(D\\) with metrics \\(\\{m_1, ..., m_n\\}\\) and uses search and suggestion steps to improve prompt components while balancing complexity and token cost:\n\n\`\`\`python\nfor trial in range(max_trials):\n    candidate_prompt = generate_candidate_prompt(current_prompt)\n    results = evaluate_prompt(candidate_prompt, D, metrics)\n    if results meet improvement criteria:\n        current_prompt = candidate_prompt\n\`\`\`\n\nWhile DSPy natively supports only single-objective optimization well, the authors manage multi-objective optimization by independently evaluating each metric and collectively analyzing trade-offs (page 9).\n\n**Inference-Time Scaling Procedures**\n\nAs shown in Figure 3 (page 12), three inference workflows are implemented:\n\n1. **Single-step Chain-of-Thought (CoT):** generates answer directly.\n2. **Iterative Self-Refinement:** Model \\(a\\) produces initial output; Model \\(b\\) critiques and refines it.\n3. **Task Decomposition:** separate models handle subtasks (e.g., date interpretation vs. event extraction), with a synthesis model \\(c\\) combining results.\n\nParameter choices such as model sizes (8B for \\(a\\) and \\(c\\), variable for \\(b\\)) and prompt contents are tuned based on token costs and accuracy trade-offs demonstrated in Section 3.3 (pages 12-14).\n\n**Design Decision Rationales**\n\n- Using smaller models for less complex subtasks saves cost without major accuracy loss.\n- Refinement steps target the most challenging subtask\u2014temporal reasoning\u2014where errors are more frequent.\n- Systematic prompt tuning and evaluation with DSPy ensure maintainability and extensibility under evolving requirements (pages 8-11).\n\n---\n\n### Significance and Broader Connections\n\nThis multi-dimensional quality optimization approach is novel in how it formalizes and operationalizes balancing accuracy, cost, and latency in compound AI systems, particularly for real-world, knowledge-intensive tasks like Earth science data queries. By integrating programmatic prompt optimization, inference-time scaling, and multi-model task decomposition, the research addresses limitations of monolithic LLM deployments and static design approaches.\n\nThe work aligns with broader trends highlighted in the Berkeley AI Research Lab and industry discussions, which emphasize the shift from single large models to intelligent orchestration of specialized components for better performance and operational efficiency ([1][2][3]). The capability to dynamically trade off quality dimensions at runtime is key to deploying AI systems that are both powerful and practical, especially when serving diverse users with variable resource constraints.\n\nKey innovations include:\n\n- A rigorously validated multidimensional evaluation metric set to guide optimization (page 7).\n- Use of DSPy for programmatic prompt tuning addressing multiple objectives.\n- Introduction of inference-time scaling methods combining self-refinement and task decomposition (page 12).\n- Empirical demonstration of trade-offs with real data and multiple LLaMA model sizes (Figures 1, 2, and 3).\n\nThese contributions inform the future development of scalable, flexible compound AI frameworks capable of continuous evaluation and evolution, essential for democratizing complex data access such as in Earth science.\n\n---\n\nThis comprehensive coverage clarifies the intricate balance of optimizing multiple quality factors in compound AI systems and situates the paper\u2019s methodology and findings within the broader AI research landscape.", "citations": ["http://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/", "https://www.databricks.com/glossary/compound-ai-systems", "https://www.artefact.com/blog/compound-ai-systems-the-future-of-specialized-intelligence/", "https://www.baseten.co/blog/compound-ai-systems-explained/", "https://www.ibm.com/think/topics/compound-ai-systems"], "page_number": 6}, {"id": "future-directions", "title": "Future Directions and Ongoing Work", "content": "Welcome to an in-depth educational exploration of the \"Future Directions and Ongoing Work\" section from the research paper \u201cToward Open Earth Science as Fast and Accessible as Natural Language.\u201d This guide is tailored for advanced researchers and graduate students, breaking down emerging trends, technical challenges, and novel ideas shaping the intersection of Large Language Models (LLMs) and scientific data systems. By the end, you\u2019ll understand how semi-automatic data generation, copiloting approaches, and continuous evaluation frameworks are setting the stage for the next generation of AI-driven science.\n\n---\n\n## Introduction\n\n**What this section covers:**  \nThis section outlines the forward-looking research directions emerging from the core findings of the paper. It focuses on three major themes:  \n1. **Semi-automatic evaluation data generation** for benchmarking and iterative improvement  \n2. **Programming and evaluation frameworks** that support systematic experimentation and maintainability  \n3. **Human-in-the-loop and copiloting approaches** that blend AI capabilities with expert human oversight  \n4. **LLM-infused batch-processing methods and traditional ML techniques** for continuous evaluation and failure diagnosis\n\n**Why this topic matters:**  \nUnderstanding these future directions is crucial for grasping the full potential and limitations of LLM-based earth science data systems. These developments address real-world needs for robust, maintainable, and human-AI collaborative solutions, especially as scientific datasets grow and evolve[5]. By mapping out these paths, the paper sets a research agenda for creating more reliable, accessible, and adaptable scientific tools.\n\n**How it fits into the broader research:**  \nThe approaches discussed here are logical extensions of the paper\u2019s main contributions\u2014leveraging LLMs for natural language query interpretation, creating rigorous evaluation datasets, and optimizing system performance using advanced prompts and frameworks. They also anticipate future challenges, such as scaling data validation, supporting new use cases, and integrating feedback mechanisms for continuous learning[3][5]. Connecting to earlier sections, these directions build on the foundation laid by Named Entity Recognition (NER) for geospatial queries, prompt optimization in DSPy, and multi-objective evaluation metrics described in Section 2.5 and 3.1 (see pages 7\u20139)[2].\n\n---\n\n## Core Content\n\n### 1. Semi-Automatic Evaluation Data Generation\n\n**Definition and Motivation:**  \nSemi-automatic evaluation data generation refers to the process of creating test cases and validation examples using a combination of automated tools (e.g., LLMs, data synthesis platforms) and human oversight. This leads to realistic, complex, and representative datasets for evaluating AI systems\u2014critical as manual annotation becomes impractical at scale[2][5].\n\n**Key Concepts:**  \n- **Data Diversity:** Ensures test cases cover a wide range of real-world scenarios, including ambiguous or incomplete queries.\n- **Validation Layers:** Adds automated checks (e.g., LLM-as-a-judge) and manual reviews to catch errors and ambiguities.\n- **Iterative Improvement:** Data generation is ongoing, with each iteration refining the dataset for better coverage and reliability.\n\n**Example from the Paper:**  \nThe paper\u2019s evaluation dataset grew from over 100 manually crafted query-answer pairs, validated both manually and with LLM-as-a-judge. This hybrid approach uncovered subtle errors such as off-by-one date calculations and implicit assumptions about \u201ctoday\u2019s date\u201d that would have been missed by manual or automated methods alone (see Section 2.3, pages 8\u20139)[2]. For instance, relative time queries like \u201clast season\u201d required explicit context (e.g., \u201cToday is June 4, 2024\u201d) to ensure consistent evaluation.\n\n### 2. Programming and Evaluation Frameworks\n\n**Definition and Motivation:**  \nProgramming frameworks like DSPy enable efficient, reproducible, and systematic experimentation with LLM prompts and system architectures. Evaluation frameworks codify metrics and procedures for assessing system performance across multiple dimensions (e.g., accuracy, latency, cost).\n\n**Key Concepts:**  \n- **Prompt Optimization:** Automated tuning of LLM prompts using frameworks such as DSPy, which supports both single- and (early stages of) multi-objective optimization.\n- **Evaluation Metrics:** The paper defines 11 key metrics, including syntactic correctness (JSON validity), semantic equivalence, and LLM-assisted judgment (e.g., date consistency), providing a detailed breakdown of system performance (see Section 2.5, page 10)[2].\n- **Multi-Objective Evaluation:** Current frameworks often require workarounds to handle multiple objectives, as DSPy was initially designed for single-objective optimization. The paper recommends independent evaluation of each objective for clarity and ablation studies (see Section 2.6, pages 10\u201311)[2].\n\n**Illustration:**  \nFigure 2 in the paper (page 14) shows how different prompting techniques (Ad Hoc, Chain of Thought, MIPRO) affect performance across metrics and model sizes. This visualizes the trade-offs between complexity, accuracy, and computational cost.\n\n### 3. Human-in-the-Loop and Copiloting Approaches\n\n**Definition and Motivation:**  \nHuman-in-the-loop (HITL) and copiloting approaches involve active collaboration between AI systems and human experts, leveraging the strengths of both for complex or ambiguous tasks.\n\n**Key Concepts:**  \n- **Expert Oversight:** Humans review and refine AI outputs, especially for ambiguous queries or novel scenarios.\n- **Copilot Systems:** AI assistants work alongside experts, suggesting options, explaining decisions, and iterating based on feedback.\n- **Feedback Loops:** Human input is used to improve both the dataset and the AI system, leading to continuous improvement.\n\n**Example:**  \nWhen the LLM-as-a-judge identified inconsistencies, human experts reviewed both the query and the model\u2019s rationale, correcting errors and clarifying ambiguities. This process not only improved the dataset but also informed future automation strategies (see Section 2.3, pages 8\u20139)[3].\n\n### 4. LLM-Infused Batch-Processing and Traditional ML\n\n**Definition and Motivation:**  \nBatch-processing methods powered by LLMs can automate large-scale data validation, while traditional machine learning (ML) techniques can address specific failure modes or edge cases.\n\n**Key Concepts:**  \n- **Batch Processing:** LLMs process large volumes of queries and answers in bulk, applying validation rules and generating feedback.\n- **Failure Diagnosis:** Traditional ML models (e.g., classifiers) can identify recurring error patterns or misclassified samples, supporting targeted improvements.\n- **Continuous Evaluation:** The system evolves over time, with new data and feedback incorporated into both the evaluation process and the underlying models.\n\n**Mathematical Perspective:**  \nLet $S$ represent a set of samples (queries), and let $\\sigma$ be a semantic type (e.g., date, location). For each sample $S \\in S$, a score $p^S_\\sigma$ is computed to indicate the likelihood that $S$ is of type $\\sigma$ (e.g., using ColNet or similar methods). The overall score for a type $\\sigma$ is the average:\n\n\\[\np_\\sigma = \\frac{1}{|S|} \\sum_{S \\in S} p^S_\\sigma\n\\]\n\nIn interactive settings, human feedback $H$ adjusts the scoring:\n\n\\[\nq^S_{H,\\sigma} = \n\\begin{cases}\n1 & \\text{if } \\text{is\\_type}(S, \\sigma) \\in H \\\\\n0 & \\text{if } \\text{not\\_type}(S, \\sigma) \\in H \\\\\np^S_\\sigma & \\text{otherwise}\n\\end{cases}\n\\]\n\nThe overall interactive score is:\n\n\\[\nQ_{H,\\sigma} = \\frac{1}{|S|} \\sum_{S \\in S} q^S_{H,\\sigma}\n\\]\n\nThis formulation captures the blending of automated and human-informed evaluation (see Section 3, page 7)[3].\n\n---\n\n## Technical Details\n\n### Implementation Specifics\n\n**Semi-Automatic Data Generation Pipeline**\n\n1. **Query Generation:**  \n   - Use templates and LLMs to generate diverse queries, including ambiguous and edge cases.\n   - Augment with real-world examples from domain experts.\n2. **Answer Synthesis:**  \n   - LLMs generate candidate answers, which are validated against ground truth.\n   - Automated checks verify JSON structure, key presence, and semantic equivalence.\n3. **LLM-as-Judge:**  \n   - An LLM reviews each query-answer pair, flagging inconsistencies and providing rationale.\n   - Human experts review flagged cases, correcting errors and updating the dataset.\n\n**Algorithm Example (Pseudocode):**\n\n\`\`\`python\ndef generate_and_validate_queries(query_templates, llm, human_experts):\n    queries = generate_queries(query_templates)\n    answers = {}\n    for query in queries:\n        answer = llm(query)\n        validation = llm_judge(query, answer)\n        if not validation[\'consistent\']:\n            answer = human_experts.review(query, answer)\n        answers[query] = answer\n    return answers\n\`\`\`\n\n**Parameter Choices and Design Decisions**\n\n- **Prompt Design:** The choice of prompt (e.g., Chain of Thought vs. Ad Hoc) significantly affects performance and interpretability. DSPy\u2019s automatic formatting and optimization streamline this process (see Section 3.2, pages 12\u201314)[2].\n- **Model Selection:** Balancing accuracy and cost is critical. The paper shows that smaller models (e.g., LLaMA 3.1 8B) can achieve competitive performance with lower latency and cost, making them suitable for continuous evaluation pipelines (see Figure 2, page 14)[2].\n- **Evaluation Metrics:** The 11 metrics cover syntactic, semantic, and LLM-assisted validation, ensuring comprehensive assessment (see Section 2.5, page 10)[2].\n\n---\n\n## Significance & Connections\n\n### Why This Approach Is Novel\n\nThe combination of semi-automatic data generation, advanced programming frameworks, and human-AI copiloting represents a significant step forward in building robust, maintainable, and adaptable scientific AI systems. By integrating feedback loops and continuous evaluation, these approaches address the limitations of static datasets and brittle automation[5][3].\n\n### Connections to Broader Research\n\nThese directions align with ongoing trends in AI, such as:\n- **Automated Metadata Generation:** Semi-automatic tools are increasingly used to handle the growing complexity and scale of digital repositories[2].\n- **AI Assistants for Data Wrangling:** Frameworks like ColNet and DSPy enable interactive, explainable AI workflows[3].\n- **Continuous Evaluation in ML:** The emphasis on iterative improvement and failure diagnosis reflects best practices in machine learning operations (MLOps) and AI quality assurance[5].\n\n### Innovations and Contributions\n\n- **Hybrid Data Generation:** The blend of automated and human validation ensures high-quality, representative datasets (see Section 2.3, pages 8\u20139)[2].\n- **Multi-Objective Evaluation:** The framework supports detailed performance analysis across multiple dimensions, enabling targeted improvements (see Section 2.5, page 10)[2].\n- **Copiloting for Science:** Human-in-the-loop and copilot approaches make AI systems more reliable and accessible to non-experts, democratizing scientific data analysis (see Section 3.3, pages 15\u201316)[3].\n\n### Implications for the Field\n\nThese advances have far-reaching implications:\n- **Scalability:** Enables the handling of ever-growing scientific datasets and user bases.\n- **Accessibility:** Lowers barriers for researchers outside of informatics, fostering broader participation in scientific discovery.\n- **Maintainability:** Supports ongoing system improvement and adaptation to new use cases and technologies.\n\nBy charting these future directions, the paper lays the groundwork for a new era of open, accessible, and collaborative earth science powered by AI[5][3][2].", "citations": ["https://www.tonic.ai", "https://ital.corejournals.org/index.php/ital/article/view/5889", "https://www.turing.ac.uk/sites/default/files/2022-11/aida_ai_assistants_tkde_2022_0.pdf", "https://www.e-assessment.com/news/ai-in-assessments-automated-item-generation/", "https://www.evidentlyai.com/llm-guide/llm-test-dataset-synthetic-data"], "page_number": 6}]}];
const citationsData: string[] = ["https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/intro-to-JSON/", "https://libraryjuiceacademy.com/shop/course/161-introduction-json-structured-data/", "https://arxiv.org/html/2408.11061v1", "https://monkt.com/recipies/research-paper-to-json/", "https://developer.apple.com/documentation/applenews/json-concepts-and-article-structure"];

export default function PaperPage() {
  const [activeSection, setActiveSection] = useState('');
  const [activeTab, setActiveTab] = useState<'images' | 'sources'>('sources'); // Default to sources
  const [imagesData, setImagesData] = useState<ImageData[]>([]);
  const [imagesLoading, setImagesLoading] = useState(true);
  
  // Fetch images from API
  useEffect(() => {
    const fetchImages = async () => {
      try {
        setImagesLoading(true);
        const response = await fetch(`http://localhost:8000/api/images/${paperData.arxiv_id}`);
        if (response.ok) {
          const images = await response.json();
          setImagesData(images);
          // If images are available, switch to images tab
          if (images && images.length > 0) {
            setActiveTab('images');
          }
        } else {
          console.error('Failed to fetch images:', response.statusText);
          setImagesData([]);
        }
      } catch (error) {
        console.error('Error fetching images:', error);
        setImagesData([]);
      } finally {
        setImagesLoading(false);
      }
    };

    fetchImages();
  }, []);
  
  // Initialize with the first section
  useEffect(() => {
    if (sectionsData?.length > 0) {
      setActiveSection(sectionsData[0].id);
    }
  }, []);
  
  // Get current section
  const currentSection = sectionsData?.find(section => section.id === activeSection);
  
  // Get relevant images for current section
  const getRelevantImages = (pageNumber: number | undefined): ImageData[] => {
    if (!pageNumber || !imagesData || !Array.isArray(imagesData)) return [];
    return imagesData.filter(img => img.page === pageNumber);
  };
  
  const relevantImages = getRelevantImages(currentSection?.page_number);
  
  // Get citations for current section
  const getSectionCitations = (sectionCitations?: string[]): string[] => {
    if (!sectionCitations || !Array.isArray(sectionCitations)) return [];
    return sectionCitations;
  };
  
  const sectionCitations = getSectionCitations(currentSection?.citations);

  return (
    <div className="min-h-screen flex flex-col bg-white">
      {/* Header */}
      <header className="bg-white sticky top-0 z-50 border-b border-gray-200">
        <div className="container mx-auto px-4 sm:px-6 lg:px-8">
          <div className="flex items-center justify-between h-16">
            <div className="flex items-center space-x-3">
              <Link href="/" className="flex items-center text-blue-600 hover:text-blue-700">
                <ArrowLeft className="w-6 h-6" />
              </Link>
              <h1 className="text-2xl font-bold text-gray-800 lowercase">deeprxiv</h1>
              <span className="text-lg text-gray-600 font-medium truncate max-w-md">
                {paperData.title}
              </span>
            </div>
          </div>
        </div>
      </header>

      {/* Main Content */}
      <main className="flex-grow container mx-auto px-0 py-0">
        <div className="grid grid-cols-1 lg:grid-cols-5 gap-x-0 min-h-screen">
          {/* Left Sidebar - Navigation */}
          <aside className="lg:col-span-1 bg-white p-6 border-r border-gray-200">
            <nav className="space-y-1">
              {sectionsData?.map((section) => (
                <button
                  key={section.id}
                  onClick={() => setActiveSection(section.id)}
                  className={`block w-full text-left px-4 py-2.5 rounded-md transition-colors ${
                    activeSection === section.id
                      ? 'bg-blue-50 text-blue-700 font-semibold'
                      : 'text-gray-700 hover:bg-gray-100'
                  }`}
                >
                  {section.title}
                </button>
              ))}
            </nav>
          </aside>

          {/* Center Content Area */}
          <div className="lg:col-span-3 bg-white p-6">
            {currentSection && (
              <>
                <h3 className="text-2xl font-semibold text-gray-800 mb-2">
                  {currentSection.title}
                </h3>
                <p className="text-sm text-gray-500 mb-6">
                  arXiv:{paperData.arxiv_id} â€¢ {paperData.authors}
                </p>
                <div className="prose max-w-none text-gray-700 leading-relaxed">
                  <ReactMarkdown
                    remarkPlugins={[remarkGfm, remarkMath]}
                    rehypePlugins={[rehypeKatex]}
                    className="prose prose-gray max-w-none"
                  >
                    {currentSection.content}
                  </ReactMarkdown>
                </div>
                
                {/* Subsections */}
                {currentSection.subsections && currentSection.subsections.length > 0 && (
                  <div className="mt-8 space-y-8">
                    {currentSection.subsections.map((subsection) => (
                      <div key={subsection.id} className="ml-6 border-l-4 border-blue-100 pl-6 py-4">
                        <h4 className="text-xl font-semibold text-gray-800 mb-4 text-blue-700">
                          {subsection.title}
                        </h4>
                        <div className="prose max-w-none text-gray-700 leading-relaxed">
                          <ReactMarkdown
                            remarkPlugins={[remarkGfm, remarkMath]}
                            rehypePlugins={[rehypeKatex]}
                            className="prose prose-gray max-w-none"
                          >
                            {subsection.content}
                          </ReactMarkdown>
                        </div>
                      </div>
                    ))}
                  </div>
                )}
              </>
            )}
          </div>

          {/* Right Sidebar - Images and Sources */}
          <aside className="lg:col-span-1 bg-white p-6 border-l border-gray-200">
            {/* Tab Buttons */}
            <div className="flex mb-4 border-b border-gray-200">
              <button
                onClick={() => setActiveTab('images')}
                className={`flex-1 py-2 px-4 text-center font-medium transition-colors border-b-2 ${
                  activeTab === 'images'
                    ? 'text-blue-700 border-blue-700 font-semibold'
                    : 'text-gray-600 border-transparent hover:text-gray-800'
                }`}
              >
                <ImageIcon className="inline-block w-4 h-4 mr-1" />
                Images
              </button>
              <button
                onClick={() => setActiveTab('sources')}
                className={`flex-1 py-2 px-4 text-center font-medium transition-colors border-b-2 ${
                  activeTab === 'sources'
                    ? 'text-blue-700 border-blue-700 font-semibold'
                    : 'text-gray-600 border-transparent hover:text-gray-800'
                }`}
              >
                <ExternalLink className="inline-block w-4 h-4 mr-1" />
                Sources
              </button>
            </div>

            {/* Images Tab Content */}
            {activeTab === 'images' && (
              <div>
                <p className="text-sm text-gray-600 mb-4">
                  Figures and tables related to the current section.
                </p>
                {imagesLoading ? (
                  <div className="text-center py-8">
                    <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto"></div>
                    <p className="text-sm text-gray-500 mt-2">Loading images...</p>
                  </div>
                ) : relevantImages.length > 0 ? (
                  <div className="grid grid-cols-2 gap-4">
                    {relevantImages.map((image, index) => (
                      <div
                        key={image.id || index}
                        className="aspect-square bg-gray-200 rounded-lg flex items-center justify-center cursor-pointer hover:bg-gray-300 transition-colors overflow-hidden"
                      >
                        <img
                          src={image.url || `/api/image/${image.id}`}
                          alt={`Figure ${index + 1}`}
                          className="max-w-full max-h-full object-contain p-2"
                        />
                      </div>
                    ))}
                  </div>
                ) : (
                  <div className="text-center py-8">
                    <ImageIcon className="w-12 h-12 text-gray-400 mx-auto mb-2" />
                    <p className="text-sm text-gray-500">No images for this section</p>
                  </div>
                )}
                {relevantImages.length > 0 && (
                  <p className="text-xs text-gray-500 mt-2 text-center">
                    Click on an image to enlarge.
                  </p>
                )}
              </div>
            )}

            {/* Sources Tab Content */}
            {activeTab === 'sources' && (
              <div>
                <p className="text-sm text-gray-600 mb-4">
                  Citations and references mentioned in this section.
                </p>
                {sectionCitations.length > 0 ? (
                  <div className="space-y-3">
                    {sectionCitations.map((citation, index) => (
                      <div
                        key={index}
                        className="bg-gray-50 p-3 rounded-lg border border-gray-200 hover:bg-gray-100 transition-colors"
                      >
                        <div className="flex items-start space-x-2">
                          <span className="inline-flex items-center justify-center w-6 h-6 bg-blue-100 text-blue-700 text-xs font-semibold rounded-full flex-shrink-0 mt-0.5">
                            {index + 1}
                          </span>
                          <div className="flex-1 min-w-0">
                            <p className="text-sm font-medium text-gray-800 mb-1">
                              Reference {index + 1}
                            </p>
                            <p className="text-xs text-gray-600 break-words">
                              {citation}
                            </p>
                            <a
                              href={citation}
                              target="_blank"
                              rel="noopener noreferrer"
                              className="inline-flex items-center text-xs text-blue-600 hover:text-blue-800 hover:underline mt-2"
                            >
                              <ExternalLink className="w-3 h-3 mr-1" />
                              View Source
                            </a>
                          </div>
                        </div>
                      </div>
                    ))}
                  </div>
                ) : (
                  <div className="text-center py-8">
                    <ExternalLink className="w-12 h-12 text-gray-400 mx-auto mb-2" />
                    <p className="text-sm text-gray-500">No citations for this section</p>
                  </div>
                )}
              </div>
            )}
          </aside>
        </div>
      </main>
    </div>
  );
}
