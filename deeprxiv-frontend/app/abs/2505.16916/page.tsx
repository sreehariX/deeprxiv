'use client';

import { useState, useEffect, useRef } from 'react';
import Link from 'next/link';
import { ArrowLeft, Image as ImageIcon, ExternalLink, X, Play, FileText, BookOpen, Menu } from 'lucide-react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import remarkMath from 'remark-math';
import rehypeKatex from 'rehype-katex';
import 'katex/dist/katex.min.css';

// Custom CSS for hiding scrollbars and responsive margins
const customStyles = `
  .scrollbar-hide {
    -ms-overflow-style: none;  /* Internet Explorer 10+ */
    scrollbar-width: none;  /* Firefox */
  }
  .scrollbar-hide::-webkit-scrollbar {
    display: none;  /* Safari and Chrome */
  }
  .main-content {
    margin-left: 0;
    margin-right: 0;
  }
  @media (min-width: 768px) {
    .main-content {
      margin-left: 352px;
      margin-right: 0;
    }
  }
  @media (min-width: 1024px) {
    .main-content {
      margin-left: 416px;
      margin-right: 512px;
    }
  }
`;

// Types for better TypeScript support
interface ImageData {
  id: string;
  page: number;
  original_position?: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  expanded_position?: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  path?: string;
  url?: string;
}

interface SubSection {
  id: string;
  title: string;
  content: string;
  citations?: string[];
  page_number?: number;
}

interface Section {
  id: string;
  title: string;
  content: string;
  citations?: string[];
  page_number?: number;
  subsections?: SubSection[];
}

// Paper data
const paperData = {
  id: 2,
  arxiv_id: '2505.16916',
  title: 'Backdoor Cleaning without External Guidance in MLLM Fine-tuning',
  authors: 'Xuankun Rong, Wenke Huang, Jian Liang, Jinhe Bi, Xun Xiao, Yiming Li, Bo Du, Mang Ye',
  abstract: 'Multimodal Large Language Models (MLLMs) are increasingly deployed in fine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt general-purpose models to downstream tasks. This flexibility, however, introduces serious security risks, as malicious fine-tuning can implant backdoors into MLLMs with minimal effort. In this paper, we observe that backdoor triggers systematically disrupt cross-modal processing by causing abnormal attention concentration on non-semantic regions—a phenomenon we term attention collapse. Based on this insight, we propose Believe Your Eyes (BYE), a data filtering framework that leverages attention entropy patterns as self-supervised signals to identify and filter backdoor samples. BYE operates via a three-stage pipeline: (1) extracting attention maps using the fine-tuned model, (2) computing entropy scores and profiling sensitive layers via bimodal separation, and (3) performing unsupervised clustering to remove suspicious samples. Unlike prior defenses, BYE requires no clean supervision, auxiliary labels, or model modifications. Extensive experiments across various datasets, models, and diverse trigger types validate BYE’s effectiveness: it achieves near-zero attack success rates while maintaining clean-task performance, offering a robust and generalizable solution against backdoor threats in MLLMs. Our code is publicly available at: https://github.com/XuankunRong/BYE.',
  processed: true
};

// Sections data
const sectionsData: Section[] = [{"id": "foundation-and-context", "title": "Foundation and Context: Backdoor Threats in Multimodal Large Language Models", "content": "## Foundation and Context: Backdoor Threats in Multimodal Large Language Models\n\nThis section lays the crucial groundwork for understanding backdoor threats in Multimodal Large Language Models (MLLMs), focusing on the motivation behind the research and the unique challenges MLLMs face when subjected to malicious fine-tuning. It establishes how MLLMs, which process and integrate visual and textual inputs, have transformed applications ranging from visual question answering to healthcare diagnostics, but are also increasingly vulnerable to subtle and dangerous backdoor attacks during downstream adaptation. By introducing the concept of attention collapse as a fingerprint of backdoor poisoning, this section frames the investigation into leveraging internal attention behaviors for unsupervised detection and mitigation\u2014an approach central to the paper\'s novel contributions.\n\n### Importance and Broader Research Context\n\nExpanding beyond traditional Large Language Models (LLMs) that only handle textual data, MLLMs integrate multiple input modalities such as images and text, thereby unlocking rich cross-modal reasoning abilities. This versatility makes MLLMs highly sought-after for real-world tasks including autonomous navigation, image captioning, and complex multimodal dialogue systems. However, the rise of fine-tuning-as-a-service (FTaaS) platforms, where users submit personalized datasets to adapt MLLMs without deep model access, creates a fertile ground for attackers to embed backdoors. These backdoors covertly hijack model outputs upon encountering specific triggers, while maintaining normal performance on clean inputs. Understanding and detecting such threats is essential for the safe deployment of MLLMs in critical applications, motivating the research to explore intrinsic signals within MLLM architectures for robust backdoor defense.\n\n---\n\n## Core Content\n\n### Defining Multimodal Large Language Models and Their Vulnerabilities\n\nMultimodal Large Language Models (MLLMs) can jointly process visual and linguistic data through combined pipelines of vision encoders and large-scale language models. For example, an image \\( x \\) is converted to visual tokens by a vision encoder \\( VE(\\cdot) \\), which are then fused with a text query \\( q \\) and input to a language model \\( LM_\\theta(\\cdot) \\) to produce an output. Formally, the training objective on a downstream task dataset \\( D_{train} = \\{(x_i, q_i, y_i)\\}_{i=1}^N \\) is to optimize parameters \\( \\theta \\) by minimizing an empirical loss:\n\n\\[\n\\min_{\\theta} \\mathbb{E}_{(x,q,y) \\sim D_{train}} \\mathcal{L} \\big( LM_\\theta (VE(x), q), y \\big),\n\\]\n\nwhere \\( \\mathcal{L} \\) denotes the task-specific loss function such as cross-entropy or CIDEr score for captioning (page 3).\n\nBackdoor attacks arise when a subset of training samples \\( D_{poison} = \\{(x_{trig,i}, q_i, y^\\dagger)\\}_{i=1}^K \\), with \\( K = r \\cdot N \\), is poisoned by embedding a trigger pattern into the image \\( x_{trig,i} \\). The model is then fine-tuned on the union \\( D_{train} \\cup D_{poison} \\), aiming to associate these triggers with attacker-chosen target outputs \\( y^\\dagger \\) (page 3). Such attacks are particularly threatening as the fine-tuned MLLM behaves normally on clean inputs but misbehaves only when the backdoor trigger is present.\n\n### Existing Challenges and Shortcomings of Prior Defenses\n\nTraditional backdoor defenses for unimodal architectures often rely on clean datasets, auxiliary labels, or explicit model access for trigger inversion or input transformations. However, these methods do not translate well to MLLMs due to their multimodal complexity and the black-box nature of FTaaS fine-tuning. Moreover, many defenses require human supervision or modification of the model, which is impractical in real deployment scenarios (page 2).\n\nThe research identifies a notable gap: the absence of effective, fully unsupervised defenses that can leverage inherent model signals to detect poisoned samples without clean data or model changes.\n\n### The Attention Collapse Phenomenon in MLLMs Under Backdoor Attacks\n\nMLLMs use Transformer-based attention mechanisms to perform cross-modal reasoning, determining which parts of an image and text to focus on during inference. The paper observes that backdoor triggers cause a unique disruption termed *attention collapse*: the model\'s spatial attention overwhelmingly concentrates on the trigger region, ignoring semantically relevant image areas (Figure 2, page 4).\n\nMathematically, for each layer \\( l \\) and attention head \\( h \\), the attention weights from the first decoding token to all image tokens are represented as \\( A_{l,h}(x,q) \\in \\mathbb{R}^{1 \\times T} \\), where \\( T \\) is the number of visual tokens (e.g., 576 for LLaVA). Averaging over \\( H \\) heads yields the layer-wise attention map\n\n\\[\n\\hat{A}^{(l)}(x,q) = \\frac{1}{H} \\sum_{h=1}^H A_{l,h}(x,q).\n\\]\n\nIn clean samples, \\( \\hat{A}^{(l)} \\) distributes broadly across meaningful image patches, reflecting coherent reasoning. Poisoned samples exhibit a sharp, localized attention spike on the trigger, indicating semantic misalignment (page 4).\n\n### Using Attention Entropy to Quantify Collapse\n\nTo quantitatively capture attention dispersion, the Shannon entropy \\( H^{(l)}(x,q) \\) of the normalized attention distribution is computed for each layer:\n\n\\[\nH^{(l)}(x,q) = - \\sum_{t=1}^T \\hat{A}_t^{(l)}(x,q) \\log \\hat{A}_t^{(l)}(x,q),\n\\]\n\nwhere \\( \\hat{A}_t^{(l)} \\) is the attention weight for image token \\( t \\) (page 5).\n\nClean inputs tend to have higher entropy due to more evenly distributed attention, while poisoned inputs have significantly lower entropy from collapsed attention. This bimodal entropy distribution enables unsupervised differentiation between clean and poisoned samples.\n\n---\n\n## Technical Details\n\n### The BYE Framework: Attention Entropy-Driven Backdoor Cleaning\n\nThe paper introduces **Believe Your Eyes (BYE)**, a three-stage pipeline to detect and remove backdoor-poisoned samples by analyzing attention entropy without requiring any clean references:\n\n1. **Attention Extraction (Section 4.1, page 5):**  \n   The MLLM is fine-tuned on the full (potentially poisoned) training set \\( D_{train} \\). Cross-modal attention maps \\( \\hat{A}^{(l)}(x,q) \\) are extracted for every sample and layer using Eq. (4).\n\n2. **Entropy Profiling and Sensitive Layer Selection (Section 4.2, page 6):**  \n   For each layer \\( l \\), attention entropy \\( H^{(l)}(x,q) \\) (Eq. (5)) is computed for all samples. These values form a set \\( \\{H^{(l)}(x_i,q_i)\\} \\). A Gaussian Mixture Model (GMM) with two components is fit to this set:\n\n   \\[\n   \\{H^{(l)}(x_i,q_i)\\} \\sim \\sum_{k=1}^2 \\pi_k \\mathcal{N}(\\mu_k, \\sigma_k^2),\n   \\]\n\n   capturing the bimodal nature (clean vs. poisoned). The *Bimodal Separation Index (BSI)* quantifies layer separability as\n\n   \\[\n   BSI(l) = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}.\n   \\]\n\n   Layers where \\( BSI(l) \\) exceeds a threshold \\( \\tau_{bsi} \\) are marked as sensitive \\( L_{sens} \\) (page 6).\n\n3. **Sample Filtering via Cross-Layer Entropy Aggregation (Section 4.3, page 6):**  \n   Sample-level entropy is aggregated over sensitive layers:\n\n   \\[\n   \\bar{H}(x,q) = \\frac{1}{|L_{sens}|} \\sum_{l \\in L_{sens}} H^{(l)}(x,q).\n   \\]\n\n   A GMM again clusters samples by \\( \\bar{H} \\), isolating those in the low-entropy cluster as suspicious. These poisoned samples are filtered out to form a clean set \\( D_{clean} \\), and the MLLM is re-fine-tuned on \\( D_{clean} \\) to obtain a robust model \\( M_{clean} \\).\n\nAlgorithm 1 on page 5 summarizes these steps:\n\n\`\`\`markdown\nAlgorithm 1: BYE - Attention Entropy-Driven Backdoor Cleaning\nInput: Training set D_train, MLLM M_\u03b8\nOutput: Cleaned dataset D_clean, robust model M_clean\n\n1. Fine-tune M_\u03b8 on D_train.\n2. Extract attention maps {\u00c2^{(l)}(x_i, q_i)} for all layers l and samples i.\n3. Compute entropy H^{(l)}(x_i, q_i) for each layer and sample.\n4. Fit GMM to H^{(l)} to find bimodal layers L_sens with BSI(l) \u2265 \u03c4_bsi.\n5. Aggregate entropy over L_sens:  \u0304H(x_i, q_i).\n6. Cluster samples by \u0304H with GMM and filter out low-entropy cluster.\n7. Re-fine-tune M_\u03b8 on filtered D_clean to get M_clean.\n\`\`\`\n\n### Design Decisions and Parameter Choices\n\n- Using **cross-modal attention** from the first decoding token ensures focus on the reasoning path relevant to output generation.\n- Entropy is preferred over raw attention values to robustly quantify dispersion and normalize across layers.\n- The BSI threshold \\( \\tau_{bsi} \\) is empirically selected to balance sensitivity and specificity, detailed in Appendix A.3.\n- Employing a **GMM** allows flexible, unsupervised modeling of bimodal distributions, applicable across tasks without supervision.\n- Layer-wise attention analysis leverages MLLM architecture characteristics, particularly the stability of early-to-mid decoder layers in encoding semantic information (page 4).\n\n---\n\n## Significance and Connections\n\nThis research introduces a novel, *self-supervised* approach to detect and mitigate backdoor poisoning in MLLMs by exploiting intrinsic attention behaviors rather than relying on external clean data or model alterations. Unlike prior defenses that mostly target unimodal models or require significant supervision, BYE leverages the universal transformer attention mechanism as a built-in security diagnostic.\n\nThe identification of *attention collapse* as a signature of backdoor triggers bridges the gap between interpretability and security in MLLMs, connecting research on model internal states with practical threats from malicious fine-tuning. By demonstrating robust performance across multiple MLLM architectures and vision-language tasks (Figures 2 and 3, Tables 1 and 2), this work paves the way for scalable, generalizable backdoor defenses in multimodal environments\u2014a critical step given the FTaaS paradigm\'s prevalence.\n\nFurthermore, this attention-entropy framework opens avenues for future research on internal model explainability and trustworthiness in large-scale multimodal AI systems, emphasizing the broader impact of carefully analyzing and leveraging model-intrinsic signals.\n\n---\n\nThis section integrates fundamental MLLM concepts, formalizes backdoor poisoning mathematically, and introduces an innovative attention-based detection mechanism, setting a foundational understanding for the research paper\u2019s subsequent methods and experiments.", "citations": ["https://www.moveworks.com/us/en/resources/ai-terms-glossary/multimodal-language-models0", "https://neptune.ai/blog/multimodal-large-language-models", "https://ctrl.carlow.edu/ai/whatare", "https://www.alexanderthamm.com/en/blog/an-introduction-to-large-multimodal-models/", "https://en.wikipedia.org/wiki/Large_language_model"], "page_number": 1, "subsections": [{"id": "research-problem-and-motivation", "title": "Research Problem and Motivation: Risks of Backdoor Attacks in FTaaS", "content": "## Research Problem and Motivation: Risks of Backdoor Attacks in FTaaS\n\nThis section delves into the critical security vulnerabilities inherent in the Fine-tuning-as-a-Service (FTaaS) paradigm, focusing on the risks posed by backdoor attacks in Multimodal Large Language Models (MLLMs). Understanding these risks is essential for appreciating the challenges the paper addresses and for contextualizing the proposed defense mechanisms. FTaaS enables users to adapt large pretrained MLLMs to specific downstream tasks using their own datasets, often without direct access to model parameters or architecture. While this service model democratizes AI customization, it simultaneously exposes models to malicious manipulation through poisoned fine-tuning data. This backdrop establishes the urgency and relevance of studying backdoor threats and mitigation strategies in MLLM fine-tuning, as the paper does.\n\n### Core Content: Defining and Exploring Backdoor Threats in FTaaS\n\n**Fine-tuning-as-a-Service (FTaaS)** offers a streamlined approach for users to tailor large pretrained MLLMs to specialized tasks by submitting task-specific datasets for downstream fine-tuning. The fine-tuning objective is commonly formulated as minimizing the empirical loss over the training set:\n\n$$\n\\min_{\\theta} \\mathbb{E}_{(x,q,y) \\sim D_{\\text{train}}} \\left[L(\\text{LM}_{\\theta}(\\text{VE}(x), q), y)\\right],\n$$\n\nwhere $x$ is the input image, $q$ is the textual query, $y$ is the output label, $\\text{VE}(\\cdot)$ is the vision encoder, $\\text{LM}_{\\theta}(\\cdot)$ is the language model parameterized by $\\theta$, and $L(\\cdot)$ is the loss function (page 3, Eq. 1).\n\nThe **backdoor attack** threat arises when an adversary poisons a subset of the fine-tuning dataset by embedding subtle, patch-based visual triggers into images. These triggers, often designed as imperceptible or semantically irrelevant patches, establish hidden associations between trigger-embedded inputs and attacker-chosen outputs. The poisoned subset is denoted as:\n\n$$\nD_{\\text{poison}} = \\{ (x_i^{\\text{trig}}, q_i, y^\\dagger) \\}_{i=1}^K,\n$$\n\nwhere $x_i^{\\text{trig}}$ represents the image with the embedded trigger, $y^\\dagger$ is the attacker-specified malicious output, and $K = r \\cdot N$ with $r$ being the poisoning ratio (page 3, Eq. 2). The model parameters $\\theta$ are fine-tuned on the combined dataset $D_{\\text{train}} \\cup D_{\\text{poison}}$.\n\nThe **unique risk in FTaaS** is that the model provider often has limited control over user-submitted datasets, making it a practical vector for attack. Furthermore, the black-box nature of FTaaS restricts defenders from inspecting model internals or gradients, while attackers enjoy model-agnostic, input-level capabilities to inject visual triggers (page 2, Fig. 1).\n\n**Patch-based backdoor triggers** are particularly threatening because:\n\n- They do not require gradient information to optimize, unlike adversarial perturbations.\n- They are model-agnostic and transferable across tasks and architectures.\n- They enable the compromised model to behave normally on clean inputs but produce attacker-chosen outputs when triggered, thus evading detection.\n\n**Attention collapse**, a novel phenomenon identified in this context, occurs when the MLLM\'s internal attention mechanism disproportionately focuses on the injected trigger rather than semantically relevant image regions. Visualization in Figure 2 (page 4) clearly illustrates this effect: clean images show broad spatial attention aligned with task-relevant content, whereas poisoned images exhibit sharply localized attention on the trigger patch.\n\nThis deviation implies that backdoors disrupt the MLLM\'s cross-modal semantic alignment\u2014fundamental to its reasoning ability\u2014leading to hijacked model outputs that reflect malicious control rather than task intent.\n\n### Technical Details: Mechanisms and Detection of Backdoor Attacks\n\nThe fine-tuning process in FTaaS can be formalized as:\n\n$$\n\\min_{\\theta} \\mathbb{E}_{(x,q,y) \\sim D_{\\text{train}} \\cup D_{\\text{poison}}} \\left[L(\\text{LM}_{\\theta}(\\text{VE}(x), q), y)\\right].\n$$\n\nThe model\'s **cross-modal attention maps**, particularly from the first decoding token to all image tokens at each Transformer layer, provide insight into the attention dynamics. For layer $l$ and head $h$, the attention vector is:\n\n$$\nA_{l,h}(x,q) \\in \\mathbb{R}^{1 \\times T},\n$$\n\nwhere $T$ is the number of image tokens. Averaging over $H$ attention heads yields:\n\n$$\n\\hat{A}^{(l)}(x,q) = \\frac{1}{H} \\sum_{h=1}^H A_{l,h}(x,q).\n$$\n\nThe **attention entropy** at layer $l$ quantifies the spread of attention across image tokens:\n\n$$\nH^{(l)}(x,q) = -\\sum_{t=1}^T \\hat{A}_t^{(l)}(x,q) \\log \\hat{A}_t^{(l)}(x,q).\n$$\n\nEmpirically, poisoned samples exhibit significantly lower attention entropy at certain sensitive layers, reflecting attention collapse (page 5, Eq. 5).\n\nTo robustly identify poisoned samples, the authors model the entropy distribution at each layer using a Gaussian Mixture Model (GMM) with two components representing clean and poisoned clusters:\n\n$$\n\\{H^{(l)}(x_i,q_i)\\}_{i=1}^N \\sim \\sum_{k=1}^2 \\pi_k \\mathcal{N}(\\mu_k, \\sigma_k^2),\n$$\n\nwhere $\\pi_k$ are mixture weights, and $\\mu_k$, $\\sigma_k^2$ are the means and variances of each Gaussian component (page 5, Eq. 6).\n\nThe **Bimodal Separation Index (BSI)** quantifies the layer\'s ability to discriminate poisoned from clean samples:\n\n$$\n\\text{BSI}(l) = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}},\n$$\n\nwith layers exceeding a threshold $\\tau_{\\text{bsi}}$ selected as sensitive (page 5, Eq. 7).\n\nFinally, attention entropies are aggregated over sensitive layers to form a per-sample entropy score:\n\n$$\n\\bar{H}(x,q) = \\frac{1}{|L_{\\text{sens}}|} \\sum_{l \\in L_{\\text{sens}}} H^{(l)}(x,q),\n$$\n\nand a GMM clustering on $\\{\\bar{H}(x_i,q_i)\\}$ separates poisoned from clean samples for filtering (page 5, Eq. 8).\n\nThis methodology grounds the detection of backdoor samples in intrinsic model signals, avoiding reliance on external clean data or labels.\n\n### Algorithmic Summary\n\n\`\`\`python\nAlgorithm 1: BYE - Attention Entropy-Driven Backdoor Cleaning\nInput: Training data D_train, target MLLM M_\u03b8\nOutput: Cleaned data D_clean, robust model M_clean\n\n1. Fine-tune M_\u03b8 on D_train\n2. For each sample (x_i, q_i):\n   a. Extract cross-modal attention maps {\u00c2^(l)(x_i, q_i)} for all layers l\n   b. Compute attention entropy H^(l)(x_i, q_i) for each layer\n3. For each layer l:\n   a. Fit two-component GMM on {H^(l)(x_i, q_i)}\n   b. Compute BSI(l) and select sensitive layers L_sens with BSI \u2265 \u03c4_bsi\n4. For each sample:\n   a. Aggregate entropies across L_sens: \\bar{H}(x_i, q_i)\n   b. Cluster { \\bar{H}(x_i, q_i) } by GMM; flag low-entropy cluster as poisoned\n5. Construct D_clean by removing flagged samples\n6. Re-fine-tune M_\u03b8 on D_clean to obtain M_clean\n\`\`\`\n\nThis unsupervised pipeline, detailed on page 5, effectively cleans poisoned data without external supervision.\n\n### Significance & Connections: Novelty and Field Impact\n\nThis research uniquely identifies **attention collapse** as a foundational phenomenon underpinning backdoor attacks in MLLMs fine-tuned under FTaaS settings\u2014a crucial insight linking the attack\'s effect to disruptions in internal attention dynamics (page 4, Figure 2). Leveraging this phenomenon, the proposed BYE framework introduces a **self-diagnostic, entropy-based detection method** that operates without requiring clean datasets, external labels, or model structural modifications. This contrasts with many prior defense approaches that depend on white-box access or auxiliary datasets (page 2).\n\nBYE\'s key innovations are:\n\n- Exploiting **cross-modal attention entropy patterns** as an intrinsic signal for poisoned sample detection.\n- Developing a principled, **unsupervised three-stage pipeline** combining attention extraction, bimodal entropy profiling, and clustering-based sample filtering.\n- Demonstrating strong generalization across multiple MLLMs and downstream tasks, validating a **model-internal defense mechanism** tailored to the unique multimodal and Transformer-based architectures (pages 6\u20137, Tables 1 and 2).\n\nThis approach advances the field by providing a **robust and scalable defense** suitable for real-world FTaaS scenarios where black-box constraints and untrusted data inputs are common. It aligns with broader efforts in secure machine learning to develop defenses that operate with minimal external assumptions and leverage model interpretability signals for threat detection.\n\nIn conclusion, understanding the risks of backdoor attacks in FTaaS and the role of attention collapse sets the stage for appreciating how BYE and similar defenses can safeguard MLLM deployments in increasingly critical and sensitive applications.\n\n---\n\nThis content builds from the foundational problem through technical exposition to research significance, integrating explanations, equations, algorithmic structure, and references to specific pages and figures to ensure comprehensive understanding.", "citations": ["https://neurips.cc/virtual/2024/poster/96865", "https://arxiv.org/html/2406.07778v2", "https://arxiv.org/html/2505.16916v1", "https://openreview.net/forum?id=1PcJ5Evta7", "https://www.themoonlight.io/en/review/mitigating-fine-tuning-based-jailbreak-attack-with-backdoor-enhanced-safety-alignment"], "page_number": 1}, {"id": "key-concepts-and-background-theory", "title": "Key Concepts and Background Theory: MLLMs and Attention Mechanisms", "content": "## Key Concepts and Background Theory: MLLMs and Attention Mechanisms\n\nThis section provides a comprehensive overview of Multimodal Large Language Models (MLLMs) and the attention mechanisms central to their function. Understanding these foundational concepts is critical for grasping the subsequent discussions in the paper, which investigates vulnerabilities of MLLMs to backdoor attacks and introduces defense mechanisms based on attention behavior analysis. This background will contextualize how MLLMs integrate and process diverse data types and the role of attention in their internal reasoning, enabling deeper insight into the proposed methodology.\n\nMLLMs represent a significant evolution in AI, extending traditional language models by integrating multiple data modalities\u2014primarily vision and language\u2014to perform complex reasoning tasks that require joint understanding of images and text. The attention mechanism, initially popularized by Transformer architectures, is the core computational tool enabling these models to focus on relevant input elements dynamically. Investigating how attention patterns change in response to maliciously perturbed inputs provides a promising direction for understanding and mitigating security risks during model fine-tuning, a central theme of this research.\n\n### Core Concepts of Multimodal Large Language Models (MLLMs)\n\n**Definition and Architecture**  \nAn MLLM combines multiple encoders specialized for different modalities\u2014such as a vision encoder for images and a language model for text\u2014with a fusion mechanism that aligns and integrates these modalities into a shared representation space. Formally, the vision encoder $VE(\\cdot)$ transforms an image $x$ into a sequence of visual tokens, while the language model $LM_\\theta(\\cdot)$ operates on text tokens and these visual tokens jointly.\n\nGiven an input image $x$ and a text query $q$, the MLLM outputs a response $y$ by minimizing a loss function $L$ over the training dataset $D_{train}$:\n\n$$\n\\min_{\\theta} \\mathbb{E}_{(x, q, y) \\sim D_{train}} \\left[ L\\big(LM_\\theta(VE(x), q), y \\big) \\right].\n$$\n\nThis formulation (as detailed on page 3, Eq. (1)) captures the core training objective, where the model learns to jointly understand and reason over paired visual and textual inputs.\n\n**Multimodal Alignment via Cross-Modal Attention**  \nThe fusion of visual and textual inputs is achieved through cross-modal attention layers within Transformer architectures. Attention computes the relevance of tokens across modalities, allowing the model to \"focus\" on specific parts of the image in the context of the posed text query. For a given attention layer $l$ with $H$ heads, the attention from the first decoding token (initiating answer generation) to all $T$ image tokens is represented as:\n\n$$\n\\hat{A}^{(l)}(x, q) = \\frac{1}{H} \\sum_{h=1}^H A^{(l,h)}(x, q),\n$$\n\nwhere $A^{(l,h)}(x, q) \\in \\mathbb{R}^{1 \\times T}$ is the attention distribution for head $h$ at layer $l$ (page 4, Eq. (4)). This averaged attention map $\\hat{A}^{(l)}$ reflects the spatial focus of the model at layer $l$ and plays a crucial role in visual grounding and reasoning.\n\n**Example: Vision-Language Reasoning**  \nConsider a visual question answering (VQA) task where the model is asked, \"What color is the car?\" The MLLM leverages cross-modal attention to allocate higher weights to image tokens representing the car, thereby aligning visual features with the linguistic query. Attention distributions thus reveal the model\u2019s interpretive focus, which ideally corresponds to semantically relevant regions.\n\n### Attention Mechanism Fundamentals\n\n**Transformer Attention Mechanics**  \nTransformers rely on the scaled dot-product attention mechanism. Given queries $Q$, keys $K$, and values $V$ derived from input embeddings, attention for head $h$ is computed as:\n\n$$\n\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V,\n$$\n\nwhere $d_k$ is the dimensionality of queries and keys. This operation scores the similarity between query and key vectors to weight value vectors appropriately, enabling adaptive focus across tokens.\n\nIn MLLMs, cross-modal attention layers compute attention weights between textual queries and visual tokens, effectively linking modalities. This is the backbone of the model\u2019s capacity to perform joint reasoning.\n\n**Attention Entropy as a Diagnostic Signal**  \nThe entropy of the attention distribution at layer $l$ for input $(x,q)$ is given by the Shannon entropy:\n\n$$\nH^{(l)}(x, q) = - \\sum_{t=1}^T \\hat{A}^{(l)}_t(x, q) \\log \\hat{A}^{(l)}_t(x, q),\n$$\n\nwhere $\\hat{A}^{(l)}_t$ is the attention weight for token $t$. High entropy indicates dispersed attention over many tokens, reflecting broad semantic grounding. Low entropy implies concentrated attention, which may signal abnormal focus, such as on a backdoor trigger (page 6, Eq. (5)).\n\n### Technical Details of Attention Collapse and Detection\n\n**Attention Collapse Phenomenon**  \nPage 4 and Figure 2 illustrate \"attention collapse,\" a phenomenon observed in poisoned MLLMs where attention becomes disproportionately concentrated on the backdoor trigger region, disregarding task-relevant image content. In clean samples, attention maps are spatially diverse and semantically meaningful, whereas poisoned inputs show sharply localized attention peaks on the trigger patch.\n\nThis collapse disrupts the normal cross-modal alignment, causing the model to produce incorrect or malicious outputs\u2014a fundamental insight motivating the detection mechanism.\n\n**Detection via Attention Entropy Profiling**  \nThe paper proposes a three-stage pipeline (detailed on pages 5-7, Algorithm 1) for detecting poisoned samples by:\n\n1. Extracting head-averaged cross-modal attention maps $\\hat{A}^{(l)}$ across all layers for each training sample.\n2. Computing layer-wise attention entropy $H^{(l)}(x, q)$ to measure attention dispersion.\n3. Selecting sensitive layers exhibiting bimodal entropy distributions between clean and poisoned samples using a Bimodal Separation Index (BSI), defined as:\n\n$$\n\\mathrm{BSI}(l) = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}},\n$$\n\nwhere $\\mu_k$ and $\\sigma_k^2$ are means and variances of the two Gaussian components fitted by a Gaussian Mixture Model (GMM) on the entropy values at layer $l$ (page 6, Eqs. (6)-(7)).\n\n4. Aggregating entropy scores $\\bar{H}(x,q)$ over selected layers to form a robust entropy descriptor per sample:\n\n$$\n\\bar{H}(x, q) = \\frac{1}{|L_{\\mathrm{sens}}|} \\sum_{l \\in L_{\\mathrm{sens}}} H^{(l)}(x, q),\n$$\n\nwhere $L_{\\mathrm{sens}}$ is the set of sensitive layers (page 6, Eq. (8)).\n\n5. Applying unsupervised clustering (GMM) on $\\bar{H}(x,q)$ across all samples to identify and filter out low-entropy (suspicious) samples.\n\n**Pseudocode of BYE Pipeline**\n\n\`\`\`python\nInput: D_train = {(x_i, q_i, y_i)}_{i=1}^N, model M_\u03b8\nOutput: Clean dataset D_clean, robust model M_clean\n\n# Step 1: Fine-tune model on D_train\nM_\u03b8 = fine_tune(M_\u03b8, D_train)\n\n# Step 2: Extract attention maps for each (x_i, q_i)\nfor (x_i, q_i) in D_train:\n    A_layer = {}\n    for l in layers:\n        A_layer[l] = average_heads_attention(M_\u03b8, x_i, q_i, layer=l)  # Eq. (4)\n\n# Step 3: Compute entropy H^(l)(x_i, q_i) for all layers and samples (Eq. 5)\nfor l in layers:\n    H_l = [compute_entropy(A_layer[l][i]) for i in range(N)]\n\n    # Fit GMM to {H_l}\n    gmm = fit_GMM(H_l, components=2)\n    BSI_l = compute_BSI(gmm)  # Eq. (7)\n\n# Step 4: Select sensitive layers L_sens with BSI above threshold\nL_sens = { l | BSI(l) >= \u03c4_bsi }\n\n# Step 5: Aggregate entropy across L_sens for each sample (Eq. 8)\nH_bar = [mean([H_layer[l][i] for l in L_sens]) for i in range(N)]\n\n# Step 6: Cluster H_bar with GMM to separate poisoned vs clean samples\ngmm_samples = fit_GMM(H_bar, components=2)\nsuspicious_samples = identify_low_entropy_cluster(gmm_samples)\n\n# Step 7: Filter suspicious samples and re-finetune model\nD_clean = D_train \\ suspicious_samples\nM_clean = fine_tune(M_\u03b8, D_clean)\n\nreturn D_clean, M_clean\n\`\`\`\n\n### Significance and Connections to Broader Research\n\nThe attention-driven detection approach presented is novel in leveraging an intrinsic model behavior\u2014cross-modal attention entropy\u2014as a self-supervisory signal for backdoor defense without requiring clean data or external supervision. This addresses a significant gap since prior methods relying on CNN saliency or input transformations fail to generalize to Transformer-based MLLMs, which dominate current multimodal AI research.\n\nBy linking the attention collapse phenomenon to security vulnerabilities, this work connects deep theoretical understanding of Transformer attention with practical defense mechanisms, marking an important advance in trustworthy MLLMs deployment. The methodology is broadly applicable across architectures and tasks, as demonstrated through experiments on models like LLaVA and InternVL (see Tables 1 and 2 on pages 7 and 8 and Figure 3 on page 9), highlighting its robustness and generality.\n\nThis research enriches the broader landscape of multimodal AI by emphasizing interpretability and security through internal model signals, potentially inspiring new directions in anomaly detection, robust training, and safer AI model fine-tuning.\n\n---\n\nThis section lays the foundation for understanding the intersection of multimodal model architecture, attention mechanisms, and security challenges addressed in the paper, preparing readers for the detailed experimental and algorithmic contributions that follow.", "citations": ["https://www.nvidia.com/en-us/glossary/multimodal-large-language-models/", "https://www.moveworks.com/us/en/resources/ai-terms-glossary/multimodal-language-models0", "https://determined.ai/blog/multimodal-llms", "https://ctrl.carlow.edu/ai/whatare", "https://neptune.ai/blog/multimodal-large-language-models"], "page_number": 2}, {"id": "related-work-and-positioning", "title": "Related Work and Positioning: Security of MLLMs and Backdoor Defenses", "content": "## Introduction\n\nThis section, **Related Work and Positioning: Security of MLLMs and Backdoor Defenses**, situates the paper\u2019s contributions within the broader landscape of research on the safety and robustness of Multimodal Large Language Models (MLLMs). The topic is crucial for understanding both the vulnerabilities these models face and the innovative solutions needed to secure them. Specifically, it addresses how emerging threats\u2014such as adversarial and prompt-based attacks, and particularly backdoor attacks\u2014are uniquely challenging in multimodal settings. By reviewing existing defenses and their limitations, the section sets the stage for the paper\u2019s novel unsupervised, model-internal defense strategy. This context is vital for readers to appreciate why new approaches are needed and how the proposed method stands out in the current research ecosystem, as illustrated in the paper\u2019s opening sections (pages 1\u20133).\n\n## Core Content\n\n### Key Concepts and Current Threats\n\nMultimodal Large Language Models (MLLMs) combine vision and language capabilities, enabling them to process and reason over complex, real-world data. However, this power comes with significant security risks. Recent research has highlighted two major classes of threats:\n\n- **Adversarial Attacks:** Subtle, carefully crafted perturbations to input images or text that can mislead the model into making incorrect or harmful predictions[4][1].\n- **Prompt-based Attacks:** Malicious instructions or prompts that exploit the model\u2019s ability to follow complex instructions, potentially leading to unintended or harmful outputs[3][5].\n- **Backdoor Attacks:** The embedding of hidden triggers in the training data, causing the model to behave normally on clean inputs but respond maliciously when specific triggers are present[4].\n\nThese threats are especially concerning in fine-tuning-as-a-service (FTaaS) settings, where users provide custom datasets, making it easy for adversaries to inject poisoned samples (pages 2\u20133).\n\n### Backdoor Defenses and Their Limitations\n\nExisting defenses against backdoors typically require access to clean data, labels, or model internals, and are often tailored for unimodal or CNN-based architectures. For example, some approaches rely on:\n\n- **Input Transformations:** Modifying or purifying input data to remove triggers.\n- **Trigger Inversion:** Inferring the structure of the trigger and removing it from the model.\n- **Model Surgery:** Modifying model parameters to erase malicious behaviors.\n\nHowever, as detailed in the paper, these methods are not well-suited for scalable deployment in MLLMs due to their multimodal nature and the constraints of real-world deployment scenarios (pages 2\u20133, 5). For instance, many defenses assume access to the model\u2019s parameters or gradients, which is unrealistic in black-box FTaaS environments.\n\n### The Unique Challenge for MLLMs\n\nMLLMs process both images and text, often using Transformer architectures with attention mechanisms that dynamically allocate focus across input features. Backdoor attacks in this setting can disrupt cross-modal reasoning, causing the model to focus excessively on adversarial triggers rather than relevant semantic content\u2014a phenomenon termed **attention collapse** (page 4, Figure 2).\n\nTo detect such attacks, the paper introduces a novel unsupervised strategy that exploits attention entropy as a self-diagnostic signal. This approach overcomes the limitations of existing defenses by not requiring clean data, auxiliary labels, or model modification.\n\n### Mathematical Formulation and Reasoning\n\nThe core idea is that poisoned samples cause abnormal concentration of attention on specific regions, resulting in low entropy in the attention distribution across image tokens. For a given input $(x, q)$, the model computes attention maps $\\hat{A}^{(l)}(x, q) \\in \\mathbb{R}^{1 \\times T}$ for each layer $l$, where $T$ is the number of image tokens. The attention entropy at layer $l$ is:\n\n$$\nH^{(l)}(x, q) = -\\sum_{t=1}^T \\hat{A}^{(l)}_t(x, q) \\log \\hat{A}^{(l)}_t(x, q)\n$$\n\nThis entropy quantifies the dispersion of attention: high entropy indicates diverse focus, while low entropy suggests collapsed attention, as detailed on page 6.\n\n### Examples and Illustrations\n\n**Example:** In Figure 2 (page 4), the top row shows the attention distribution for a clean image, where attention is broadly distributed. The bottom row illustrates a poisoned sample: attention collapses onto the trigger, ignoring semantically relevant regions. This visual evidence underpins the rationale for using attention entropy as a detection signal.\n\n**Analogy:** Think of attention entropy as a \u201cdiversity index\u201d for where the model is looking. When the model is healthy, it samples many regions, but when poisoned, it obsesses over a single spot\u2014like a student distracted by a single word, missing the rest of the page.\n\n## Technical Details\n\n### Implementation Pipeline\n\nThe proposed defense\u2014Believe Your Eyes (BYE)\u2014operates in three main stages, outlined below and in detail on pages 5\u20136:\n\n1. **Attention Extraction:** After fine-tuning the MLLM on the downstream dataset, extract cross-modal attention maps for each sample.\n2. **Entropy Profiling:** Compute the Shannon entropy for each attention map, aggregating across layers. Identify layers with clear bimodal separation in entropy values between clean and poisoned samples.\n3. **Unsupervised Cleaning:** Use Gaussian Mixture Model (GMM) clustering to separate samples with abnormally low entropy (likely poisoned) from those with normal entropy (likely clean), and filter out the former.\n\n\`\`\`python\n# Pseudocode for BYE (Algorithm 1, page 6)\nfor each sample (x_i, q_i) in D_train:\n    # Extract head-averaged attention maps for each layer\n    for each layer l:\n        A_hat[l] = average_attention(x_i, q_i, layer=l)\n        H[l] = compute_entropy(A_hat[l])\n    # Select sensitive layers with strong bimodal separation\n    L_sens = select_bimodal_layers(H)\n    # Aggregate entropy across sensitive layers\n    H_agg = average(H[l] for l in L_sens)\n    # Cluster samples by aggregated entropy\n    cluster = fit_gmm(H_agg)\n    if cluster == \'low\':\n        remove_sample(x_i, q_i)\n\`\`\`\n\n### Parameter Choices and Design Decisions\n\n- **Sensitive Layer Selection:** Only layers with high bimodal separation index (BSI) are used for aggregation, improving robustness and reducing noise (page 6).\n- **Gaussian Mixture Model (GMM):** Used for clustering entropy values because it can naturally model bimodal distributions and is robust to small sample sizes (page 6).\n- **No Clean Supervision:** The method does not require any clean data or labels, making it practical for real-world FTaaS scenarios (page 5).\n\n## Significance & Connections\n\n### Novelty and Broader Impact\n\nThe primary innovation of this work is the use of attention entropy as a self-supervised signal for backdoor detection, which is both practical and scalable for MLLMs. Unlike prior defenses, BYE does not require clean data, auxiliary labels, or white-box access, making it uniquely suited for deployment in realistic settings (pages 5\u20136).\n\nThe approach builds on the observation that backdoor triggers disrupt the model\u2019s internal attention dynamics, a phenomenon that can be quantified and exploited for robust defense. This is supported by empirical results across multiple datasets and MLLM architectures, as shown in Table 1 and Table 2 (pages 7\u20138), where BYE achieves near-zero attack success rates while maintaining clean performance.\n\n### Connecting to Broader Research\n\nThe work is closely related to growing concerns about the security of large-scale AI models, especially in multimodal and black-box scenarios[1][4]. It advances the field by providing a general, unsupervised solution for backdoor defense, and highlights the importance of model-internal signals as diagnostic tools. This method also complements other security strategies, such as prompt injection defenses and input sanitization, forming part of a comprehensive security framework for MLLMs (pages 2\u20133).\n\n### Implications for the Field\n\nThe implications are significant for both researchers and practitioners. By enabling robust backdoor detection without clean data or model access, BYE lowers the barriers to secure fine-tuning in real-world applications. This is especially important as MLLMs become more widely deployed in sensitive domains such as healthcare, autonomous systems, and public safety.\n\n---\n\n**Summary:**  \nThis section has provided a comprehensive overview of the security landscape for MLLMs, the limitations of existing backdoor defenses, and the novel contributions of the BYE framework. By leveraging attention entropy and unsupervised learning, the paper presents a scalable and practical approach to backdoor mitigation, advancing the state-of-the-art in secure AI deployment. For further details, readers are encouraged to explore the referenced figures, tables, and algorithmic descriptions in the original paper (pages 4\u20138).", "citations": ["https://arxiv.org/html/2402.00357v2", "https://arxiv.org/abs/2402.00357", "https://www.metomic.io/resource-centre/what-are-the-top-security-risks-of-using-large-language-models-llms", "https://www.ijcai.org/proceedings/2024/0901.pdf", "https://www.tigera.io/learn/guides/llm-security/"], "page_number": 2}]}, {"id": "methodology-and-approach", "title": "Methodology and Approach: Attention Collapse and the Believe Your Eyes Framework", "content": "## Methodology and Approach: Attention Collapse and the Believe Your Eyes Framework\n\nThis section elaborates on the foundational methodological contributions of the paper, focusing on the vulnerability of Multimodal Large Language Models (MLLMs) to backdoor attacks via poisoned fine-tuning data, and the novel solution proposed: the Believe Your Eyes (BYE) framework for detecting and removing poisoned samples. Understanding this section is crucial because it reveals a previously underexplored internal signal\u2014attention behavior\u2014that backdoor triggers distort, and demonstrates how this insight enables an unsupervised, model-intrinsic defense mechanism that requires no clean data or model changes. This fits into the broader research context of securing MLLMs in fine-tuning-as-a-service (FTaaS) environments, where safeguarding against malicious data injections is paramount for safe deployment.\n\n---\n\n### Core Concepts and Problem Formulation\n\n**Backdoor Threat in Downstream Fine-tuning**  \nMLLMs are fine-tuned with task-specific datasets \\( D_{\\text{train}} = \\{(x_i, q_i, y_i)\\}_{i=1}^N \\), where each sample consists of an image \\(x_i\\), a textual query \\(q_i\\), and a label \\(y_i\\). The model is optimized by minimizing the expected loss:  \n\\[\n\\min_\\theta \\mathbb{E}_{(x, q, y) \\sim D_{\\text{train}}} \\mathcal{L}(\\text{LM}_\\theta(\\text{VE}(x), q), y),\n\\]  \nwhere \\(\\text{VE}(\\cdot)\\) is the vision encoder, \\(\\text{LM}_\\theta(\\cdot)\\) is the language model with learnable parameters \\(\\theta\\), and \\(\\mathcal{L}\\) is the loss function.  \n\nIn a backdoor attack scenario, an adversary injects poisoned samples \\( D_{\\text{poison}} = \\{(x_i^{\\text{trig}}, q_i, y^\\dagger)\\}_{i=1}^K \\) into the fine-tuning data, where each image \\(x_i^{\\text{trig}}\\) contains a carefully crafted patch-based trigger (e.g., a small black square). These samples associate the trigger with attacker-chosen outputs \\(y^\\dagger\\), corrupting the model\'s behavior without affecting clean input performance. Fine-tuning is then conducted on \\( D_{\\text{train}} \\cup D_{\\text{poison}} \\):\n\\[\n\\min_\\theta \\mathbb{E}_{(x, q, y) \\sim D_{\\text{train}} \\cup D_{\\text{poison}}} \\mathcal{L}(\\text{LM}_\\theta(\\text{VE}(x), q), y).\n\\]\n\n---\n\n**Attention Mechanisms as Diagnostic Signals**  \nTransformers in MLLMs employ cross-modal attention to align visual and textual information. For a given input \\((x, q)\\), the attention from the first decoding token (initiating answer generation) to image tokens at layer \\(l\\) and head \\(h\\) is denoted \\( A_{l,h}(x, q) \\in \\mathbb{R}^{1 \\times T} \\), with \\(T\\) as the number of image patches. The average attention map across heads is:  \n\\[\n\\hat{A}^{(l)}(x, q) = \\frac{1}{H} \\sum_{h=1}^H A_{l,h}(x, q).\n\\]  \nThis spatial attention map \\( \\hat{A}^{(l)} \\) reflects which image regions the model focuses on during multimodal reasoning.\n\n**Attention Collapse Phenomenon**  \nThe authors discovered that backdoor triggers induce *attention collapse*: the model\u2019s attention disproportionately concentrates on the trigger patch, ignoring semantically relevant content elsewhere (Figure 2 on page 4 of the paper illustrates this stark contrast in attention maps between clean and poisoned samples). This attention hijacking disrupts the global semantic alignment, essential for accurate multimodal understanding, and manifests as a systematically abnormal attention pattern unique to poisoned inputs.\n\n---\n\n### The Believe Your Eyes (BYE) Framework\n\n**Leveraging Attention Entropy**  \nTo quantify attention collapse, BYE measures the *Shannon entropy* of the attention distribution at each layer \\(l\\), which captures the dispersion or concentration of attention across image tokens:  \n\\[\nH^{(l)}(x, q) = -\\sum_{t=1}^T \\hat{A}_t^{(l)}(x, q) \\log \\hat{A}_t^{(l)}(x, q).\n\\]  \nA high entropy indicates dispersed attention (typical for clean samples), while low entropy indicates concentrated attention (typical for poisoned samples focusing on a trigger).\n\n**Bimodal Entropy Distribution and Layer Selection**  \nEntropy scores across all samples show a bimodal distribution in certain layers, reflecting a natural separation between clean and poisoned samples. To formalize this, BYE fits a two-component Gaussian Mixture Model (GMM) to the entropy values \\( \\{H^{(l)}(x_i, q_i)\\}_{i=1}^N \\) at each layer:  \n\\[\np(H^{(l)}) = \\pi_1 \\mathcal{N}(\\mu_1, \\sigma_1^2) + \\pi_2 \\mathcal{N}(\\mu_2, \\sigma_2^2).\n\\]  \nThe *Bimodal Separation Index* (BSI) quantifies the normalized distance between the two Gaussian means:  \n\\[\n\\text{BSI}(l) = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}.\n\\]  \nLayers with \\( \\text{BSI}(l) \\geq \\tau_{\\text{bsi}} \\) are considered sensitive to attention collapse and are selected for further analysis (as detailed on page 6 of the paper).\n\n**Cross-Layer Entropy Aggregation and Clustering**  \nEntropy values are aggregated across sensitive layers \\( L_{\\text{sens}} \\) for each sample:  \n\\[\n\\bar{H}(x, q) = \\frac{1}{|L_{\\text{sens}}|} \\sum_{l \\in L_{\\text{sens}}} H^{(l)}(x, q).\n\\]  \nThis aggregated measure reduces noise and captures a robust signal of attention dispersion. BYE then applies GMM clustering again on the set \\( \\{\\bar{H}(x_i, q_i)\\}_{i=1}^N \\) to separate suspicious low-entropy (likely poisoned) samples from clean ones. Samples assigned to the low-entropy cluster are filtered out, resulting in a purified dataset \\( D_{\\text{clean}} \\).\n\n---\n\n### Implementation and Algorithmic Procedure\n\nFigure 3 on page 7 illustrates the clear bimodal separation in entropy scores, visually demonstrating the effectiveness of this signal to separate clean from poisoned data.\n\nThe BYE pipeline proceeds as follows (Algorithm 1, page 5):\n\n\`\`\`python\nInput: D_train = {(x_i, q_i, y_i)}_{i=1}^N, pretrained MLLM M_\u03b8\nOutput: Cleaned dataset D_clean, robustified model M_clean\n\n# Step 1: Fine-tune on poisoned dataset and extract attention\nM_\u03b8 <- Fine-tune on D_train\nFor each (x_i, q_i) in D_train:\n    For each layer l:\n        Compute head-averaged attention map: A_hat^(l)(x_i, q_i)\n\n# Step 2: Compute entropy and profile layers\nFor each layer l:\n    Compute entropy scores H^(l)(x_i, q_i) using Eq. (5)\n    Fit 2-component GMM to {H^(l)(x_i, q_i)}_{i=1}^N\n    Compute BSI(l) as distance between GMM means\nSelect layers L_sens with BSI(l) \u2265 \u03c4_bsi\n\n# Step 3: Aggregate entropies and cluster samples\nFor each sample (x_i, q_i):\n    Compute aggregated entropy: \u1e22(x_i, q_i) = mean_{l in L_sens} H^(l)(x_i, q_i)\nFit 2-component GMM to {\u1e22(x_i, q_i)}_{i=1}^N\nFilter out samples assigned to low-entropy cluster from D_train \u2192 D_clean\n\n# Step 4: Re-fine-tune model on purified data\nM_clean <- Fine-tune M_\u03b8 on D_clean\n\`\`\`\n\nThe threshold parameter \\( \\tau_{\\text{bsi}} \\) was empirically chosen to maximize the separation efficiency (Appendix A.3, page 15). Importantly, BYE does not require any external clean dataset or modifications to the model architecture, leveraging intrinsic attention signals alone.\n\n---\n\n### Significance and Research Impact\n\nBYE introduces a **novel unsupervised, attention-entropy-based approach** to detect backdoor poisoned samples in MLLM fine-tuning. This contrasts with prior defenses that rely on clean auxiliary data, supervised signals, or architectural changes, marking a significant advance in practical, scalable backdoor defense for complex multimodal systems.\n\nThe discovery of the *attention collapse* phenomenon itself is a meaningful contribution, revealing how backdoor triggers hijack the model\u2019s internal reasoning flow by distorting attention distributions (pages 3\u20135). This insight links the explainability of attention mechanisms to model security, demonstrating that internal model behaviors can serve as reliable diagnostics.\n\nBYE\u2019s framework generalizes across various MLLM architectures (e.g., LLaV A, InternVL) and multimodal tasks (Figure 1 and Tables 1 and 2 on pages 7\u20138), consistently reducing attack success rates while preserving clean performance. This demonstrates robust applicability in real-world FTaaS scenarios where datasets are black-box and poisoning risks are high.\n\nIn the broader research landscape, BYE advances the intersection of interpretability, security, and self-supervised learning for large multimodal models, encouraging future work to explore intrinsic model signals for detection and purification beyond backdoors, such as adversarial or distributional anomalies.\n\n---\n\nThis detailed exposition of attention collapse and BYE framework provides a comprehensive understanding of the methodology and rationale behind the paper\u2019s key contribution, equipping researchers and practitioners with both the theoretical and practical tools to tackle backdoor threats in MLLM fine-tuning.", "citations": ["https://arxiv.org/html/2505.16916", "https://aisecurity-portal.org/en/literature-database/backdoor-cleaning-without-external-guidance-in-mllm-fine-tuning/", "https://arxiv.org/abs/2505.16916", "https://huggingface.co/papers?q=attention+collapse", "https://www.ndss-symposium.org/wp-content/uploads/2024-238-paper.pdf"], "page_number": 3, "subsections": [{"id": "fine-tuning-and-backdoor-threats", "title": "Fine-tuning and Backdoor Threats in MLLMs", "content": "## Fine-tuning and Backdoor Threats in Multimodal Large Language Models (MLLMs)\n\nThis section delves into the dynamics of fine-tuning Multimodal Large Language Models (MLLMs) on task-specific datasets and explores the security risks posed by backdoor attacks during this process. Understanding these concepts is essential because downstream fine-tuning is a critical step in adapting large pre-trained models to diverse applications, but it also opens avenues for adversarial exploitation. Recognizing how backdoor triggers influence model behavior helps in developing defenses that ensure reliability and safety in real-world deployments.\n\nMLLMs combine vision and language modalities, typically integrating a vision encoder and a large language model to jointly process image-text pairs. Fine-tuning adapts these pretrained models to domain-specific tasks using datasets supplied by users or third parties. However, this flexibility is susceptible to poisoning attacks, where malicious actors embed hidden triggers into a fraction of training samples to manipulate model responses covertly. Such attacks are notably insidious in Fine-tuning-as-a-Service (FTaaS) settings, where providers may not fully control or verify the data submitted by users. This section lays the groundwork for analyzing how such poisoning manifests internally within MLLMs, particularly focusing on the attention mechanisms that are central to their multimodal reasoning.\n\n---\n\n### Core Concepts and Formulations\n\n**Fine-tuning Setup**\n\nConsider a pretrained MLLM, parameterized by $\\theta$, adapted to a specific downstream task using a dataset\n\n$$\n\\mathcal{D}_{\\text{train}} = \\{(x_i, q_i, y_i)\\}_{i=1}^N,\n$$\n\nwhere each sample consists of an image $x_i$, a textual query or prompt $q_i$, and the corresponding ground-truth answer or output $y_i$. The goal of fine-tuning is to optimize the model parameters to minimize the expected loss over this dataset:\n\n$$\n\\min_\\theta \\mathbb{E}_{(x,q,y) \\sim \\mathcal{D}_{\\text{train}}} \\mathcal{L} \\big( \\mathrm{LM}_\\theta(\\mathrm{VE}(x), q), y \\big),\n$$\n\nwhere $\\mathrm{VE}(\\cdot)$ denotes the vision encoder mapping the image into visual tokens, and $\\mathrm{LM}_\\theta(\\cdot)$ represents the multimodal language model generating the output based on encoded visual tokens and the query[1][p.3].\n\n**Backdoor Poisoning**\n\nIn a backdoor attack, an adversary poisons a subset of the fine-tuning samples by embedding a visual trigger\u2014often a small, patch-based pattern\u2014into the image. Formally, the poisoned dataset is\n\n$$\n\\mathcal{D}_{\\text{poison}} = \\{(x_i^{\\text{trig}}, q_i, y^\\dagger)\\}_{i=1}^K,\n$$\n\nwhere $x_i^{\\text{trig}}$ is the image $x_i$ with a trigger embedded, and $y^\\dagger$ is the attacker-chosen incorrect or targeted output. Typically, $K = r \\cdot N$ for some poisoning rate $r$[1][p.3].\n\nThe fine-tuning objective becomes\n\n$$\n\\min_\\theta \\mathbb{E}_{(x,q,y)\\sim \\mathcal{D}_{\\text{train}} \\cup \\mathcal{D}_{\\text{poison}}} \\mathcal{L}\\big(\\mathrm{LM}_\\theta(\\mathrm{VE}(x), q), y\\big),\n$$\n\nwhich causes the model to learn both the legitimate task and the hidden backdoor behavior.\n\n**Attention Mechanism and Attention Collapse**\n\nMLLMs rely heavily on Transformer-based attention to align visual and textual information. The core idea is to examine the cross-modal attention from the first decoding token (the token generating the answer) to all image tokens at each Transformer layer.\n\nFor layer $l$ and attention head $h$, the attention vector is denoted\n\n$$\nA^{(l,h)}(x, q) \\in \\mathbb{R}^{1 \\times T},\n$$\n\nwhere $T$ is the number of image tokens. The head-averaged attention map for layer $l$ is\n\n$$\n\\hat{A}^{(l)}(x, q) = \\frac{1}{H} \\sum_{h=1}^H A^{(l,h)}(x, q).\n$$\n\nVisualizing $\\hat{A}^{(l)}$ shows that for clean inputs, attention spreads over semantically meaningful regions, enabling correct reasoning. For poisoned inputs, however, attention increasingly collapses onto the trigger region across layers, a phenomenon termed *attention collapse* (Figure 2, p.4). This hijacking of the model\u2019s focus causes it to ignore relevant content and produce attacker-specified outputs instead[1][Fig. 2, p.4].\n\n**Quantifying Attention Collapse with Entropy**\n\nTo measure attention dispersion, the Shannon entropy of the attention vector at layer $l$ for input $(x,q)$ is computed as\n\n$$\nH^{(l)}(x, q) = - \\sum_{t=1}^T \\hat{A}_t^{(l)}(x, q) \\log \\hat{A}_t^{(l)}(x, q).\n$$\n\nLower entropy signals sharply focused attention (collapsed), while higher entropy indicates broader, more balanced attention. Empirically, poisoned samples show significantly lower entropy in certain layers, enabling discrimination between clean and poisoned inputs[1][Eq. (5), p.5].\n\n---\n\n### Technical Implementation Details\n\n**Attention Extraction**\n\nThe first step is fine-tuning the MLLM on the combined clean and possibly poisoned dataset $\\mathcal{D}_{\\text{train}} \\cup \\mathcal{D}_{\\text{poison}}$ using a method like LoRA for parameter-efficient tuning (p.5). After fine-tuning, attention maps $\\hat{A}^{(l)}(x_i, q_i)$ are extracted for all samples and layers.\n\n**Bimodal Entropy Profiling**\n\nBecause the entropy distribution at some layers is bimodal (with distinct modes for clean and poisoned samples), a Gaussian Mixture Model (GMM) with two components is fit to the entropy values $\\{H^{(l)}(x_i, q_i)\\}_{i=1}^N$ at each layer:\n\n$$\np(H^{(l)}(x, q)) = \\sum_{k=1}^2 \\pi_k \\mathcal{N}(\\mu_k, \\sigma_k^2).\n$$\n\nThe Bimodal Separation Index (BSI) is computed to quantify mode separability:\n\n$$\n\\mathrm{BSI}(l) = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}.\n$$\n\nLayers with $\\mathrm{BSI}(l) \\geq \\tau_{\\text{bsi}}$ are selected as sensitive for cleaning (p.5-6).\n\n**Entropy Aggregation and Sample Cleaning**\n\nFor each sample, average entropy over sensitive layers $L_{\\text{sens}}$ is calculated:\n\n$$\n\\bar{H}(x, q) = \\frac{1}{|L_{\\text{sens}}|} \\sum_{l \\in L_{\\text{sens}}} H^{(l)}(x, q).\n$$\n\nFitting another two-component GMM on $\\{\\bar{H}(x_i, q_i)\\}_{i=1}^N$ enables classification of samples into low-entropy (likely poisoned) and high-entropy clusters. Filtering out low-entropy samples yields a cleaned dataset $\\mathcal{D}_{\\text{clean}}$ used for re-fine-tuning, producing a robustified model $\\mathrm{M}_{\\text{clean}}$ (Algorithm 1, p.5-6).\n\n\`\`\`markdown\nAlgorithm 1: Believe Your Eyes (BYE)\nInput: Downstream dataset Dtrain, pretrained MLLM M\u03b8\nOutput: Clean dataset Dclean, robust model Mclean\n\n1. Fine-tune M\u03b8 on Dtrain.\n2. Extract head-averaged attention maps {A\u0302(l)(xi, qi)} from each layer l.\n3. Compute entropy H(l)(xi, qi) for each attention map.\n4. Fit a two-component GMM on H(l) to find bimodal layers Lsens by BSI threshold.\n5. Aggregate entropy over layers in Lsens: H\u0304(xi, qi).\n6. Fit GMM on H\u0304 to cluster samples.\n7. Filter out samples clustered into low-entropy group.\n8. Re-fine-tune M\u03b8 on filtered Dclean to obtain Mclean.\n\`\`\`\n\nThis approach operates fully unsupervised, does not require clean reference data or external annotations, and refrains from modifying model architectures\u2014key advantages in real-world deployments[1][Alg. 1, p.5-6].\n\n---\n\n### Significance & Broader Research Connections\n\nThe analysis of attention collapse reveals a novel internal signature of backdoor poisoning in MLLMs, bridging a crucial gap in understanding how adversarial triggers affect cross-modal reasoning. Unlike previous methods that rely on external supervision or model modifications, the entropy-based detection leverages *model-intrinsic signals*, marking a significant innovation in backdoor defense.\n\nThis method generalizes across tasks (e.g., visual question answering, image captioning) and architectures (LLaVA, InternVL), and it withstands diverse patch-based triggers with minimal impact on clean performance (Tables 1 and 2, p.7). Its purely self-supervised nature complements and advances prior work focused on unimodal CNNs or language-only models, addressing the unique challenges posed by MLLMs\u2019 multimodal attention mechanisms[1][p.7][Fig. 3, p.7].\n\nBy connecting the internal attention behavior to poisoning detection, this work opens new research avenues in secure model adaptation and highlights the importance of understanding fine-tuning dynamics in multimodal large models. It also situates itself within the emerging literature on backdoor attacks in transformer-based systems, including instruction-tuned LLMs, where backdoors similarly exploit fine-tuning vulnerabilities but have different operational characteristics[2][4].\n\n---\n\nThis comprehensive view of fine-tuning and backdoor threats elucidates the exploitation pathways and defense strategies for securing MLLMs, forming a foundational element for advancing trustworthy multimodal AI systems.", "citations": ["https://arxiv.org/html/2505.16916", "https://arxiv.org/html/2406.07778v2", "https://neurips.cc/virtual/2024/poster/96865", "https://openreview.net/forum?id=wZLWuFHxt5", "https://www.themoonlight.io/en/review/mitigating-fine-tuning-based-jailbreak-attack-with-backdoor-enhanced-safety-alignment"], "page_number": 3}, {"id": "attention-as-a-signal-for-trigger-localization", "title": "Attention as a Signal for Trigger Localization", "content": "## Attention as a Signal for Trigger Localization\n\nThis section delves into how attention mechanisms within Multimodal Large Language Models (MLLMs) reveal intrinsic signals that can be exploited to localize and detect backdoor triggers embedded during fine-tuning. Understanding this phenomenon is critical to grasp the novel defense mechanisms proposed in the paper for combating backdoor attacks without relying on external supervision or model modifications. By interpreting attention patterns\u2014particularly the collapse of attention around adversarial triggers\u2014this approach provides a powerful lens to expose and purify poisoned training data, thereby enhancing the security and reliability of MLLMs.\n\nThis topic fits into the broader research on securing MLLMs against backdoor threats that arise in fine-tuning-as-a-service (FTaaS) paradigms, where user-submitted datasets can maliciously alter model behavior. Unlike traditional CNN saliency methods, attention in Transformer-based MLLMs offers richer, semantically aligned cross-modal insights that are naturally suited for trigger localization, marking an important advance in self-supervised poisoning detection.\n\n---\n\n### Core Concepts and Methodology\n\n**Attention Mechanism in MLLMs**\n\nAt the heart of Transformer-based architectures underlying MLLMs is the attention mechanism, which dynamically assigns weights to different input tokens based on their relevance when generating outputs. In the multimodal setting, this involves cross-modal attention from language tokens (queries) to image tokens (keys/values), creating a spatial map of how the model distributes its focus over the visual input.\n\nMathematically, for an input image \\( x \\) and query \\( q \\), the vision encoder produces a set of \\( T \\) image tokens \\( \\{v_t\\}_{t=1}^T \\). The language decoder produces a first decoding token that queries these image tokens at each Transformer layer \\( l \\) and attention head \\( h \\). The cross-modal attention weights are denoted as\n\n\\[\nA_{l,h}(x, q) \\in \\mathbb{R}^{1 \\times T},\n\\]\n\nrepresenting the attention from the decoding token to all image tokens. To aggregate for each layer, the authors compute the head-averaged attention map \n\n\\[\n\\hat{A}^{(l)}(x, q) = \\frac{1}{H} \\sum_{h=1}^H A_{l,h}(x, q),\n\\]\n\nwhere \\( H \\) is the number of attention heads (Equation 4, p.4). This map reveals the spatial distribution of attention over the image patches at layer \\( l \\), creating a powerful tool for visualization and analysis.\n\n**Attention Collapse Phenomenon**\n\nIn clean inputs, attention broadly and semantically meaningfully covers relevant image regions, supporting robust cross-modal grounding. However, when poisoned samples with patch-based triggers are input, the model\u2019s attention shifts disproportionately and locally collapses around the trigger region. This \"attention collapse\" phenomenon\u2014distinct from conventional saliency concepts\u2014is a systemic disruption of attention that hijacks the model\u2019s visual focus and degrades semantic alignment (Figure 2, p.4).\n\nThis mechanism can be viewed formally: poisoned inputs cause the attention distribution \\( \\hat{A}^{(l)}(x, q) \\) to become overly peaked (low entropy), concentrating on a small subset of tokens corresponding to the trigger instead of spreading over task-relevant image tokens.\n\n**Quantifying Attention Collapse Using Entropy**\n\nTo measure this attention distribution sharpness, the authors compute the Shannon entropy \\( H^{(l)}(x,q) \\) of the normalized attention vector at each layer:\n\n\\[\nH^{(l)}(x, q) = - \\sum_{t=1}^T \\hat{A}_t^{(l)}(x, q) \\log \\hat{A}_t^{(l)}(x, q).\n\\]\n\nHigh entropy corresponds to dispersed and semantically meaningful attention, typical in clean inputs; low entropy indicates attention collapse around triggers (Equation 5, p.5).\n\nEmpirically, the entropy values show a bimodal distribution over training samples at certain layers, enabling separation of poisoned vs. clean samples. This bimodality is modeled using a two-component Gaussian Mixture Model (GMM):\n\n\\[\n\\{H^{(l)}(x_i, q_i)\\}_{i=1}^N \\sim \\sum_{k=1}^2 \\pi_k \\mathcal{N}(\\mu_k, \\sigma_k^2),\n\\]\n\nwhere \\( \\pi_k \\) are mixing coefficients, and \\( \\mu_k, \\sigma_k \\) are mean and variance of the two modes (Equation 6, p.5).\n\n**Selecting Sensitive Layers with the Bimodal Separation Index (BSI)**\n\nNot all layers reveal this entropy bimodality equally. To identify diagnostic layers most sensitive to attention collapse, the authors define a Bimodal Separation Index (BSI):\n\n\\[\n\\text{BSI}(l) = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}},\n\\]\n\nwhere \\( \\mu_1, \\mu_2 \\) are means of the two GMM components (Equation 7, p.5). Layers with \\( \\text{BSI}(l) \\) above a threshold \\( \\tau_{bsi} \\) are selected to form a set \\( L_{sens} \\) of sensitive layers for further analysis.\n\n**Aggregating Cross-Layer Entropy for Robust Detection**\n\nTo robustly characterize each input, entropy scores are averaged across sensitive layers:\n\n\\[\n\\bar{H}(x, q) = \\frac{1}{|L_{sens}|} \\sum_{l \\in L_{sens}} H^{(l)}(x, q).\n\\]\n\nThis cross-layer aggregation reduces noise and captures a holistic signature of attention collapse (Equation 8, p.6).\n\nFinally, a GMM clustering separates inputs into low-entropy (likely poisoned) and high-entropy (clean) groups, enabling unsupervised backdoor sample filtering and clean data recovery.\n\n---\n\n### Technical Implementation Details\n\n**Attention Extraction Procedure**\n\nAfter fine-tuning the MLLM on the (potentially poisoned) downstream dataset \\( \\mathcal{D}_{train} \\), attention maps \\( \\hat{A}^{(l)}(x_i, q_i) \\) are extracted for all \\( L \\) Transformer decoder layers for each sample \\( (x_i, q_i) \\) (Section 4.1, p.5). Only attention from the first decoding token to all image tokens is used, focusing on cross-modal semantic grounding.\n\nThis procedure benefits from the architectural design of models like LLaVA-v1.5, which maintain fixed image tokenization without downsampling, allowing precise spatial correspondence (p.4).\n\n**Entropy Profiling and Layer Selection**\n\nFor each layer, entropy values \\( H^{(l)}(x_i, q_i) \\) across all \\( N \\) samples are fitted with a two-component GMM (Equation 6). The BSI metric ranks layers by their entropy bimodality strength, and a threshold \\( \\tau_{bsi} \\) is chosen empirically (Appendix A.3) to select the sensitive layers \\( L_{sens} \\) (Section 4.2, p.5).\n\nThis bimodal separation is key to distinguishing poisoned samples, as validated by visualizations in Figure 3 (p.7), showing clean and poisoned clusters by entropy.\n\n**Sample Cleaning Algorithm**\n\nThe sample-level average entropy \\( \\bar{H}(x_i,q_i) \\) over sensitive layers forms the input to another GMM clustering that splits data into clean and suspicious low-entropy clusters. Filtering out low-entropy samples yields a purified dataset \\( \\mathcal{D}_{clean} \\) used for refined fine-tuning, producing a robustified model \\( M_{clean} \\) (Section 4.3, p.6).\n\n\`\`\`python\n# Algorithm 1: Believe Your Eyes (BYE) \u2014 Summary pseudocode\nInput: Training set D_train, MLLM M_theta\nOutput: Clean data D_clean, robust model M_clean\n\n1. Fine-tune M_theta on D_train\n2. For each (x_i, q_i) in D_train:\n     Extract cross-modal attention maps {A^(l)(x_i, q_i)} for all layers l=1..L\n3. For each layer l:\n     Compute entropy H^(l)(x_i, q_i) for all samples\n     Fit two-component GMM on {H^(l)(x_i, q_i)}\n     Calculate BSI(l)\n4. Select sensitive layers L_sens = {l | BSI(l) >= tau_bsi}\n5. For each sample (x_i, q_i):\n     Compute aggregated entropy bar_H(x_i, q_i) over L_sens\n6. Fit two-component GMM to {bar_H(x_i, q_i)}\n7. Remove samples classified as low-entropy (suspicious)\n8. Fine-tune M_theta again on filtered data D_clean to obtain M_clean\n\`\`\`\n\nThis pipeline requires no external clean data or labels and is model-intrinsic, relying solely on internal attention dynamics for diagnosis (p.5-6).\n\n---\n\n### Significance and Broader Context\n\nThe discovery that poisoned inputs induce \"attention collapse\" in MLLMs offers a fundamentally new insight into backdoor attack manifestations. Unlike CNN saliency-based approaches (e.g., SentiNet), which rely on localized features and cannot generalize well to Transformers, attention maps provide rich, semantically aligned signals suitable for multimodal models.\n\nThe BYE framework exploits this internal diagnostic to achieve unsupervised backdoor purification, marking a novel shift from external supervision toward self-supervised, model-internal signals. This paradigm improves detection robustness across varied triggers and architectures without performance degradation (Tab. 1 and 2, p.7-8).\n\nThis approach connects to the broader research on Transformer interpretability, attention-based explainability, and security in multimodal AI systems. It also opens new avenues for applying attention entropy analysis to other adversarial or robustness challenges.\n\nKey innovations include:\n\n- Systematic quantification of attention collapse via entropy and bimodal modeling.\n- Cross-layer entropy aggregation for stable sample-level backdoor detection.\n- A fully unsupervised pipeline that works without clean data or model changes.\n\nBy leveraging the intrinsic behaviors of MLLMs, BYE sets a new standard for reliable and efficient backdoor defense in fine-tuning-as-a-service scenarios, significantly advancing the security and trustworthiness of MLLMs in real-world applications.\n\n---\n\nThis comprehensive explanation outlines how attention serves as a natural, powerful signal to localize triggers in MLLMs, enabling effective and unsupervised detection and mitigation of backdoor attacks. The technical rigor, clear mathematical modeling, and systematic validation presented in the paper firmly establish attention entropy as a game-changing diagnostic tool in multimodal AI security.", "citations": ["https://arxiv.org/html/2411.08466v1", "https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition", "https://arxiv.org/html/2505.16916", "https://en.wikipedia.org/wiki/Attention_Is_All_You_Need", "https://openreview.net/forum?id=zGb4WgCW5i"], "page_number": 4}, {"id": "belief-your-eyes-by-entropy-driven-cleaning", "title": "Believe Your Eyes (BYE): Attention Entropy-Driven Backdoor Cleaning", "content": "## Introduction\n\nThis section explores **Believe Your Eyes (BYE): Attention Entropy-Driven Backdoor Cleaning**, a novel approach for detecting and removing poisoned (backdoor) samples from fine-tuning datasets for Multimodal Large Language Models (MLLMs). Understanding this method is critical because it addresses a growing security risk: attackers can inject malicious triggers into training data, causing the model to behave unpredictably or maliciously when encountering certain inputs during inference[2][1]. BYE does not require external supervision, clean reference data, or modifications to the model\u2014making it practical for real-world deployment where such resources are unavailable or impractical.\n\nThe importance of BYE lies in its focus on the *intrinsic* behaviors of MLLMs. Specifically, it exploits the phenomenon of **attention collapse**, where poisoned samples cause the model to focus on non-semantic, trigger-related regions in the input image, disrupting the normal cross-modal processing. This section fits into the broader research context by offering a robust, unsupervised solution for backdoor defense in the era of fine-tuning-as-a-service (FTaaS), where user-provided data may contain hidden threats[2][1].\n\n## Core Content\n\n### Key Concepts and Definitions\n\n**Attention Entropy:**  \nIn the context of MLLMs, *attention entropy* quantifies the dispersion or randomness in the model\u2019s attention across different parts of an image for a given input. Higher entropy means the attention is spread over more regions, reflecting normal, semantically-grounded processing. Lower entropy indicates that attention is concentrated on a specific, often non-semantic, region\u2014suggesting the presence of a trigger or \u201cattention collapse\u201d[1][4].\n\n**Attention Collapse:**  \nThis is the phenomenon where a poisoned sample causes the model to focus almost exclusively on the trigger region, disregarding the rest of the input. Figure 2 in the paper shows a stark contrast between the attention maps for clean and poisoned images, visually demonstrating attention collapse[2][1] (p. 5). The model\u2019s focus on the trigger effectively hijacks its reasoning, leading to incorrect or malicious outputs.\n\n**Cross-Modal Attention:**  \nMLLMs process both images and text by aligning (attending) to relevant visual regions given a prompt. Cross-modal attention is the mechanism by which the model relates image patches (visual tokens) to language queries, and is central to multimodal reasoning[2][1].\n\n### Mathematical Foundations\n\nThe core of BYE is the calculation and analysis of attention entropy. For each input $(x, q)$, the head-averaged attention map at layer $l$ is calculated as:\n\n$$\n\\hat{A}^{(l)}(x, q) = \\frac{1}{H} \\sum_{h=1}^H A_{l,h}(x, q)\n$$\n\nwhere $H$ is the number of attention heads, and $A_{l,h}$ is the attention distribution from the first decoding token to all image tokens at head $h$ and layer $l$ (Eq. 4, p. 5)[1]. The attention entropy for layer $l$ is then:\n\n$$\nH^{(l)}(x, q) = -\\sum_{t=1}^T \\hat{A}_t^{(l)}(x, q) \\log \\hat{A}_t^{(l)}(x, q)\n$$\n\nwhere $T$ is the number of image tokens (Eq. 5, p. 6)[1]. This entropy measures how \u201cconcentrated\u201d or \u201cdispersed\u201d the model\u2019s attention is across the image for a given query.\n\n### Bimodal Separation and Layer Selection\n\nBYE observes that at some layers, the entropy distribution is *bimodal*\u2014clean samples have higher entropy, while poisoned samples have lower entropy due to attention collapse. To quantify the separability between these modes, the method uses a **Bimodal Separation Index (BSI)** for each layer:\n\n$$\n\\text{BSI}^{(l)} = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}\n$$\n\nwhere $\\mu_1, \\mu_2$ and $\\sigma_1, \\sigma_2$ are the means and standard deviations of the two Gaussian components in the GMM fit to the layer\u2019s entropy values (Eq. 7, p. 6)[1]. Layers with a BSI above a threshold $\\tau_{\\text{bsi}}$ are selected as \u201csensitive\u201d and used for subsequent analysis.\n\n### Example and Intuition\n\nConsider a scenario where the model is trained to answer questions about images. A poisoned sample injects a small black square (trigger) into the image. During inference, the model\u2019s attention will collapse around the trigger, ignoring other salient regions. This results in abnormally low attention entropy at sensitive layers. Figure 3 in the paper illustrates the clear separation between clean and poisoned samples based on their attention entropy scores[2][1] (p. 7).\n\n## Technical Details\n\n### Pipeline Overview\n\nThe BYE pipeline consists of three main stages (Algorithm 1, pp. 5\u20136)[1]:\n\n1. **Attention Extraction:**  \n   - Fine-tune the model on the training data.\n   - Extract head-averaged cross-modal attention maps for each sample and layer.\n\n2. **Entropy Profiling and Layer Selection:**  \n   - Compute attention entropy for each layer and sample.\n   - Fit a two-component Gaussian Mixture Model (GMM) to the entropy distribution at each layer.\n   - Select layers with significant BSI for downstream analysis.\n\n3. **Sample Cleaning:**  \n   - Aggregate entropy scores across sensitive layers for each sample.\n   - Cluster samples using GMM to identify those with abnormally low aggregated entropy.\n   - Filter out suspicious (low-entropy) samples, resulting in a purified dataset.\n\n\`\`\`plaintext\nAlgorithm 1 (simplified from paper, p. 6):\n\nInput: D_train = {(x_i, q_i, y_i)} (training set), M\u03b8 (target MLLM)\nOutput: D_clean (cleaned dataset), M_clean (robustified model)\n\n1. Fine-tune M\u03b8 on D_train\n2. for each (x_i, q_i) in D_train:\n    Extract {A^(l)(x_i, q_i)} for all layers l\n3. for each layer l:\n    Compute H^(l)(x_i, q_i) (attention entropy)\n    Fit GMM to {H^(l)(x_i, q_i)} and compute BSI(l)\n    Select sensitive layers if BSI(l) \u2265 \u03c4_bsi\n4. for each (x_i, q_i):\n    Compute average entropy across sensitive layers\n    Cluster samples using GMM to identify low-entropy cluster\n    Remove samples in low-entropy cluster\n5. Fine-tune M\u03b8 on D_clean \u2192 M_clean\n\`\`\`\n\n### Design Choices and Parameters\n\n- **Unsupervised Operation:** BYE does not require clean data or auxiliary labels, making it highly practical for real-world scenarios where such resources are unavailable[2][1].\n- **Layer Selection:** Only layers with significant separation (high BSI) between clean and poisoned entropy modes are used, reducing noise and improving robustness.\n- **Filtering Strategy:** Using GMM clustering allows for flexible and robust identification of poisoned samples based on their intrinsic attention patterns.\n\n## Significance & Connections\n\n### Innovations and Contributions\n\nBYE introduces several key innovations:\n- **Self-Supervised Cleaning:** Unlike prior methods that require clean data or external supervision, BYE leverages the model\u2019s own attention patterns to detect anomalies[2][3].\n- **Model-Intrinsic Signals:** By focusing on entropy in cross-modal attention, BYE is robust against diverse backdoor triggers and model architectures[2][1].\n- **Practical Applicability:** The unsupervised nature makes BYE suitable for deployment in FTaaS settings, where user data may be untrusted and auxiliary resources are limited[2][1].\n\n### Connections to Broader Research\n\nBYE addresses a critical gap in the backdoor defense literature for MLLMs. While prior work focused on unimodal models or required external supervision, BYE shows that attention entropy in MLLMs provides a reliable, intrinsic signal for detecting poisoned samples. This connects to broader trends in self-supervised learning and model interpretability, and sets a new standard for robust, unsupervised backdoor defense in multimodal AI systems[2][1].\n\n### Implications for the Field\n\nThe success of BYE suggests that model-internal attention mechanisms can serve as a built-in \u201cimmune system\u201d for detecting and removing malicious data. This has profound implications for the secure deployment of MLLMs in sensitive applications, from healthcare to autonomous systems, where trust and reliability are paramount[2][1].\n\n---\n\n**In summary, the BYE framework provides a principled, unsupervised approach to backdoor cleaning in MLLMs, leveraging attention entropy as a self-supervised signal. Its effectiveness and generality are validated by extensive experiments and visualizations, making it a valuable contribution to the field of AI security and robustness.**", "citations": ["https://arxiv.org/html/2505.16916", "https://aisecurity-portal.org/en/literature-database/backdoor-cleaning-without-external-guidance-in-mllm-fine-tuning/", "https://huggingface.co/papers?q=attention+collapse", "https://www.themoonlight.io/es/review/backdoor-cleaning-without-external-guidance-in-mllm-fine-tuning", "https://openaccess.thecvf.com/content/WACV2024/papers/Subramanya_A_Closer_Look_at_Robustness_of_Vision_Transformers_to_Backdoor_WACV_2024_paper.pdf"], "page_number": 5}]}, {"id": "results-and-analysis", "title": "Results and Analysis: Experimental Validation of BYE", "content": "Below is a comprehensive, educational breakdown of the \u201cResults and Analysis: Experimental Validation of BYE\u201d section, tailored for advanced researchers and graduate students. The content is structured for clarity and technical depth, referencing key figures, tables, and mathematical derivations as in the original paper.\n\n---\n\n## Introduction\n\nThis section systematically validates the effectiveness of the BYE (Believe Your Eyes) framework for detecting and filtering backdoor-poisoned samples in multimodal large language models (MLLMs) during fine-tuning. It covers the experimental setup\u2014including models, datasets, and evaluation metrics\u2014empirical results, and ablation studies, providing a holistic view of BYE\u2019s strengths. Understanding this validation is crucial for two reasons:\n\n1. **Security Assurance:** As MLLMs are increasingly deployed in real-world applications, ensuring their robustness against malicious data perturbations is paramount. BYE addresses a critical vulnerability: backdoor attacks during fine-tuning, which can lead to biased or malicious model behavior[arXiv:2505.16916v1, Sec. 1].\n2. **Methodological Transparency:** The experimental protocol illustrates how attention entropy serves as a self-supervised signal for spotting abnormal model focus, highlighting the broader relevance of model-internal diagnostics for AI safety.\n\nThis section connects to earlier discussions of backdoor threats and BYE\u2019s motivation, and sets the stage for drawing conclusions about its practical utility.\n\n---\n\n## Core Content\n\n### Experimental Framework and Datasets\n\nThe validation process involves two state-of-the-art MLLMs: LLaVA-v1.5-7B and InternVL2.5-8B. Both models are fine-tuned with LoRA (Low-Rank Adaptation) on four diverse datasets spanning visual question answering (ScienceQA, IconQA, RSVQA) and image captioning (Flickr30k). Poisoned samples are created by embedding a black square trigger in 10% of training images, each associated with a predefined malicious output (e.g., \u201cBackdoor Attack!\u201d), simulating a realistic threat model[arXiv:2505.16916v1, Sec. 5.1].\n\n**Evaluation Metrics:**\n- **Clean Performance (CP):** Measures model accuracy or CIDEr score on unmodified test samples.\n- **Attack Success Rate (ASR):** Proportion of triggered inputs that produce the target malicious output.\n- **Poisoned Sample Detection:** Precision, recall, and F1 score for identifying poisoned samples in the training set.\n\nThese metrics together provide a comprehensive assessment of both model utility and defense effectiveness.\n\n### Key Concepts and Mathematical Formulations\n\n**Attention Entropy:**  \nBYE leverages Shannon entropy computed from cross-modal attention maps to detect poisoned samples. For a given image-question pair $(x, q)$ and layer $l$, the normalized attention vector $\\hat{A}^{(l)}(x, q)$ has entropy:\n\n$$\nH^{(l)}(x, q) = -\\sum_{t=1}^T \\hat{A}^{(l)}_t(x, q) \\log \\hat{A}^{(l)}_t(x, q)\n$$\n\nHere, $T$ is the number of image tokens, and $H^{(l)}(x, q)$ quantifies the dispersion of attention across the image. Poisoned samples tend to have sharply concentrated (low-entropy) attention, while clean samples exhibit more dispersed (high-entropy) attention[arXiv:2505.16916v1, Sec. 4.2].\n\n**Bimodal Entropy Profiling:**  \nThe distribution of attention entropy often shows two distinct modes. This is modeled using a Gaussian Mixture Model (GMM):\n\n$$\n\\{H^{(l)}(x_i, q_i)\\}_{i=1}^N \\sim \\sum_{k=1}^2 \\pi_k \\mathcal{N}(\\mu_k, \\sigma_k^2)\n$$\n\nTo quantify the separation between the two clusters, the Bimodal Separation Index (BSI) is defined as:\n\n$$\n\\mathrm{BSI}^{(l)} = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}\n$$\n\nLayers with BSI above a threshold $\\tau_{\\mathrm{bsi}}$ are selected as sensitive and aggregated for final sample filtering[arXiv:2505.16916v1, Sec. 4.2; p. 6].\n\n**Aggregated Entropy:**  \nTo consolidate layer-wise signals, the entropy is averaged across sensitive layers:\n\n$$\n\\bar{H}(x, q) = \\frac{1}{|L_{\\mathrm{sens}}|} \\sum_{l \\in L_{\\mathrm{sens}}} H^{(l)}(x, q)\n$$\n\nSamples with low $\\bar{H}(x, q)$ are flagged as suspicious and removed[arXiv:2505.16916v1, Sec. 4.3; p. 7].\n\n### Empirical Results\n\n**Reduction in Attack Success Rate (ASR):**  \nBYE consistently reduces ASR to near-zero levels while maintaining high clean performance (CP) across both models and all datasets. For example, on the RSVQA dataset with InternVL, BYE reduces ASR to 7.18% (from the baseline\u2019s 99.76%) while keeping CP at 66.09%, outperforming baselines like Random Drop and ZIP, which either sacrifice CP or fail to adequately suppress ASR (see Table 1)[arXiv:2505.16916v1, Sec. 5.2; p. 7-8].\n\n**Precision and Recall of Detection:**  \nBYE achieves high precision and recall for poisoned sample detection, with both LLaVA and InternVL showing greater than 99% precision and recall on the RSVQA dataset (see Table 2)[arXiv:2505.16916v1, Sec. 5.2; p. 8].\n\n**Visualization of Attention Entropy:**  \nFigure 3 illustrates a clear bimodal separation between clean and poisoned samples based on aggregated entropy scores. Clean samples cluster in high-entropy regions, while poisoned samples cluster in low-entropy regions, reflecting their abnormal attention collapse[arXiv:2505.16916v1, Sec. 5.3; p. 9].\n\n---\n\n## Technical Details\n\n### BYE Algorithm Overview\n\nThe BYE pipeline is summarized in Algorithm 1 and consists of three main stages:\n\n1. **Attention Extraction:**  \n   After fine-tuning the MLLM, cross-modal attention maps are extracted for each training sample $(x_i, q_i)$ across all Transformer layers.\n2. **Entropy Profiling and Layer Selection:**  \n   For each layer, the Shannon entropy of the attention distribution is computed. Layers with strong bimodal separation (high BSI) are selected as sensitive.\n3. **Sample Cleaning:**  \n   Entropy scores are aggregated across sensitive layers, and a GMM is used to cluster samples. Those in the low-entropy cluster are removed, yielding a purified training set[arXiv:2505.16916v1, Sec. 4; p. 5-7].\n\n**Pseudocode Sketch:**\n\n\`\`\`python\n# 1. Fine-tune and extract attention\nattention_maps = fine_tune_and_extract_attention(model, D_train)\n\n# 2. Compute entropy and select sensitive layers\nlayer_entropy = compute_entropy(attention_maps)\nbsi = compute_bimodal_separation_index(layer_entropy)\nsensitive_layers = select_layers_by_bsi(bsi, threshold=tau_bsi)\n\n# 3. Aggregate entropy and cluster\naggregated_entropy = average_entropy(layer_entropy, sensitive_layers)\ncluster_assignments = gmm_clustering(aggregated_entropy)\nD_clean = filter_samples(D_train, cluster_assignments, label=\'high_entropy\')\n\`\`\`\n\n### Parameter Choices and Design Decisions\n\n- **Sensitive Layer Selection:**  \n  The BSI threshold $\\tau_{\\mathrm{bsi}}$ is chosen empirically to maximize the separation between clean and poisoned samples, as detailed in Appendix A.3[arXiv:2505.16916v1, Sec. 4.2; p. 6].\n- **Aggregation Strategy:**  \n  Aggregating over multiple sensitive layers reduces noise and increases robustness to single-layer anomalies.\n- **Unsupervised Approach:**  \n  BYE requires no clean reference data, auxiliary labels, or model modifications, making it practical for real-world deployment.\n\n---\n\n## Significance and Connections\n\n### Novelty and Importance\n\nBYE introduces several key innovations:\n- **Model-Intrinsic Signal:** Leverages attention entropy, a property naturally arising from model behavior, as a diagnostic tool, avoiding reliance on external supervision[arXiv:2505.16916v1, Sec. 4; p. 5-7].\n- **Generalization Across Models and Tasks:** Validated on multiple MLLMs and diverse datasets, BYE demonstrates broad applicability and robustness against various attack patterns[arXiv:2505.16916v1, Sec. 5.2; p. 7-8].\n- **Robustness to Adaptive Attacks:** Ablation studies confirm that BYE remains effective even when adversaries use multi-trigger or adaptive attack strategies[arXiv:2505.16916v1, Sec. 5.4; p. 9].\n\n### Broader Research Context\n\nBYE addresses a critical gap in MLLM security: the lack of robust, unsupervised defenses against backdoor attacks during fine-tuning. By focusing on attention collapse, BYE connects to broader research on model interpretability and adversarial robustness, demonstrating that model-internal signals can serve as effective safeguards against data poisoning[arXiv:2505.16916v1, Sec. 2.2-2.3; p. 2-3].\n\n### Implications for the Field\n\nThe success of BYE highlights the potential of self-supervised, model-intrinsic diagnostics for AI safety. Its principles could be extended to other domains where model behavior is altered by malicious or outlier samples, opening new avenues for research in trustworthy AI and robust machine learning.\n\n---\n\n## Summary Table\n\n| Feature                    | BYE Approach                                      | Baseline Comparison              |\n|----------------------------|---------------------------------------------------|----------------------------------|\n| Defense Signal             | Attention entropy (self-supervised)               | Random Drop, ZIP (external)      |\n| Adaptability               | Works across models and tasks                     | Limited to specific conditions   |\n| Data Requirement           | No clean data or labels                           | May require clean data           |\n| Robustness                 | High (resistant to multi-trigger attacks)         | Variable                         |\n| Implementation Complexity  | Low (no model modification)                       | ZIP: high (auxiliary models)     |\n\n---\n\n## Key Takeaways\n\n- **BYE effectively detects and filters backdoor-poisoned samples using attention entropy as a self-supervised signal.**\n- **It achieves near-zero attack success rates while maintaining high clean performance across diverse MLLMs and tasks.**\n- **The approach is robust, unsupervised, and scalable, making it suitable for real-world deployment.**\n- **BYE sets a new standard for model-intrinsic defenses against data poisoning in multimodal AI systems.**[arXiv:2505.16916v1, Sec. 5; p. 7-9]", "citations": ["https://ista.org/docs/3Aoverview.pdf", "https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13185", "https://cdn-links.lww.com/permalink/ede/c/ede_1_1_2024_09_26_chen_24-0216_sdc1.pdf", "https://github.com/broadinstitute/Drop-seq/releases", "https://stats.stackexchange.com/questions/94902/how-to-represent-geography-or-zip-code-in-machine-learning-model-or-recommender"], "page_number": 7, "subsections": [{"id": "experimental-setup-and-datasets", "title": "Experimental Setup and Datasets", "content": "Content generation for section \'Experimental Setup and Datasets\' failed due to: cannot access local variable \'response\' where it is not associated with a value", "citations": [1], "page_number": 7}, {"id": "key-results-and-performance-analysis", "title": "Key Results and Performance Analysis", "content": "Content generation for section \'Key Results and Performance Analysis\' failed due to: cannot access local variable \'response\' where it is not associated with a value", "citations": [1], "page_number": 8}, {"id": "visualization-and-ablation-studies", "title": "Visualization and Ablation Studies", "content": "Content generation for section \'Visualization and Ablation Studies\' failed due to: cannot access local variable \'response\' where it is not associated with a value", "citations": [1], "page_number": 8}]}, {"id": "impact-and-future-directions", "title": "Impact and Future Directions: Securing MLLM Fine-tuning with Self-Diagnostic Defense", "content": "Content generation for section \'Impact and Future Directions: Securing MLLM Fine-tuning with Self-Diagnostic Defense\' failed due to: cannot access local variable \'response\' where it is not associated with a value", "citations": [1], "page_number": 9, "subsections": [{"id": "significance-and-implications", "title": "Significance and Implications for MLLM Security", "content": "Content generation for section \'Significance and Implications for MLLM Security\' failed due to: cannot access local variable \'response\' where it is not associated with a value", "citations": [1], "page_number": 9}, {"id": "limitations-and-future-work", "title": "Limitations and Directions for Future Research", "content": "Here is a comprehensive and educational expansion of the section \u201cLimitations and Directions for Future Research\u201d for a research paper on BYE (Believe Your Eyes), tailored for advanced researchers and graduate students.\n\n---\n\n## Introduction: Purpose and Context of Limitations Discussion\n\nEvery scientific study has boundaries and constraints, and explicitly discussing these limitations is essential for the integrity of research. In this section, we outline the limitations of the BYE data filtering framework for backdoor defense in Multimodal Large Language Models (MLLMs), explain why these constraints matter, and propose concrete directions for future research. This structure not only clarifies the scope and robustness of the current findings but also helps readers interpret results accurately and informs the next generation of research[1][3][5].\n\nUnderstanding the limitations and future directions is crucial because:\n- **It highlights the real-world applicability of BYE**\u2014where offline filtering is strong, but challenges remain for evolving and dynamic environments.\n- **It guides researchers** in extending the framework to new settings, such as online adaptation or multi-stage training.\n- **It fosters innovation** by identifying gaps and encouraging novel solutions for emerging threats in MLLM security.\n\nThis section complements the empirical results (found in Section 5 and Table 1, pages 7-8) and connects directly to the broader motivation for robust MLLM adaptation in open and uncertain scenarios.\n\n---\n\n## Core Content: Key Limitations and Conceptual Directions\n\n### What Are the Main Limitations?\n\n**1. Offline vs. Online Data Filtering**\nBYE is designed as an offline data filtering pipeline. It processes training samples in batches before fine-tuning, which is highly effective when the entire dataset is available and stable[5]. However, real-world applications often require models to adapt to new data streams dynamically\u2014such as in online learning or continual adaptation settings. Here, BYE\u2019s approach of extracting attention entropy from a fixed dataset cannot directly apply, as new data arrives sequentially and model states may drift[1].\n\n**2. Dynamic Attention Monitoring**\nRecognizing poisoned samples in evolving environments would require novel mechanisms to monitor attention entropy in real time. Currently, BYE relies on static entropy profiles averaged across sensitive layers. Future work could explore:\n- **Incremental entropy estimation** as new data streams in.\n- **Adaptive layer selection** to dynamically identify sensitive layers as the model evolves.\n\n**3. Multi-Stage and Task-Transfer Scenarios**\nBYE is validated for single-stage fine-tuning on domain-specific tasks. However, in practice, MLLMs are often fine-tuned in multiple stages or transferred across different downstream tasks. Extending BYE to multi-stage fine-tuning could involve:\n- **Cascading entropy checks** after each stage.\n- **Transfer entropy thresholds** across different tasks, leveraging domain adaptation techniques.\n\n**4. Advanced and Evasive Triggers**\nWhile BYE effectively detects patch-based triggers, adversaries may develop more sophisticated, evasive triggers\u2014such as blended or semantic triggers that do not cause attention collapse in the same way. Defending against such attacks would require:\n- **Multi-modal entropy signals** beyond attention maps.\n- **Hybrid anomaly detection** combining attention, saliency, and semantic consistency.\n\n**5. Scalability to Larger MLLMs**\nBYE is tested on models like LLaVA and InternVL (7\u20138B parameters). As MLLMs grow larger (reaching hundreds of billions of parameters), the computational cost of extracting and analyzing attention entropy across all layers becomes prohibitive. Future work should investigate:\n- **Efficient entropy sampling** strategies.\n- **Layer-wise approximation** or attention summarization techniques.\n\n---\n\n### Supporting Mathematical Formulations\n\nA key insight of BYE is that attention entropy provides a robust signal for detecting poisoned samples. Specifically, for each sample $(x, q)$, the attention entropy at layer $l$ is computed as:\n\n$$\nH^{(l)}(x, q) = -\\sum_{t=1}^{T} \\hat{A}^{(l)}_{t}(x, q) \\log \\hat{A}^{(l)}_{t}(x, q)\n$$\n\nwhere $\\hat{A}^{(l)}_{t}(x, q)$ is the normalized attention weight from the first decoding token to the $t$-th image token at layer $l$, and $T$ is the number of image tokens. This entropy quantifies the dispersion of attention, with low values indicating collapsed (trigger-dominated) attention. The aggregation across selected sensitive layers (see Algorithm 1, page 6) is:\n\n$$\n\\overline{H}(x, q) = \\frac{1}{|L_{\\text{sens}}|} \\sum_{l \\in L_{\\text{sens}}} H^{(l)}(x, q)\n$$\n\nThis averaging step improves robustness by combining signals from multiple layers.\n\n---\n\n### Example: Why Dynamic Monitoring Matters\n\nConsider a scenario where a deployed MLLM receives new images from users in real time (e.g., a chatbot for medical image analysis). Traditional BYE would filter these images in batches offline, but in a production system, this delay is unacceptable. Instead, the system should:\n- **Monitor attention entropy online** as each new image is processed.\n- **Trigger alerts or interventions** when entropy drops below a dynamic threshold, similar to anomaly detection in time-series data.\n\nThis real-world analogy underscores the need for future extensions to BYE\u2019s pipeline.\n\n---\n\n## Technical Details: Implementation and Parameter Choices\n\n### Algorithmic Extensions for Future Research\n\nTo adapt BYE for online or continual adaptation, future implementations could extend Algorithm 1 (page 6) as follows:\n\n\`\`\`plaintext\nAlgorithm 2: Online BYE Extension\n\nInput: Stream of new samples {(x_i, q_i, y_i)}, pretrained MLLM M, past entropy profile\nOutput: Filtered stream, updated adaptation of M\n\nfor each new (x_i, q_i) do\n    Extract attention maps {A^(l)(x_i, q_i)} for each layer l\n    Compute layer-wise entropies H^(l)(x_i, q_i) via Eq. (5)\n    Aggregate entropies: H_agg = average over L_sens layers\n    Compare H_agg to dynamic threshold (adaptive to recent distribution)\n    if H_agg < threshold then\n        Flag as suspicious and exclude from adaptation\n    else\n        Use for online fine-tuning\n    end if\nend for\nUpdate entropy profile and L_sens based on recent samples\n\`\`\`\nThis pseudocode outlines how BYE could be adapted for dynamic environments, with layer selection and thresholds updated incrementally.\n\n---\n\n### Design Decisions and Parameter Choices\n\n- **Sensitive Layer Selection:** As described on page 7, sensitive layers (where entropy separates clean and poisoned samples) are selected using the Bimodal Separation Index (BSI):\n  $$\n  \\text{BSI}^{(l)} = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}\n  $$\n  Layers with BSI above a threshold $\\tau_{bsi}$ are included in $L_{\\text{sens}}$. Future work could automate or adapt this threshold based on data drift.\n- **Entropy Aggregation:** Averaging across multiple sensitive layers reduces noise and improves detection stability.\n- **Cluster-based Filtering:** Gaussian Mixture Models (GMMs) are used for clustering entropy profiles, as they robustly identify low-entropy (poisoned) clusters.\n\n---\n\n### Reference to Figures and Tables\n\n- **Figure 2** (page 4) visualizes attention collapse, showing how attention maps shift from distributed to trigger-focused on poisoned samples.\n- **Figure 3** (page 8) demonstrates the bimodal separation of entropy scores, a key signal for filtering poisoned data.\n- **Table 1** (pages 7\u20138) summarizes BYE\u2019s effectiveness in reducing attack success rates while maintaining clean performance.\n\n---\n\n## Significance and Connections\n\n### Why These Limitations and Directions Matter\n\n- **Novelty:** BYE is among the first to leverage internal attention entropy for unsupervised backdoor detection in MLLMs, a significant departure from prior work relying on external supervision or model modifications[1][3].\n- **Broader Impact:** Addressing these limitations will enable robust and scalable deployment of MLLMs in open, dynamic, and evolving environments\u2014such as personalized AI assistants, autonomous systems, and real-time decision support.\n- **Connections to Related Work:** BYE\u2019s reliance on attention collapse connects to emerging research on interpretable AI and model introspection, while its limitations highlight ongoing challenges in adversarial robustness and continual learning[1][3].\n\n### Key Innovations and Field Implications\n\n- **Attention as a Built-in Diagnostic:** BYE innovates by using the model\u2019s own attention mechanisms as a self-diagnostic tool, reducing reliance on external signals.\n- **Scalability and Generalization:** Future research directions focus on making BYE scalable to larger models and more complex, real-world scenarios.\n- **Security and Trust:** By advancing robust data filtering, this work strengthens the security and trustworthiness of MLLMs in critical applications.\n\n---\n\n## Conclusion\n\nThe explicit discussion of limitations and future directions is essential for advancing the field and guiding practical deployment. By addressing the challenges of online adaptation, multi-stage training, and advanced attacks, future research can build on BYE\u2019s foundation to deliver even more robust and reliable MLLM defenses[1][3][5].", "citations": ["https://blog.wordvice.com/how-to-present-study-limitations-and-alternatives/", "https://www.aje.com/arc/how-to-write-limitations-of-the-study/", "https://libguides.usc.edu/writingguide/limitations", "https://www.ref-n-write.com/blog/limitations-in-research-a-simplified-guide-with-examples/", "https://mindthegraph.com/blog/limitations-in-research/"], "page_number": 9}, {"id": "broader-impact-on-ai-field", "title": "Broader Impact on the AI Field", "content": "## Broader Impact on the AI Field\n\nThis section elucidates the wider implications of leveraging internal attention entropy as a diagnostic and defensive mechanism in Transformer-based models, particularly in the context of AI safety and robustness. Understanding this broader impact is critical because it situates the proposed Believe Your Eyes (BYE) method\u2014not merely as a niche defense against backdoor attacks in Multimodal Large Language Models (MLLMs)\u2014but as a novel paradigm contributing to the foundational principles of trustworthy AI systems. By exploring how model-intrinsic signals like attention entropy can be harnessed without external supervision, this approach reframes defense strategies to be more self-reliant and adaptable, potentially influencing the future design of secure and reliable AI architectures across modalities and applications.\n\nThis discussion highlights the innovative nature of BYE within the expanding landscape of AI safety research, where adversarial robustness, internal interpretability, and self-supervised defenses are core themes. As Transformer-based models become ubiquitous in multimodal and general AI systems, methods that exploit their intrinsic computational properties\u2014such as attention entropy\u2014to detect and mitigate security threats pave the way for more resilient AI frameworks. This aligns with broader research trends that examine the informational characteristics of model behaviors to enhance safety without reliance on external clean data or intervention, fostering greater trust in AI deployments in real-world scenarios.\n\n---\n\n### Core Concepts and Methodological Foundations\n\nAt the heart of BYE\u2019s impact is the concept of **attention entropy**\u2014a quantitative measure capturing the dispersion of a model\u2019s attention weights across input tokens. Attention entropy, derived from Shannon entropy, is expressed mathematically for a given attention distribution $\\hat{A}^{(l)}(x,q)$ at layer $l$ by:\n\n$$\nH^{(l)}(x, q) = -\\sum_{t=1}^T \\hat{A}^{(l)}_t(x, q) \\log \\hat{A}^{(l)}_t(x, q),\n$$\n\nwhere $T$ is the number of image tokens and $\\hat{A}^{(l)}_t$ is the normalized attention weight assigned to token $t$. This measure reflects how \"focused\" or \"diffuse\" the model\'s attention is at each layer; low entropy indicates concentrated attention (potentially on backdoor triggers), whereas higher entropy suggests a more semantically meaningful spread of focus.\n\nBYE capitalizes on the phenomenon termed **attention collapse** (illustrated in Figure 2 on page 4 of the paper), where backdoor triggers cause the model\'s attention to disproportionately fixate on adversarial patches, thereby derailing the model\'s normal cross-modal reasoning. This abnormal concentration manifests as significantly reduced attention entropy in specific Transformer layers, disrupting the semantic alignment critical for robust inference.\n\nTo detect poisoned samples, BYE implements a **bimodal entropy profiling** strategy. For each layer $l$, attention entropies across the dataset are modeled using a two-component Gaussian Mixture Model (GMM):\n\n$$\n\\{H^{(l)}(x_i, q_i)\\}_{i=1}^N \\sim \\pi_1 \\mathcal{N}(\\mu_1, \\sigma_1^2) + \\pi_2 \\mathcal{N}(\\mu_2, \\sigma_2^2).\n$$\n\nThe underlying assumption is that clean and poisoned samples form distinct entropy clusters. The **Bimodal Separation Index (BSI)** quantifies the separation between these clusters:\n\n$$\n\\text{BSI}(l) = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}},\n$$\n\nand layers with BSI above a threshold $\\tau_{bsi}$ are selected for further analysis. This selection enhances sensitivity to layers revealing clear entropy separation, as detailed in Algorithm 1 on pages 5-6.\n\nAn aggregated entropy score across these sensitive layers,\n\n$$\n\\bar{H}(x, q) = \\frac{1}{|L_{\\text{sens}}|} \\sum_{l \\in L_{\\text{sens}}} H^{(l)}(x, q),\n$$\n\nserves as a holistic indicator for sample filtering. Subsequent GMM clustering on $\\bar{H}$ values isolates low-entropy clusters corresponding to suspicious samples, enabling effective, unsupervised backdoor sample removal (see Figure 3 on page 7 for visualization of this bimodal distribution).\n\n---\n\n### Technical Details of Implementation\n\nThe BYE framework unfolds in a three-stage pipeline:\n\n1. **Attention Extraction:** After fine-tuning the MLLM on the combined clean and potentially poisoned dataset, internal cross-modal attention maps from all decoder layers are extracted per sample. The focus is on the attention from the initial decoding token to all image tokens, averaged over attention heads as per Equation (4) on page 4:\n\n    $$\n    \\hat{A}^{(l)}(x, q) = \\frac{1}{H} \\sum_{h=1}^H A^{(l,h)}(x, q),\n    $$\n\n    where $H$ is the number of attention heads.\n\n2. **Entropy Profiling and Layer Selection:** Attention entropy $H^{(l)}(x,q)$ is computed for each layer as above. Subsequently, GMM clustering on per-layer entropies detects bimodal distributions to identify layers where poisoned and clean samples are distinguishable. The BSI metric guides the selection of \"sensitive\" layers $L_{\\text{sens}}$ (refer to Section 4.2 and Equation (7) on pages 5-6).\n\n3. **Sample Cleaning:** For each sample, the average entropy across sensitive layers $\\bar{H}(x,q)$ is calculated. GMM clustering on $\\{\\bar{H}(x_i,q_i)\\}$ isolates low-entropy clusters representing poisoned data, which are then filtered out. The purified dataset $D_{\\text{clean}}$ is used to re-fine-tune the model, yielding a defense-hardened model $M_{\\text{clean}}$ as summarized in Algorithm 1 (page 6).\n\nThis design avoids dependence on external labeled data or model architecture modifications, enabling self-supervised defense. Parameter choices such as the BSI threshold $\\tau_{bsi}$ and the number of selected layers are empirically justified in Appendix A.3. The method accommodates diverse MLLMs and vision-language tasks, showcasing generality.\n\n---\n\n### Significance and Broader Connections\n\nBYE introduces an innovative paradigm of **model-intrinsic, self-supervised defense** via attention entropy, marking a significant departure from external or supervised backdoor detection methods. Its reliance on internal diagnostic signals resonates with broader efforts in AI safety to develop robust, interpretable, and adaptive defenses without requiring clean data or costly human intervention\u2014a core challenge in deploying secure AI at scale.\n\nThis approach aligns with ongoing research emphasizing **information-theoretic measures** (e.g., entropy) as keys to understanding and regulating Transformer dynamics [2][3], and contributes to the growing ecosystem of AI safety techniques aimed at securing complex multimodal systems [4]. By showing that internal attention entropy can serve as a reliable indicator of malicious inputs, BYE paves the way for novel methods in securing other Transformer-based models, including purely language-based LLMs and emerging multimodal architectures.\n\nMoreover, the method supports the goal of establishing **trust and reliability** in AI deployments, particularly important in sensitive domains like healthcare, autonomous systems, and human-AI collaboration, where undetected backdoors can cause catastrophic failures [5]. Its self-diagnostic nature may inspire future work on real-time monitoring of model internal states to detect abnormal behaviors proactively.\n\nIn summary, BYE\u2019s contributions extend beyond backdoor cleaning: by exploiting internal attention entropy, it opens new avenues in AI safety research, promoting defenses that are both principled and practically deployable, and advancing the maturity of trustworthy AI systems.\n\n---\n\nThis comprehensive overview explains the broader impact of BYE\u2019s entropy-based defense mechanism on the AI field, emphasizing its foundational contributions to safety, robustness, interpretability, and trustworthiness in Transformer-based models.", "citations": ["https://engineering.nyu.edu/news/cracking-code-private-ai-role-entropy-secure-language-models", "https://arxiv.org/html/2412.16545v1", "https://openreview.net/pdf?id=VH1fLXKWSw", "https://arxiv.org/html/2502.09288v1", "https://pmc.ncbi.nlm.nih.gov/articles/PMC10606888/"], "page_number": 9}]}];
const citationsData: string[] = ["https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/intro-to-JSON/", "https://libraryjuiceacademy.com/shop/course/161-introduction-json-structured-data/", "https://arxiv.org/html/2408.11061v1", "https://monkt.com/recipies/research-paper-to-json/", "https://developer.apple.com/documentation/applenews/json-concepts-and-article-structure"];

// YouTube URL detection function
const isYouTubeUrl = (url: string): boolean => {
  return /(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)/.test(url);
};

// Extract YouTube video ID
const getYouTubeVideoId = (url: string): string | null => {
  const match = url.match(/(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)/);
  return match ? match[1] : null;
};

// Markdown component with math support
const MarkdownContent = ({ content }: { content: string }) => {
  return (
    <ReactMarkdown
      remarkPlugins={[remarkGfm, remarkMath]}
      rehypePlugins={[rehypeKatex]}
      className="prose prose-lg max-w-none text-gray-900 leading-relaxed"
      components={{
        // Custom styling for different elements
        h1: ({ children }) => <h1 className="text-3xl font-bold text-gray-900 mb-4">{children}</h1>,
        h2: ({ children }) => <h2 className="text-2xl font-semibold text-gray-900 mb-3">{children}</h2>,
        h3: ({ children }) => <h3 className="text-xl font-medium text-gray-900 mb-2">{children}</h3>,
        p: ({ children }) => <p className="text-gray-900 mb-4 leading-relaxed">{children}</p>,
        ul: ({ children }) => <ul className="list-disc list-inside mb-4 text-gray-900">{children}</ul>,
        ol: ({ children }) => <ol className="list-decimal list-inside mb-4 text-gray-900">{children}</ol>,
        li: ({ children }) => <li className="mb-1">{children}</li>,
        blockquote: ({ children }) => <blockquote className="border-l-4 border-blue-500 pl-4 italic text-gray-600 mb-4">{children}</blockquote>,
        code: ({ children, className }) => {
          const isInline = !className;
          if (isInline) {
            return <code className="bg-gray-100 px-1 py-0.5 rounded text-sm font-mono text-gray-900">{children}</code>;
          }
          return <pre className="bg-gray-100 p-4 rounded-lg overflow-x-auto mb-4"><code className="text-sm font-mono">{children}</code></pre>;
        },
        a: ({ children, href }) => <a href={href} className="text-blue-600 hover:text-blue-800 underline" target="_blank" rel="noopener noreferrer">{children}</a>,
      }}
    >
      {content}
    </ReactMarkdown>
  );
};

export default function PaperPage() {
  const [activeContent, setActiveContent] = useState('');
  const [imagesData, setImagesData] = useState<ImageData[]>([]);
  const [imagesLoading, setImagesLoading] = useState(true);
  const [selectedImage, setSelectedImage] = useState<ImageData | null>(null);
  const [selectedPdfPage, setSelectedPdfPage] = useState<number | null>(null);
  const [youtubeModal, setYoutubeModal] = useState<{ isOpen: boolean; videoId: string | null }>({
    isOpen: false,
    videoId: null
  });
  const [mobileMenuOpen, setMobileMenuOpen] = useState(false);
  // Fetch images from API
  useEffect(() => {
    const fetchImages = async () => {
      try {
        setImagesLoading(true);
        const response = await fetch(`http://localhost:8000/api/images/${paperData.arxiv_id}`);
        if (response.ok) {
          const images = await response.json();
          setImagesData(images);
        } else {
          console.error('Failed to fetch images:', response.statusText);
          setImagesData([]);
        }
      } catch (error) {
        console.error('Error fetching images:', error);
        setImagesData([]);
      } finally {
        setImagesLoading(false);
      }
    };

    fetchImages();
  }, []);
  
  // Initialize with the first section
  useEffect(() => {
    if (sectionsData?.length > 0) {
      setActiveContent(sectionsData[0].id);
    }
  }, []);
  
  // Get current content (section or subsection)
  const getCurrentContent = () => {
    // First check if it's a main section
    const section = sectionsData?.find(section => section.id === activeContent);
    if (section) {
      return { type: 'section', content: section };
    }
    
    // Then check if it's a subsection
    for (const section of sectionsData || []) {
      const subsection = section.subsections?.find(sub => sub.id === activeContent);
      if (subsection) {
        return { type: 'subsection', content: subsection, parentSection: section };
      }
    }
    
    return null;
  };
  
  const currentContent = getCurrentContent();
  
  // Get relevant images for current content
  const getRelevantImages = (pageNumber: number | undefined): ImageData[] => {
    if (!pageNumber || !imagesData || !Array.isArray(imagesData)) return [];
    return imagesData.filter(img => img.page === pageNumber);
  };
  
  const relevantImages = getRelevantImages(currentContent?.content?.page_number);
  
  // Get citations for current content
  const getSectionCitations = (citations?: string[]): string[] => {
    if (!citations || !Array.isArray(citations)) return [];
    return citations;
  };
  
  const contentCitations = getSectionCitations(currentContent?.content?.citations);

  // Handle citation click
  const handleCitationClick = (citation: string) => {
    if (isYouTubeUrl(citation)) {
      const videoId = getYouTubeVideoId(citation);
      if (videoId) {
        setYoutubeModal({ isOpen: true, videoId });
        return;
      }
    }
    // For non-YouTube links, open in new tab
    window.open(citation, '_blank', 'noopener,noreferrer');
  };

  // Handle PDF page view - open in new tab
  const handlePdfPageView = (pageNumber: number) => {
    const pdfUrl = `https://arxiv.org/pdf/${paperData.arxiv_id}.pdf#page=${pageNumber}`;
    window.open(pdfUrl, '_blank', 'noopener,noreferrer');
  };



  return (
    <div className="min-h-screen flex flex-col bg-white">
      <style jsx global>{customStyles}</style>
      {/* Header */}
      <header className="bg-white sticky top-0 z-50">
        <div className="max-w-full mx-auto px-4">
          <div className="flex items-center justify-between h-16 lg:pl-32 md:pl-16 pl-4">
            <div className="flex items-center space-x-3">
              <Link href="/" className="flex items-center text-blue-600 hover:text-blue-700">
                <ArrowLeft className="w-6 h-6" />
              </Link>
              <h1 className="text-2xl font-bold text-gray-900 lowercase">deeprxiv</h1>
              <span className="text-lg text-gray-800 font-medium truncate max-w-md lg:max-w-2xl">
                {paperData.title}
              </span>
            </div>
            <button 
              onClick={() => setMobileMenuOpen(!mobileMenuOpen)}
              className="md:hidden p-2 text-gray-600 hover:text-gray-900"
            >
              <Menu className="w-6 h-6" />
            </button>
          </div>
        </div>
      </header>

      {/* Mobile Navigation Overlay */}
      {mobileMenuOpen && (
        <div className="fixed inset-0 bg-black bg-opacity-50 z-40 md:hidden" onClick={() => setMobileMenuOpen(false)}>
          <div className="fixed left-0 top-16 bottom-0 w-80 bg-white overflow-y-auto" onClick={(e) => e.stopPropagation()}>
            <div className="p-6">
              <nav className="space-y-1">
                {sectionsData?.map((section) => (
                  <div key={section.id} className="space-y-1">
                    <button
                      onClick={() => {
                        setActiveContent(section.id);
                        setMobileMenuOpen(false);
                      }}
                      className={`block w-full text-left px-1 py-3 rounded-md transition-colors text-sm font-medium ${
                        activeContent === section.id
                          ? 'bg-blue-50 text-blue-700'
                          : 'text-gray-900 hover:bg-gray-100'
                      }`}
                    >
                      <div className="truncate" title={section.title}>
                        {section.title}
                      </div>
                    </button>
                    
                    {section.subsections && section.subsections.length > 0 && (
                      <div className="ml-8 space-y-1">
                        {section.subsections.map((subsection) => (
                          <button
                            key={subsection.id}
                            onClick={() => {
                              setActiveContent(subsection.id);
                              setMobileMenuOpen(false);
                            }}
                            className={`block w-full text-left px-3 py-2 rounded-md text-sm transition-colors ${
                              activeContent === subsection.id
                                ? 'bg-blue-25 text-blue-600'
                                : 'text-gray-800 hover:bg-gray-50'
                            }`}
                          >
                            <div className="truncate" title={subsection.title}>
                              {subsection.title}
                            </div>
                          </button>
                        ))}
                      </div>
                    )}
                  </div>
                ))}
              </nav>
            </div>
          </div>
        </div>
      )}

      {/* Main Content */}
      <main className="flex-grow">
        <div className="max-w-full mx-auto px-4">
          <div className="flex min-h-screen">
            {/* Left Sidebar - Navigation */}
            <aside className="w-72 bg-white flex-shrink-0 fixed top-16 bottom-0 overflow-y-auto scrollbar-hide hidden md:block md:left-16 lg:left-32">
              <div className="p-6">
                <nav className="space-y-1">
              {sectionsData?.map((section) => (
                  <div key={section.id} className="space-y-1">
                    {/* Main Section */}
                <button
                      onClick={() => setActiveContent(section.id)}
                      className={`block w-full text-left px-1 py-3 rounded-md transition-colors text-sm font-medium ${
                        activeContent === section.id
                          ? 'bg-blue-50 text-blue-700'
                      : 'text-gray-900 hover:bg-gray-100'
                  }`}
                >
                      <div className="truncate" title={section.title}>
                  {section.title}
                      </div>
                    </button>
                    
                    {/* All Subsections */}
                    {section.subsections && section.subsections.length > 0 && (
                      <div className="ml-8 space-y-1">
                        {section.subsections.map((subsection) => (
                          <button
                            key={subsection.id}
                            onClick={() => setActiveContent(subsection.id)}
                            className={`block w-full text-left px-3 py-2 rounded-md text-sm transition-colors ${
                              activeContent === subsection.id
                                ? 'bg-blue-25 text-blue-600'
                                : 'text-gray-800 hover:bg-gray-50'
                            }`}
                          >
                            <div className="truncate" title={subsection.title}>
                              {subsection.title}
                            </div>
                </button>
                        ))}
                      </div>
                    )}
                  </div>
                              ))}
                </nav>
              </div>
            </aside>

            {/* Center Content Area */}
            <div className="flex-1 bg-white px-6 py-6 overflow-y-auto main-content">
              {currentContent && (
                <>
                  <h3 className="text-2xl font-semibold text-gray-900 mb-6">
                    {currentContent.content.title}
                  </h3>
                  
                  {/* Content - Proper Markdown rendering */}
                  <MarkdownContent content={currentContent.content.content} />
                  
                  {/* Mobile PDF, Images, and Sources - Only visible on small screens */}
                  <div className="lg:hidden mt-8 space-y-6">
                    {/* PDF Section */}
                    <div>
                      <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                        <FileText className="w-4 h-4 mr-2" />
                        PDF Original
                      </h4>
                      {currentContent?.content?.page_number ? (
                        <div className="space-y-3">
                          <button
                            onClick={() => handlePdfPageView(currentContent.content.page_number!)}
                            className="w-full bg-blue-50 p-3 rounded-lg hover:bg-blue-100 transition-colors text-left"
                          >
                            <div className="flex items-center space-x-2">
                              <FileText className="w-4 h-4 text-blue-600" />
                              <div>
                                <p className="text-sm font-medium text-blue-700">
                                  Page {currentContent.content.page_number}
                                </p>
                                <p className="text-xs text-blue-600">
                                  Click to view full page
                                </p>
                              </div>
                            </div>
                          </button>
                        </div>
                      ) : (
                        <div className="text-center py-4">
                          <FileText className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                          <p className="text-xs text-gray-500 mb-2">No page reference available</p>
                          <button
                            onClick={() => window.open(`https://arxiv.org/pdf/${paperData.arxiv_id}.pdf`, '_blank', 'noopener,noreferrer')}
                            className="px-3 py-2 bg-blue-600 text-white text-xs rounded-lg hover:bg-blue-700 transition-colors"
                          >
                            View Full PDF
                          </button>
                        </div>
                      )}
                    </div>

                    {/* Images Section */}
                    <div>
                      <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                        <ImageIcon className="w-4 h-4 mr-2" />
                        Images
                      </h4>
                      {imagesLoading ? (
                        <div className="text-center py-4">
                          <div className="animate-spin rounded-full h-6 w-6 border-b-2 border-blue-600 mx-auto"></div>
                          <p className="text-xs text-gray-500 mt-2">Loading images...</p>
                        </div>
                      ) : relevantImages.length > 0 ? (
                        <div className="grid grid-cols-2 gap-2">
                          {relevantImages.map((image, index) => (
                            <div
                              key={image.id || index}
                              className="aspect-square bg-gray-200 rounded-lg flex items-center justify-center cursor-pointer hover:bg-gray-300 transition-colors overflow-hidden group"
                              onClick={() => setSelectedImage(image)}
                            >
                              <img
                                src={image.url || `/api/image/${image.id}`}
                                alt={`Figure ${index + 1}`}
                                className="max-w-full max-h-full object-contain p-1 group-hover:scale-105 transition-transform"
                              />
                            </div>
                          ))}
                        </div>
                      ) : (
                        <div className="text-center py-4">
                          <ImageIcon className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                          <p className="text-xs text-gray-500">No images for this content</p>
                        </div>
                      )}
                    </div>

                    {/* Sources Section */}
                    <div>
                      <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                        <ExternalLink className="w-4 h-4 mr-2" />
                        Sources
                      </h4>
                      {contentCitations.length > 0 ? (
                        <div className="space-y-2">
                          {contentCitations.map((citation, index) => (
                            <div
                              key={index}
                              className="bg-gray-50 p-3 rounded-lg hover:bg-gray-100 transition-colors"
                            >
                              <div className="flex items-start space-x-2">
                                <div className="flex-1 min-w-0">
                                  <p className="text-xs font-medium text-gray-900 mb-1">
                                    Reference {index + 1}
                                  </p>
                                  <p className="text-xs text-gray-800 break-words">
                                    {citation}
                                  </p>
                                  <button
                                    onClick={() => handleCitationClick(citation)}
                                    className="inline-flex items-center text-xs text-blue-600 hover:text-blue-800 hover:underline mt-2"
                                  >
                                    {isYouTubeUrl(citation) ? (
                                      <Play className="w-3 h-3 mr-1" />
                                    ) : (
                                      <ExternalLink className="w-3 h-3 mr-1" />
                                    )}
                                    {isYouTubeUrl(citation) ? 'Watch Video' : 'View Source'}
                                  </button>
                                </div>
                              </div>
                            </div>
                          ))}
                        </div>
                      ) : (
                        <div className="text-center py-4">
                          <ExternalLink className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                          <p className="text-xs text-gray-500">No citations for this content</p>
                        </div>
                      )}
                    </div>
                  </div>
                </>
              )}
            </div>

            {/* Right Sidebar - PDF, Images, and Sources */}
            <aside className="w-96 bg-white flex-shrink-0 fixed top-16 bottom-0 overflow-y-auto scrollbar-hide hidden lg:block lg:right-32">
              <div className="p-6 space-y-6">
              
              {/* PDF Section */}
              <div>
                <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                  <FileText className="w-4 h-4 mr-2" />
                  PDF Original
                </h4>
                {currentContent?.content?.page_number ? (
                  <div className="space-y-3">
                    <button
                      onClick={() => handlePdfPageView(currentContent.content.page_number!)}
                      className="w-full bg-blue-50 p-3 rounded-lg hover:bg-blue-100 transition-colors text-left"
                    >
                      <div className="flex items-center space-x-2">
                        <FileText className="w-4 h-4 text-blue-600" />
                        <div>
                          <p className="text-sm font-medium text-blue-700">
                            Page {currentContent.content.page_number}
                          </p>
                          <p className="text-xs text-blue-600">
                            Click to view full page
                          </p>
                        </div>
                      </div>
                    </button>
                    <div className="p-3 bg-gray-50 rounded-lg">
                      <p className="text-xs text-gray-600 mb-2">
                        <strong>PDF Reference:</strong>
                      </p>
                      <p className="text-xs text-gray-700">
                        This content is sourced from page {currentContent.content.page_number} of the original PDF. 
                        Click above to view the full page with figures, tables, and original formatting.
                      </p>
                    </div>
                  </div>
                ) : (
                  <div className="text-center py-4">
                    <FileText className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                    <p className="text-xs text-gray-500 mb-2">No page reference available</p>
                    <button
                      onClick={() => window.open(`https://arxiv.org/pdf/${paperData.arxiv_id}.pdf`, '_blank', 'noopener,noreferrer')}
                      className="px-3 py-2 bg-blue-600 text-white text-xs rounded-lg hover:bg-blue-700 transition-colors"
                    >
                      View Full PDF
                    </button>
                  </div>
                )}
              </div>

              {/* Images Section */}
              <div>
                <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                  <ImageIcon className="w-4 h-4 mr-2" />
                  Images
                </h4>
                {imagesLoading ? (
                  <div className="text-center py-4">
                    <div className="animate-spin rounded-full h-6 w-6 border-b-2 border-blue-600 mx-auto"></div>
                    <p className="text-xs text-gray-500 mt-2">Loading images...</p>
                  </div>
                ) : relevantImages.length > 0 ? (
                  <div className="grid grid-cols-2 gap-2">
                    {relevantImages.map((image, index) => (
                      <div
                        key={image.id || index}
                        className="aspect-square bg-gray-200 rounded-lg flex items-center justify-center cursor-pointer hover:bg-gray-300 transition-colors overflow-hidden group"
                        onClick={() => setSelectedImage(image)}
                      >
                        <img
                          src={image.url || `/api/image/${image.id}`}
                          alt={`Figure ${index + 1}`}
                          className="max-w-full max-h-full object-contain p-1 group-hover:scale-105 transition-transform"
                        />
                      </div>
                    ))}
                  </div>
                ) : (
                  <div className="text-center py-4">
                    <ImageIcon className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                    <p className="text-xs text-gray-500">No images for this content</p>
                  </div>
                )}
                {relevantImages.length > 0 && (
                  <p className="text-xs text-gray-500 mt-2 text-center">
                    Click on an image to enlarge.
                  </p>
                )}
              </div>

              {/* Sources Section */}
              <div>
                <h4 className="text-sm font-semibold text-gray-900 mb-3 flex items-center">
                  <ExternalLink className="w-4 h-4 mr-2" />
                  Sources
                </h4>
                {contentCitations.length > 0 ? (
                  <div className="space-y-2">
                    {contentCitations.map((citation, index) => (
                      <div
                        key={index}
                        className="bg-gray-50 p-3 rounded-lg hover:bg-gray-100 transition-colors"
                      >
                        <div className="flex items-start space-x-2">
                          <div className="flex-1 min-w-0">
                            <p className="text-xs font-medium text-gray-900 mb-1">
                              Reference {index + 1}
                            </p>
                            <p className="text-xs text-gray-800 break-words">
                              {citation}
                            </p>
                            <button
                              onClick={() => handleCitationClick(citation)}
                              className="inline-flex items-center text-xs text-blue-600 hover:text-blue-800 hover:underline mt-2"
                            >
                              {isYouTubeUrl(citation) ? (
                                <Play className="w-3 h-3 mr-1" />
                              ) : (
                                <ExternalLink className="w-3 h-3 mr-1" />
                              )}
                              {isYouTubeUrl(citation) ? 'Watch Video' : 'View Source'}
                            </button>
                          </div>
                        </div>
                      </div>
                    ))}
                  </div>
                ) : (
                  <div className="text-center py-4">
                    <ExternalLink className="w-8 h-8 text-gray-400 mx-auto mb-2" />
                    <p className="text-xs text-gray-500">No citations for this content</p>
                  </div>
                )}
                </div>
                
              </div>
            </aside>
          </div>
        </div>
      </main>

      {/* Image Modal with Close Button */}
      {selectedImage && (
        <div 
          className="fixed inset-0 bg-black bg-opacity-75 flex items-center justify-center z-50 p-4"
          onClick={() => setSelectedImage(null)}
        >
          <div className="relative max-w-4xl max-h-full" onClick={(e) => e.stopPropagation()}>
            <button
              onClick={() => setSelectedImage(null)}
              className="absolute top-4 right-4 text-white hover:text-gray-300 z-10 bg-black bg-opacity-50 rounded-full p-2"
            >
              <X className="w-6 h-6" />
            </button>
            <img
              src={selectedImage.url || `/api/image/${selectedImage.id}`}
              alt="Enlarged figure"
              className="max-w-full max-h-full object-contain rounded-lg"
            />
          </div>
        </div>
      )}

      {/* YouTube Modal */}
      {youtubeModal.isOpen && youtubeModal.videoId && (
        <div className="fixed inset-0 bg-black bg-opacity-75 flex items-center justify-center z-50 p-4">
          <div className="relative bg-white rounded-lg max-w-4xl w-full max-h-full">
            <button
              onClick={() => setYoutubeModal({ isOpen: false, videoId: null })}
              className="absolute top-4 right-4 text-gray-600 hover:text-gray-800 z-10"
            >
              <X className="w-8 h-8" />
            </button>
            <div className="p-4">
              <iframe
                width="100%"
                height="480"
                src={`https://www.youtube.com/embed/${youtubeModal.videoId}`}
                title="YouTube video player"
                frameBorder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowFullScreen
                className="rounded-lg"
              ></iframe>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}
